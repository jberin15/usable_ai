{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to our Usable AI Course!","text":"<p>This course provides an in-depth introduction to Usable Artificial Inteligence, emphasizing practical skills and real-world applications. You\u2019ll learn to implement essential AI techniques across various fields, using Python and popular libraries to enhance your ability to analyze, optimize, and communicate data-driven insights in areas like social science, sustainability, health, and design.</p> <p>As AI becomes increasingly more integrated in our data workflows, understanding how to use these tools effectively is essential. In addition to foundational techniques, we will also explore emerging tools like large language models (LLMs), which can support tasks such as summarization and text analysis. LLMs offer a unique way to make sense of complex information and can be particularly useful for communicating findings to a wider audience.</p> <p>This course is heavily based on Prof. Patrick C. Shih Usable AI course.</p>"},{"location":"#course-context","title":"Course Context","text":"<p>In the last decade, data generated from the Internet, sensors, wearables, and online communities has exploded, creating immense opportunities to empirically study social, behavioral, and environmental phenomena. This course will equip you with the AI tools needed to work with \u201cbig social data\u201d and beyond, enabling you to derive insights from complex datasets.</p> <p>Our interdisciplinary approach spans AI applications to real-world issues in behavioral research, health, environmental studies, and design. Alongside other quantitative methods, you will briefly engage with LLMs to explore how they can complement traditional data analysis and presentation methods.</p>"},{"location":"#objectives","title":"Objectives","text":"<p>By the end of this course, you will be able to:  - Develop foundational knowledge in AI and machine learning for real-world data analysis.  - Utilize Python libraries such as scikit-learn for machine learning, Pandas and NumPy for data handling, and Matplotlib and others for visualization.  - Prepare and manipulate basic data types, including numerical, categorical, and textual data, to make datasets ready for analysis.  - Analyze data using exploratory visualization techniques to identify patterns, trends, and insights.  - Perform data cleaning and transformation, including merging, reshaping, and aggregating datasets.  - Apply essential machine learning models for tasks like classification, clustering, and regression using scikit-learn.  - Understand and apply basic feature engineering techniques, such as dimensionality reduction, to enhance model performance.  - Have a basic knowledge of deep learning architectures and basic embedding/vectorial representations of data.  - Explore fundamental NLP techniques using libraries like NLTK and spaCy to work with textual data.  - Use large language models (LLMs) for general machine learning tasks using local or API-based models. (optional modules)</p> <p>You will showcase your learned skills by undertaking a semester-long project. This project will require detailed documentation of each step involved in its development, from initial concept to final execution.</p>"},{"location":"#course-organization","title":"Course Organization","text":"<p>This course combines lectures, hands-on exercises, and a team-based project. Lectures introduce key AI methods, including an introduction to LLMs for specific applications. In labs, you\u2019ll receive guidance on data analysis and gain experience with a range of AI tools.</p> <p>Your team-based project will involve designing, analyzing, and presenting data insights on a chosen topic. Projects will include data collection, AI-driven analysis, and visualization to create impactful narratives, with a structured timeline for each stage to keep you on track.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>In this course, we will primarily use Python for data analysis and machine learning tasks. Thus, you are required to have a good understanding of algorithms and practical experience with Python. You are encouraged to also have a basic understanding of statistics and probability, as well as notions of linear algebra.</p> <p>For self-assessment, please, visit the following link: http://bit.ly/dvizselfassess  (created by YY Ahn). Contact the instructor if you are uncertain about your background.</p>"},{"location":"#course-structure","title":"Course Structure","text":"<p>Each week, we'll explore different topics in Machine Learning and Artificial Intelligence, starting from the basics and gradually moving to more advanced concepts. The course is designed to be hands-on, with a mix of theory, practical exercises, and projects.</p> <p>Here's a tentative outline of the course for Spring 2025:</p>"},{"location":"#grades","title":"Grades","text":"<p>You will be evaluated based on performance in assignments and final project. The final grade will be calculated as follows:</p> <ul> <li>30% - Assignments</li> <li>35% - Midterm project</li> <li>35% - Final project</li> </ul> <p>An extra 10% will be given for active participation on online discussions, such as github contributions, slack discussions, asking questions, helping others, and contributing to the course materials</p> <p>Grades will not be curved and will be based on the following scale:</p> <pre><code>93.00 - 100% = A\n90.00 - 92.99% = A-\n87.00 - 89.99% = B+\n83.00 - 86.99% = B\n80.00 - 82.99% = B-\n77.00 - 79.99% = C+\n73.00 - 76.99% = C\n70.00 - 72.99% = C-\n67.00 - 69.99% = D+\n63.00 - 66.99% = D\n60.00 - 62.99% = D-\n&lt;60.00 = F\n</code></pre>"},{"location":"#communication","title":"Communication","text":"<p>We will use Canvas and GitHub for all course materials, assignments, and for the projects. Slack will be used for communication, discussions, and to provide feedback on assignments and projects. We encourage you to actively participate in discussions, ask questions, and share your thoughts and ideas. Please be respectful and considerate of others' opinions and ideas, also do not post your personal information or sensitive data in the Slack channel.</p> <p>Slack Channel for the course: [https://github.com/filipinascimento/usable_ai]</p> <p>Canvas and Email also work for communication but with a certain delay. We encourage you to use Slack for faster communication.</p> <p>If you have suggestions, criticism or feedback on improving the course, please feel free to share them with us. You can use Slack or use the anonymous feedback form: [https://forms.gle/HQmpgVRfc6USV48L6]</p>"},{"location":"#course-project","title":"Course project","text":"<p>You can choose your own project topic (individually or as a team), but it is recommended to discuss your choice with the instructor first. For the project, you will need to submit a final report detailing your findings and explaining your approach to creating them. Throughout the course, you will work on a group project, which involves researching, designing, analyzing, and using machine learning pipelines on data sets related to a chosen problem area.</p> <p>A timeline for each project stage will be provided, as the project requires both designing and executing data collection and analysis. Given this fast-paced schedule, each phase has only a few weeks allocated, so careful time management is essential.</p>"},{"location":"#recommended-books-and-resources","title":"Recommended Books and Resources","text":"<p>No textbook is required; all essential materials are covered in lectures. Here are some highly recommended books and resources on Machine learning and general artificial intelligence Python:</p> <p>M\u00fcller, A. C., &amp; Guido, S. (2016). Introduction to Machine Learning with Python: A Guide for Data Scientists. O\u2019Reilly Media, Inc.  - Raschka, S. (2019). Python Machine Learning. Packt Publishing Ltd.  - Richert, W. (2018). Building Machine Learning Systems with Python. Packt Publishing Ltd.  - Hugging Face documentation on LLMs for those interested in exploring these tools further</p> <p>Wait for more resources to be added to this list or suggest your own!</p>"},{"location":"#course-materials","title":"Course Materials","text":"<p>Here's what you can find in our repository:</p> <ul> <li>Python Jupyter Notebooks: Interactive notebooks with code, explanations, and exercises.</li> <li>PDF Presentations: Slides covering key concepts and examples. </li> <li>Assignments: Python notebook assignments to apply what you've learned.</li> <li>Datasets: A collection of datasets used in our materials, including links to Kaggle datasets for hands-on practice.</li> <li>Additional Resources: Links to further reading and external resources.</li> </ul> <p>The course videos will be shared on Canvas.</p> <p>Most of these materials will be available when the course starts.</p>"},{"location":"#course-modules","title":"Course Modules","text":"<p>The course is divided into several modules, each covering a different aspect of AI and machine learning. Here's an overview of the modules we'll cover:</p> <ol> <li> <p>Course Introduction and Setup: Introduction to the course, setting up your environment, and getting started with Python and Jupyter Notebooks.</p> </li> <li> <p>Python Basics 1: Introduction to Python, basic syntax, loops, conditional statements, and functions.</p> </li> <li> <p>Python Basics 2: Data types, classes, dealing with lists, dictionaries and sets.</p> </li> <li> <p>Data management: Introduction to databases, tables, queries, and matrix representation via numpy.</p> </li> <li> <p>Pandas and Visualization: Introduction to the Pandas library for data manipulation and analysis. Visualization with Matplotlib and other libraries.</p> </li> <li> <p>Introduction to Machine Learning: Overview of machine learning. Terminology, types of learning, basic algorithms and introduction to scikit-learn. (Midterm project)</p> </li> <li> <p>Preprocessing and Evaluation: Data preprocessing, model evaluation, cross-validation, and hyperparameter tuning.</p> </li> <li> <p>Regressions: Basics and advanced regression models.</p> </li> <li> <p>Classification and clustering: Supervised and unsupervised learning. Classification and clustering algorithms. Logistic regression, decision trees, random forests, neural networks, and k-means clustering.</p> </li> <li> <p>Feature Selection and Explainability: Feature selection, extraction, boosting, and model interpretability. Explainability via SHAP.</p> </li> <li> <p>Natural Language Processing: Introduction to NLP, text processing, tokenization, sentiment analysis, topic extraction, and introduction to neural network models.</p> </li> <li> <p>Artificial Intelligence and embeddings: Introduction to artificial neural networks and embedding representation. (Final project)</p> </li> <li> <p>Using Large Language Models (LLMs) (optional): Introduction to LLMs for various tasks, such as summarization, text generation, and question-answering. Covers API-based models and local models, transformers, prompt engineering, fine-tuning and RAG.</p> </li> </ol> <p>Some modules may be subject to change based on the course progress and feedback from students. We will keep you updated on any changes.</p>"},{"location":"#writing-assistance","title":"Writing Assistance","text":"<p>Writing reports is a large part of the class. In addition to excellent content, there are high expectations for the quality of the writing (organization, clarity, grammar, etc.). For free help at any phase of the writing process\u2014from brainstorming to polishing the final draft\u2014call Writing Tutorial Services (WTS, pronounced \u201cwits\u201d) at 812-855-6738 for an appointment. When you visit WTS, you\u2019ll find a tutor who is a sympathetic and helpful reader of your prose. To be assured of an appointment with the tutor who will know most about your class, please call in advance.</p> <p>WTS, in the new Learning Commons on the first floor of the West Tower of Wells Library, is open Monday-Thursday 10:00 a.m. to 8:00 p.m. and Friday 10:00 a.m. to 5:00 p.m. WTS tutors are also available for walk-in appointments in the Academic Support Centers in Briscoe, Forest, and Teter residence halls, in the Neal-Marshall Black Culture Center, at La Casa, and at the Groups Scholars Program Office in Maxwell. Call WTS or check our Web site for hours.</p> <p>https://wts.indiana.edu/</p> <p>Catching Up &amp; Special Cases</p> <ul> <li>All materials will be made available online, and the TAs are here to help you stay on track.</li> <li>The instructor will do his best to accommodate special situations (like illness, family emergencies, travel) by providing alternative due dates for assignments, projects, etc.</li> </ul> <p>Religious Observances</p> <p>Indiana University respects the right of all students to observe their religious holidays. Accordingly, course directors are to make reasonable accommodation, upon request, for such observances. It is the responsibility of the students involved to notify their course directors in a timely manner concerning their need for such accommodation. In this class, please send me e-mail or visit me in office hours to notify me of such a situation at least a few days in advance of the event. See full details at: https://bulletin.indiana.edu/policies/religious-observances.html</p>"},{"location":"#policies","title":"Policies","text":""},{"location":"#general-policies","title":"General policies","text":"<p>(Copied from Prof. YY Ahn's course)</p> <ol> <li>Be honest. Don\u2019t be a cheater.    Your assignments and papers should be your own work. If you find useful resources for your assignments, share them and cite them. If your friends helped you, acknowledge them. You should feel free to discuss both online and offline (except for the exam), but do not show your code directly. Any cases of academic misconduct (cheating, fabrication, plagiarism, etc.) will be reported to the School and the Dean of Students, following the standard procedure. Cheating is not cool.</li> <li>You have the responsibility of backing up all your data and code.    Always back up your code and data. You should at least use Google Drive or Dropbox at the minimum. You can also use cloud services like Google Colaboratory. Ideally, learn version control systems and use https://github.iu.edu/ or https://github.com/. Loss of data, code, or papers (e.g., due to malfunction of your laptop) is not an acceptable excuse for delayed or missing submission.</li> <li>Disabilities.    Every attempt will be made to accommodate qualified students with disabilities (e.g., mental health, learning, chronic health, physical, hearing, vision, neurological, etc.). You must have established your eligibility for support services through Disability Services for Students. Note that services are confidential, may take time to put into place, and are not retroactive. Captions and alternate media for print materials may take three or more weeks to get produced. Please contact Disability Services for Students at http://disabilityservices.indiana.edu/ or 812-855-7578 as soon as possible if accommodations are needed. The office is located on the third floor, west tower, of the Wells Library (Room W302). Walk-ins are welcome 8 AM to 5 PM, Monday through Friday. You can also locate a variety of campus resources for students and visitors who need assistance at https://accessibility.iu.edu/ada/requesting-accommodations/for-students/index.html.</li> <li>Bias-based incidents.    Any act of discrimination or harassment based on race, ethnicity, religious affiliation, gender, gender identity, sexual orientation, or disability can be reported to biasincident@indiana.edu or to the Dean of Students Office at (812) 855-8188.</li> <li> <p>Sexual misconduct and Title IX.    Title IX and IU\u2019s Sexual Misconduct Policy prohibit sexual misconduct in any form, including sexual harassment, sexual assault, stalking, and dating and domestic violence. If you have experienced sexual misconduct, or know someone who has, you can use university resources:  </p> <ul> <li>a) The Sexual Assault Crisis Services (SACS) at (812) 855-8900 (counseling services)  </li> <li>b) Confidential Victim Advocates (CVA) at (812) 856-2469 (advocacy and advice services)  </li> <li>c) IU Health Center at (812) 855-4011 (health and medical services)</li> </ul> </li> <li> <p>If you have any mental health issues, don\u2019t hesitate to contact IU\u2019s Counseling and Psychological Services, which provides free counseling sessions. Also, please contact Disability Services for Students at http://disabilityservices.indiana.edu/ or 812-855-7578 as soon as possible if accommodations are needed.</p> </li> </ol>"},{"location":"#policy-on-the-use-of-generative-ai-and-llms","title":"Policy on the Use of Generative AI and LLMs","text":"<p>In accordance with IU\u2019s Generative AI Policies, you have the instructor permission to use generative AI (GAI) and large language models (LLMs), including GPT, Gemini, Llama, Copilot, and similar tools, in this course as long as you do so responsibly and transparently. This includes tasks such as code completion, brainstorming, or improving the clarity of your text. However, these tools are not recommended for fully automating assignments or drafting entire project reports. Keep in mind that the quality of GAI output can be unreliable, and you remain fully accountable for any inaccuracies, biases, or offensive content you submit.</p> <ul> <li>Disclosure: If GAI usage played a substantial role in shaping your work, you must clearly acknowledge it. Provide a brief description of how you used the tool, and if it was essential to your final result, include a copy of the prompt(s).</li> <li>Citation Example (no particular format required beyond basic transparency):</li> </ul> <p>OpenAI. (2024). ChatGPT (Mar 14 version) [Large language model]. https://chat.openai.com/chat</p> <ul> <li>Verification: For code generated by AI, make sure it does exactly what you intend. You must be able to explain its logic and functionality; \u201cblindly trusting\u201d AI outputs is strongly discouraged.</li> <li>Limitations: No GAI tools may be used during exams or project presentations. Doing so will be considered a violation of course policies.</li> <li>Integrity: Use of GAI without proper acknowledgment can be treated as plagiarism or cheating.</li> </ul> <p>Note: There is currently no IU-approved or otherwise reliable method to detect AI-generated content. Some GAI systems can produce excessively verbose or uniquely structured text, but this is not consistent or guaranteed. We rely on your honesty and accurate self-reporting regarding the use of these tools.</p> <p>In short, while GAI can streamline your workflow, it is not a substitute for human critical thinking. Leverage these tools thoughtfully, verify their outputs, and be prepared to justify your methods.</p>"},{"location":"#special-thanks","title":"Special Thanks","text":"<ul> <li>Patrick C. Shih for kindly providing the most of the materials for this course.</li> <li>YY Ahn for kindly providing materials for his Data Visualization course and his organization spreadsheets.</li> <li>Francisco Alfaro originally helped the migration with mkdocs in the original YY's course. </li> </ul>"},{"location":"__init__/","title":"init","text":""},{"location":"communication/","title":"Communication","text":""},{"location":"communication/#channels","title":"Channels","text":"<p>We will have Canvas as the primary and official channel, but also have Slack as a more casual/quick communication channel. </p> <p>You need to use IU email or Canvas for any communication that contains your personal information or specific grade.  However, I may be able to see Slack messages more quickly (I'm often behind my emails). </p> <p>It is really important that if you email me, you add <code>I513</code> to the subject line. This will help me to filter the emails and respond more quickly.</p> <p>So I'd encourage to use Slack as the default, but then Canvas/Email for anything personal or sensitive. Even when you communicate via Canvas, you can still ping me on Slack to ensure that I see it quickly. </p>"},{"location":"communication/#how-to-get-your-answers-quickly-and-effectively","title":"How to get your answers quickly and effectively","text":""},{"location":"communication/#not-so-effective","title":"Not so effective","text":"<ol> <li> <p>Sending DMs to a single TA or only to the instructor individually: You are less likely to get a timely response and you may need to send multiple messages to multiple people and then coordinate them. What could be a single message in a group DM can become dozens of scattered messages. Unless it is very personal/sensitive, avoid doing this. </p> </li> <li> <p>Using Canvas/email as the primary channel: This is totally fine given that it is our primary communication channel. However, please note that the response may be much slower given that I am usually swamped with emails and cannot respond quickly. I can see Slack messages more often. </p> </li> </ol>"},{"location":"communication/#more-effective","title":"More effective","text":"<ol> <li> <p>Post in <code>#q-and-a</code> channel: this will maximize the number of people who can see and answer your question (the instructor, TAs, and other students). </p> </li> <li> <p>Create a group DM with all AIs &amp; the instructor: This again will make sure that everyone in the team see your message and everyone is on the same page. </p> </li> </ol>"},{"location":"communication/#how-to-ask-a-good-question","title":"How to ask a good question?","text":"<p>The quality and promptness of answers you get will largely depend on the quality of your question! </p> <p>provide enough context. For instance, if you encounter an error, you may want to show us, for instance, the error message, what you were doing, screenshots, what you did to solve the issue, your hypothesis about why it is happening, and the notebook file itself, and so on. Actually, one of the best ways to solve an issue is trying to ask a really good question properly.</p> <p>Show the screenshot. Since it is impossible to remember every single quiz question or parts of the assignments, not having the context right there means: open a browser, go to canvas website, type login password, wait for the duo login, navigate to the course, navigate to the module, open the right question, and read it. It creates a huge unnecessary friction. If you\u2019re looking at the question (or any text), just copy &amp; paste it. </p> <p>Don\u2019t summon, just ask your question. </p> <ul> <li>Don't: \"Hello professor? Can I ask a question?\" - Of course you can ask a question! \ud83d\ude0a You can just ask! </li> <li>Do: \"Could you clarify the following quiz question? Here's the screenshot of the quiz and it is module X review quiz. I thought that the answer was X but it was Y. blah blah ...\"</li> </ul>"},{"location":"hooks/","title":"Hooks","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nfrom bs4 import BeautifulSoup\n</pre> import os from bs4 import BeautifulSoup In\u00a0[\u00a0]: Copied! <pre>def on_files(files, config):\n    \"\"\"\n    Ensure all notebook files are processed and their Markdown versions include our header.\n    \"\"\"\n    for file in files:\n        if file.src_path.endswith(\".ipynb\"):\n            notebook_path = file.src_path\n            md_path = file.dest_path.replace(\".ipynb\", \".md\")\n\n            repo_url = \"https://github.com/filipinascimento/usable_ai/blob/main/\"\n            colab_url = \"https://colab.research.google.com/github/filipinascimento/usable_ai/blob/master/\"\n            \n            colab_link = f\"{colab_url}{notebook_path}\"\n            github_link = f\"{repo_url}{notebook_path}\"\n\n            prepend_content = f\"\"\"\n&lt;table class=\"m01-notebook-buttons\" align=\"left\"&gt;\n  &lt;td&gt;\n    &lt;a target=\"_blank\" href=\"{colab_link}\"&gt;\n      &lt;img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /&gt;Run in Google Colab\n    &lt;/a&gt;\n  &lt;/td&gt;\n  &lt;td&gt;\n    &lt;a href=\"{github_link}\"&gt;\n      &lt;img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /&gt;View on Github\n    &lt;/a&gt;\n  &lt;/td&gt;\n&lt;/table&gt;\n\n&lt;br clear=\"all\"&gt;\n\n\"\"\"\n\n            # Modify the markdown output of notebooks\n            full_md_path = os.path.join(config['site_dir'], md_path)\n\n            print(\"Modifying\", full_md_path)\n            if os.path.exists(full_md_path):\n                with open(full_md_path, \"r\", encoding=\"utf-8\") as f:\n                    content = f.read()\n                    # the content needs to be added before the first &lt;h1&gt; tag\n                    soup = BeautifulSoup(content, 'html.parser')\n                    h1 = soup.find(\"h1\")\n                    if h1:\n                        h1.insert_before(prepend_content)\n                    else:\n                        soup.insert(0, prepend_content)\n</pre> def on_files(files, config):     \"\"\"     Ensure all notebook files are processed and their Markdown versions include our header.     \"\"\"     for file in files:         if file.src_path.endswith(\".ipynb\"):             notebook_path = file.src_path             md_path = file.dest_path.replace(\".ipynb\", \".md\")              repo_url = \"https://github.com/filipinascimento/usable_ai/blob/main/\"             colab_url = \"https://colab.research.google.com/github/filipinascimento/usable_ai/blob/master/\"                          colab_link = f\"{colab_url}{notebook_path}\"             github_link = f\"{repo_url}{notebook_path}\"              prepend_content = f\"\"\"  Run in Google Colab      View on Github        \"\"\"              # Modify the markdown output of notebooks             full_md_path = os.path.join(config['site_dir'], md_path)              print(\"Modifying\", full_md_path)             if os.path.exists(full_md_path):                 with open(full_md_path, \"r\", encoding=\"utf-8\") as f:                     content = f.read()                     # the content needs to be added before the first  tag                     soup = BeautifulSoup(content, 'html.parser')                     h1 = soup.find(\"h1\")                     if h1:                         h1.insert_before(prepend_content)                     else:                         soup.insert(0, prepend_content)"},{"location":"m00-setup/class/","title":"Setup our environment","text":"<p>In this lecture, we will recap the key technologies needed for the Usable AI course and set up the development environment.</p> <p>We will cover the setup of essential tools for the course: Python and related packages. We will also discuss the basic usage of GitHub and Terminal commands.</p> <p>We recommend the use of Jupyter Notebooks, VSCode or Google Colab for the course.</p> <p>If you are new to these tools, follow the instructions below to set up your environment and check out the tutorials to get started.</p> <p>If you are unfamiliar with the terminal or Git, or if you would like to refresh your knowledge, we recommend visiting the other setup section of this course.</p>"},{"location":"m00-setup/class/#tutorials","title":"Tutorials:","text":"<ul> <li>Terminal Basics</li> <li>Git Basics</li> <li>Jupyter Basics</li> <li>VSCode Basics</li> <li>Google Colab Basics</li> </ul>"},{"location":"m00-setup/class/#cloning-the-repository","title":"Cloning the repository","text":"<p>Github is a platform where you can store your code and collaborate with others. Git is a version control system that allows you to track changes in your code. If something goes wrong, you can always revert to a previous version.</p> <p>To get started, you need to clone the repository to your local machine. This will create a copy of the repository on your computer.</p> <ol> <li>Installing GIT:</li> <li>Windows: Download Git for Windows and install. You can use Git Bash or Command Prompt after installation.</li> <li>macOS: Git often comes pre-installed. If not, install Xcode Command Line Tools (<code>xcode-select --install</code>) or get the official installer.</li> <li> <p>Linux: Install via your package manager, e.g. <code>sudo apt-get install git</code> (Debian/Ubuntu) or <code>sudo dnf install git</code> (Fedora).</p> </li> <li> <p>Configure Git (All Platforms): After installation, open a terminal and set your name and email for commits: <pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n</code></pre> You can check your configuration with: <pre><code>git config --list\n</code></pre></p> </li> <li> <p>Navigate to the Repository:</p> <ul> <li>Go to the github.com/filipinascimento/usable_ai repository on GitHub.</li> </ul> </li> <li> <p>Clone the Repository:</p> <ul> <li>Click on the \"Code\" button at the top right corner of the page.</li> <li>Copy the URL under \"Clone with HTTPS\".</li> <li>Open a terminal on your computer and run <code>git clone [URL]</code> (replace <code>[URL]</code> with the copied URL).</li> </ul> <p>Note: You can also use the download ZIP option if you don't want to use Git. Note: If you are using SSH, you can use the SSH URL instead of HTTPS.</p> </li> <li> <p>Navigate to the Repository:</p> <ul> <li>Go to the cloned repository on your local machine.</li> </ul> </li> </ol> <p>The files in the repository are now available on your local machine. You now have access to the course materials in your local environment.</p>"},{"location":"m00-setup/class/#updating-the-repository","title":"Updating the Repository","text":"<p>In case there are updates to the repository, you can pull the changes to your local machine. This will ensure that you have the latest version of the course materials.</p> <ol> <li> <p>Navigate to the Repository:</p> <ul> <li>Open a terminal and navigate to the cloned repository on your local machine.</li> </ul> </li> <li> <p>Pull the Changes:</p> <ul> <li>Run <code>git pull origin main</code> to pull the changes from the main branch of the repository.</li> </ul> </li> </ol> <p>The changes from the main branch will be merged into your local repository. You now have the latest version of the course materials on your local machine.</p>"},{"location":"m00-setup/class/#setting-up-the-python-environment","title":"Setting up the Python Environment","text":"<p>We suggest using Miniforge or Miniconda to install python packages and setup your environment. You can also use Anaconda, but it is a larger package and may take longer to install. Alternatively, you can also setup your own python environment using pip and virtualenv (this approach will not be covered in this document).</p>"},{"location":"m00-setup/class/#step-1-install-miniforge","title":"Step 1: Install Miniforge","text":"<p>Miniforge is a minimal installer for Conda, a package manager and an environment manager. Here's how to install it:</p> <ol> <li> <p>Download Miniforge:</p> <ul> <li>Visit the Miniforge download page.</li> <li>Choose the version suitable for your operating system (Windows, macOS, or Linux).</li> <li>Download the appropriate installer.</li> </ul> </li> <li> <p>Install Miniforge:</p> <ul> <li>Windows: Run the downloaded <code>.exe</code> file and follow the on-screen instructions.</li> <li>macOS/Linux: Open a terminal, navigate to the folder containing the downloaded file, and run <code>bash Miniforge3-MacOSX-arm64.sh</code> (adjust the filename as needed).</li> </ul> </li> <li> <p>Verify the Installation:</p> <ul> <li>Open a new terminal window.<ul> <li>On Windows, you can look for the Miniforge Prompt.</li> </ul> </li> <li>Type <code>conda list</code>. If Miniforge is installed correctly, you'll see a list of installed packages.</li> </ul> </li> </ol>"},{"location":"m00-setup/class/#step-2-create-a-conda-environment","title":"Step 2: Create a Conda Environment","text":"<p>Creating a separate environment for your Data Science projects is good practice:</p> <ol> <li> <p>Create a New Environment for this course:</p> <ul> <li>Run the command: <code>conda env create -f environment.yml</code>. This will create a new environment called usableai with all the necessary packages installed.</li> </ul> </li> <li> <p>Activate the Environment:</p> <ul> <li>Run: <code>conda activate usableai</code>.</li> </ul> </li> <li> <p>Launch Jupyter Lab:</p> <ul> <li>Run: <code>jupyter lab</code>.</li> <li>This will open Jupyter Lab in your default web browser.</li> </ul> </li> </ol>"},{"location":"m00-setup/class/#step-4-verify-installation","title":"Step 4: Verify Installation","text":"<p>Make sure everything is installed correctly:</p> <ol> <li> <p>Open a New Notebook in Jupyter Lab:</p> <ul> <li>In Jupyter Lab, create a new notebook.</li> </ul> </li> <li> <p>Test the Packages:</p> <ul> <li>Try importing the packages: <code>import numpy as np</code>, <code>import pandas as pd</code>, <code>import matplotlib.pyplot as plt</code>.</li> <li>If there are no errors, the packages are installed correctly.</li> </ul> </li> </ol>"},{"location":"m00-setup/class/#additional-tips","title":"Additional Tips","text":"<ul> <li>Updating Conda: Keep Conda and your packages updated with <code>conda update conda</code> and <code>conda update --all</code>.</li> <li>Managing Environments: View your environments with <code>conda env list</code> and switch between them using <code>conda activate &lt;env_name&gt;</code>.</li> <li>Finding Packages: To find available packages, use <code>conda search &lt;package_name&gt;</code>.</li> <li>Conda Cheat Sheet: For more commands, see the Conda Cheat Sheet.</li> </ul>"},{"location":"m00-setup/git_basics/","title":"GIT/github Basics","text":"<p>Git is a distributed version control system that allows you to track changes in your code, collaborate with others, and revert to previous versions if needed. This tutorial covers the basics of Git, including installation, configuration, creating a local repository, working with remotes, branching, merging, and more.</p> <p>In this tutorial, you will learn how to: - Install and set up Git on your computer. - Create a local repository and make your first commit. - Create a remote repository on GitHub and connect it to your local repo. - Clone an existing repository to your local machine. - Work with branches, merging, and rebasing. - Fork a repository on GitHub and keep your fork in sync with the original project.</p>"},{"location":"m00-setup/git_basics/#1-installing-and-setting-up-git","title":"1. Installing and Setting Up Git","text":""},{"location":"m00-setup/git_basics/#install-git","title":"Install Git","text":"<ul> <li>Windows: Download Git for Windows and install. You can use Git Bash or Command Prompt after installation.</li> <li>macOS: Git often comes pre-installed. If not, install Xcode Command Line Tools (<code>xcode-select --install</code>) or get the official installer.</li> <li>Linux: Install via your package manager, e.g. <code>sudo apt-get install git</code> (Debian/Ubuntu) or <code>sudo dnf install git</code> (Fedora).</li> </ul>"},{"location":"m00-setup/git_basics/#configure-git-all-platforms","title":"Configure Git (All Platforms)","text":"<p>After installation, set your name and email for commits: <pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n</code></pre> You can check your configuration with: <pre><code>git config --list\n</code></pre></p>"},{"location":"m00-setup/git_basics/#2-starting-a-local-repository","title":"2. Starting a Local Repository","text":"<p>You can create a new folder on your computer and turn it into a Git repository: <pre><code>mkdir my-new-project\ncd my-new-project\ngit init\n</code></pre> - <code>git init</code> creates a hidden <code>.git</code> folder that tracks your code changes.</p>"},{"location":"m00-setup/git_basics/#first-commit","title":"First Commit","text":"<ol> <li>Create a file, e.g. <code>README.md</code>.</li> <li>Add content to the file (e.g., \u201cThis is my new project\u201d).</li> <li> <p>Stage and commit it:     <pre><code>git add README.md\ngit commit -m \"Initial commit\"\n</code></pre></p> </li> <li> <p><code>git add</code> tells Git which files you want to include in the next commit.  </p> </li> <li><code>git commit -m \"message\"</code> creates a snapshot of the staged changes with a brief message.</li> </ol>"},{"location":"m00-setup/git_basics/#3-creating-a-remote-repository-on-github","title":"3. Creating a Remote Repository on GitHub","text":"<p>You can do this in two main ways:</p>"},{"location":"m00-setup/git_basics/#method-a-creating-on-githubs-website","title":"Method A: Creating on GitHub\u2019s Website","text":"<ol> <li>Go to github.com and log in.  </li> <li>Click the \u201cNew\u201d button or \u201c+\u201d \u2192 \u201cNew repository.\u201d  </li> <li>Name your repository (e.g., <code>my-new-project</code>) and optionally add a description.  </li> <li>Leave it public or private, depending on your needs.  </li> <li>Do not initialize with a README if you already have one locally.  </li> <li>Click \u201cCreate repository.\u201d  </li> <li>GitHub will display instructions with a remote URL (something like <code>https://github.com/YourUsername/my-new-project.git</code>).</li> </ol> <p>Connect your local repo to this remote: <pre><code>git remote add origin https://github.com/YourUsername/my-new-project.git\ngit branch -M main\ngit push -u origin main\n</code></pre> - <code>git remote add origin ...</code> links your local repo to the remote. - <code>git push -u origin main</code> uploads (pushes) your local main branch to GitHub and sets <code>origin/main</code> as the default remote branch to track.</p>"},{"location":"m00-setup/git_basics/#method-b-creating-a-repo-from-the-command-line-github-cli","title":"Method B: Creating a Repo from the Command Line (GitHub CLI)","text":"<p>If you have the GitHub CLI installed: <pre><code>gh repo create my-new-project --public --source=. --remote=origin --push\n</code></pre> - This automates the repo creation on GitHub and sets up the remote connection.</p>"},{"location":"m00-setup/git_basics/#4-cloning-an-existing-repository","title":"4. Cloning an Existing Repository","text":"<p>If you want to get a copy of an existing repo from GitHub (or another service): <pre><code>git clone https://github.com/SomeoneElse/some-repo.git\ncd some-repo\n</code></pre> - This downloads the repository to your local machine. - You\u2019ll have a <code>.git</code> folder ready, so you can start making commits right away.</p>"},{"location":"m00-setup/git_basics/#5-basic-git-workflow","title":"5. Basic Git Workflow","text":"<p>Once your local repository is set up and connected to a remote, the daily workflow typically looks like this:</p> <ol> <li> <p>Pull the latest changes to ensure your local copy is up to date:     <pre><code>git pull origin main\n</code></pre>     (Substitute <code>main</code> with the branch you\u2019re working on if needed.)</p> </li> <li> <p>Make changes (edit files, add new files, etc.).</p> </li> <li> <p>Stage changes:     <pre><code>git add .\n</code></pre>     or specify file names individually: <pre><code>git add my_file.py\n</code></pre></p> </li> <li> <p>Commit with a descriptive message:     <pre><code>git commit -m \"Add new feature X\"\n</code></pre></p> </li> <li> <p>Push your local commits to GitHub:     <pre><code>git push origin main\n</code></pre></p> </li> </ol>"},{"location":"m00-setup/git_basics/#6-branching-and-merging","title":"6. Branching and Merging","text":""},{"location":"m00-setup/git_basics/#creating-and-switching-to-a-branch","title":"Creating and Switching to a Branch","text":"<p>A branch is a separate line of development to safely work on new features or experiments.</p> <p><pre><code>git checkout -b my-feature\n# or:\n# git switch -c my-feature\n</code></pre> - <code>checkout -b</code> creates and switches you to the new branch. - Make your changes, then commit them as usual (<code>git add</code>, <code>git commit -m \"...\"</code>).</p>"},{"location":"m00-setup/git_basics/#merging-a-branch-into-main","title":"Merging a Branch into <code>main</code>","text":"<p>After you finish the feature and it\u2019s tested, you merge it back into <code>main</code>: 1. Switch back to <code>main</code>:     <pre><code>git checkout main\n# or git switch main\n</code></pre> 2. Merge the feature branch:     <pre><code>git merge my-feature\n</code></pre> 3. Push the updated <code>main</code> to the remote:     <pre><code>git push origin main\n</code></pre></p> <p>If you run into merge conflicts, Git will tell you which files have conflicting changes. Resolve them manually in your text editor by keeping or adjusting the lines you need, then stage and commit again.</p>"},{"location":"m00-setup/git_basics/#7-forking-a-repository","title":"7. Forking a Repository","text":"<p>Forking is a common way to contribute to someone else\u2019s project:</p> <ol> <li>On GitHub, go to the repository you want to fork.  </li> <li>Click the Fork button in the top-right corner.  </li> <li>GitHub creates a copy under your account, e.g., <code>github.com/YourUsername/some-repo</code>.  </li> <li>Clone your fork to your local machine:     <pre><code>git clone https://github.com/YourUsername/some-repo.git\n</code></pre></li> <li>Make changes, commit, and push to your fork:     <pre><code>git add .\ngit commit -m \"Fix typo / Add feature\"\ngit push origin main\n</code></pre></li> <li>Open a Pull Request on the original repository to propose your changes.</li> </ol>"},{"location":"m00-setup/git_basics/#8-getting-updates-from-upstream-keeping-your-fork-in-sync","title":"8. Getting Updates from \u201cUpstream\u201d (Keeping Your Fork in Sync)","text":"<p>Often, you\u2019ll want to keep your local fork up to date with the original project (\u201cupstream\u201d):</p> <ol> <li>Add the original repository as an upstream remote:     <pre><code>git remote add upstream https://github.com/OriginalAuthor/some-repo.git\n</code></pre></li> <li>Fetch updates from upstream:     <pre><code>git fetch upstream\n</code></pre></li> <li>Merge changes into your local <code>main</code> branch:     <pre><code>git checkout main\ngit merge upstream/main\n</code></pre></li> <li>Push those changes back to your fork on GitHub:     <pre><code>git push origin main\n</code></pre></li> </ol>"},{"location":"m00-setup/git_basics/#9-rebase-optional-advanced-topic","title":"9. Rebase (Optional Advanced Topic)","text":"<p>Rebasing is an alternative to merging that applies your commits on top of the latest changes from another branch, resulting in a cleaner, more linear history. It\u2019s optional but can be very helpful in maintaining a tidy commit log.</p> <ul> <li>Example (rebasing your feature branch onto the updated <code>main</code>):   <pre><code># Make sure your main is up to date:\ngit checkout main\ngit pull origin main\n\n# Switch to your feature branch:\ngit checkout my-feature\n\n# Rebase feature branch on top of the new main:\ngit rebase main\n</code></pre></li> <li>If there are conflicts, resolve them and then continue the rebase:   <pre><code>git add .\ngit rebase --continue\n</code></pre></li> <li>Finally, push your rebased branch. You may need <code>--force</code> if you\u2019ve rewritten commit history:   <pre><code>git push origin my-feature --force\n</code></pre> For beginners, merges are typically enough. Rebasing is optional and recommended once you\u2019re comfortable with Git basics, especially if the project enforces a rebased (linear) history.</li> </ul>"},{"location":"m00-setup/git_basics/#10-summary-of-common-commands","title":"10. Summary of Common Commands","text":"<ul> <li>Initialization: <code>git init</code></li> <li>Clone a repo: <code>git clone &lt;repo_url&gt;</code></li> <li>View status: <code>git status</code></li> <li>Stage changes: <code>git add .</code> (or file names)</li> <li>Commit: <code>git commit -m \"Message\"</code></li> <li>Push: <code>git push origin &lt;branch&gt;</code></li> <li>Pull: <code>git pull origin &lt;branch&gt;</code></li> <li>Check branches: <code>git branch</code></li> <li>Create/switch branch: <code>git checkout -b my-branch</code> or <code>git switch -c my-branch</code></li> <li>Merge: <code>git merge &lt;branch&gt;</code></li> <li>Rebase (advanced): <code>git rebase main</code></li> <li>Fork (on GitHub) \u2192 <code>git remote add upstream &lt;original_repo&gt;</code> \u2192 <code>git fetch upstream</code> \u2192 <code>git merge upstream/main</code></li> </ul>"},{"location":"m00-setup/google_colab/","title":"Google Colab Basics","text":"In\u00a0[\u00a0]: Copied! <pre>from google.colab import drive\nimport pandas as pd\n\ndrive.mount('/content/drive')\n</pre> from google.colab import drive import pandas as pd  drive.mount('/content/drive') In\u00a0[\u00a0]: Copied! <pre># Update the file path as necessary\nfile_path = '/content/drive/MyDrive/data/dataset.csv'\n\ntry:\n    df = pd.read_csv(file_path)\n    print(\"Data loaded successfully. Here are the first few rows:\")\n    print(df.head())\nexcept FileNotFoundError:\n    print(f\"Error: The file at {file_path} was not found. Please verify the path.\")\nexcept Exception as e:\n    print(\"An error occurred while loading the file:\", str(e))\n</pre> # Update the file path as necessary file_path = '/content/drive/MyDrive/data/dataset.csv'  try:     df = pd.read_csv(file_path)     print(\"Data loaded successfully. Here are the first few rows:\")     print(df.head()) except FileNotFoundError:     print(f\"Error: The file at {file_path} was not found. Please verify the path.\") except Exception as e:     print(\"An error occurred while loading the file:\", str(e)) In\u00a0[\u00a0]: Copied! <pre>!pip install seaborn\n</pre> !pip install seaborn In\u00a0[\u00a0]: Copied! <pre>import seaborn as sns\n# example usage:\nsns.lineplot(x=[1, 2, 3, 4], y=[1, 4, 9, 16])\n</pre> import seaborn as sns # example usage: sns.lineplot(x=[1, 2, 3, 4], y=[1, 4, 9, 16])"},{"location":"m00-setup/google_colab/#google-colab-basics","title":"Google Colab Basics\u00b6","text":"<p>Welcome to the Usable AI course! This notebook will help you get started with Google Colab, an interactive, cloud-based Jupyter environment. If you already have access to some notebooks, simply click on the Open in Colab links provided in your course materials. This is one of the options for running the course notebooks.</p>"},{"location":"m00-setup/google_colab/#using-google-colab","title":"Using Google Colab\u00b6","text":"<p>One option for this course is to use Google Colab.</p> <p>Google Colab allows you to write and execute Python code on remote servers. This means:</p> <ul> <li><p>No need to install Python or any libraries on your local machine.</p> </li> <li><p>Access to powerful computing resources directly from the browser.</p> </li> <li><p>Easy collaboration by sharing notebooks with peers.</p> </li> </ul> <p>While Google Colab offers a convenient cloud-based environment, keep in mind the following limitations:</p> <ul> <li><p>Sessions are temporary and may disconnect after a period of inactivity.</p> </li> <li><p>There are restrictions on GPU/TPU usage and RAM.</p> </li> <li><p>The runtime environment resets after disconnecting, which means unsaved work is lost.</p> </li> <li><p>Dependency management can be challenging if specific package versions are required.</p> </li> <li><p>You always need to install and import the necessary libraries at the beginning of each notebook.</p> </li> </ul> <p>For more details, check out the Google Colab Documentation.</p>"},{"location":"m00-setup/google_colab/#1-setting-up-the-colab-environment","title":"1. Setting Up the Colab Environment\u00b6","text":"<p>To begin:</p> <ul> <li><p>Open your Colab notebook from Google Colab.</p> </li> <li><p>You can also use the Open in Colab button provided by your course materials.</p> </li> <li><p>Once your notebook opens, you'll see this guide and can start running cells immediately.</p> </li> </ul>"},{"location":"m00-setup/google_colab/#2-mounting-google-drive","title":"2. Mounting Google Drive\u00b6","text":"<p>Many datasets in this course are stored on Google Drive. The cell below shows how to mount your drive:</p> <ol> <li><p>Run the cell.</p> </li> <li><p>Click on the link that appears.</p> </li> <li><p>Log into your Google account and authorize the access.</p> </li> <li><p>Copy and paste the provided authorization code back into the notebook.</p> </li> </ol>"},{"location":"m00-setup/google_colab/#3-importing-data-from-google-drive","title":"3. Importing Data from Google Drive\u00b6","text":"<p>After mounting your drive, you can access your data files. For example, to load a CSV file called <code>dataset.csv</code> located in <code>MyDrive/data/</code>, update the file path accordingly and run the cell below.</p> <p>This example includes a simple error handling block in case the file isn't found.</p>"},{"location":"m00-setup/google_colab/#4-installing-libraries","title":"4. Installing Libraries\u00b6","text":"<p>Google Colab comes with many popular libraries pre-installed. However, if you need to install additional packages, you can do so using <code>!pip install</code>.</p> <p>For example, to install the <code>seaborn</code> library, run the cell below:</p>"},{"location":"m00-setup/google_colab/#importing-installed-libraries","title":"Importing Installed Libraries\u00b6","text":"<p>After installing a library, you can import it in your notebook as usual:</p>"},{"location":"m00-setup/google_colab/#5-next-steps","title":"5. Next Steps\u00b6","text":"<p>Now that you've set up your environment:</p> <ul> <li><p>Explore the notebooks provided in the course.</p> </li> <li><p>Try running additional code cells and modifying examples.</p> </li> <li><p>Consider experimenting with different datasets and visualizations.</p> </li> <li><p>If you encounter issues, refer to the course documentation or ask for help.</p> </li> </ul> <p>Enjoy your journey with Usable AI and happy coding!</p>"},{"location":"m00-setup/jupyter_basics/","title":"Jupyter Lab Guide for Usable AI","text":""},{"location":"m00-setup/jupyter_basics/#this-guide-walks-you-through-using-jupyter-lab-for-your-usable-ai-projects-it-is-assumed-that-your-environment-is-already-set-up-with-the-necessary-packages-if-you-are-using-conda-and-prepared-the-environment-as-described-in-setup-guide-this-is-the-usableai-conda-environment","title":"This guide walks you through using Jupyter Lab for your Usable AI projects. It is assumed that your environment is already set up with the necessary packages. If you are using conda and prepared the environment as described in Setup Guide, this is the <code>usableai</code> conda environment.","text":""},{"location":"m00-setup/jupyter_basics/#1-launching-jupyter-lab","title":"1. Launching Jupyter Lab","text":""},{"location":"m00-setup/jupyter_basics/#using-conda-environment","title":"Using Conda Environment","text":"<p>If you're using the <code>usableai</code> conda environment: 1. Open your terminal. Remember that if you installed MiniForge on windows, you should use the Miniforge Prompt from the Start Menu. 2. Activate your environment:     <pre><code>conda activate usableai\n</code></pre> 3. Launch Jupyter Lab by running:     <pre><code>jupyter lab\n</code></pre></p>"},{"location":"m00-setup/jupyter_basics/#using-other-environments","title":"Using Other Environments","text":"<p>If you have set up your environment with virtualenv or another tool: 1. Activate your virtual environment (e.g., for virtualenv):     <pre><code>source path/to/your/virtualenv/bin/activate\n</code></pre> 2. Launch Jupyter Lab:     <pre><code>jupyter lab\n</code></pre></p> <p>Note: The command is the same regardless of which environment management tool you use, as long as all required packages are installed.</p>"},{"location":"m00-setup/jupyter_basics/#accessing-jupyter-lab-from-a-browser","title":"Accessing Jupyter Lab from a Browser","text":"<p>Jupyter Lab will open in your default web browser. If it doesn't, you can normally access it at <code>http://localhost:8888/lab</code>.</p>"},{"location":"m00-setup/jupyter_basics/#2-navigating-the-jupyter-lab-interface","title":"2. Navigating the Jupyter Lab Interface","text":"<p>When Jupyter Lab starts, you'll see: - File Browser: Browse directories and open notebooks, scripts, and other files. - Launcher: Easily open new notebooks, terminals, text files, and consoles. - Tabs and Panels: Organize multiple documents side by side.</p> <p>Spend time exploring the layout to customize your workflow.</p>"},{"location":"m00-setup/jupyter_basics/#3-creating-and-using-notebooks","title":"3. Creating and Using Notebooks","text":""},{"location":"m00-setup/jupyter_basics/#creating-a-new-notebook","title":"Creating a New Notebook","text":"<ol> <li>Click on the Launcher tab or choose File &gt; New &gt; Notebook from the top menu.</li> <li>Select the desired kernel. By default, it may pick the Python version installed in your environment.</li> </ol>"},{"location":"m00-setup/jupyter_basics/#running-cells","title":"Running Cells","text":"<ul> <li>Insert Code: Click on a cell to insert code.</li> <li>Run Cell: Press <code>Shift + Enter</code> to execute a cell.</li> <li>Insert Markdown: Change cell type to Markdown to add headings, explanations, or notes.</li> </ul>"},{"location":"m00-setup/jupyter_basics/#useful-shortcuts","title":"Useful Shortcuts","text":"<ul> <li>Run current cell: Shift + Enter</li> <li>Insert new cell below: B</li> <li>Insert new cell above: A</li> <li>Save Notebook: Ctrl + S (Cmd + S on macOS)</li> </ul>"},{"location":"m00-setup/jupyter_basics/#4-managing-kernels-and-environments","title":"4. Managing Kernels and Environments","text":""},{"location":"m00-setup/jupyter_basics/#switching-kernels","title":"Switching Kernels","text":"<p>If the notebook isn\u2019t using the desired kernel: 1. Click on Kernel &gt; Change Kernel... in the menu. 2. Select the correct kernel corresponding to your environment. 3. Confirm if the kernel has the same name as your environment (e.g., <code>Python (usableai)</code> if you are using our conda installation).</p>"},{"location":"m00-setup/jupyter_basics/#adding-new-kernels-optional","title":"Adding New Kernels (optional)","text":"<p>For custom environments, you might need to manually add a kernel using <code>ipykernel</code>: 1. Install <code>ipykernel</code> in your environment if it's not already installed.     <pre><code>pip install ipykernel\n</code></pre> 2. Add the kernel:     <pre><code>python -m ipykernel install --user --name my_env --display-name \"Python (my_env)\"\n</code></pre> 3. Restart Jupyter Lab and select the new kernel from Kernel &gt; Change Kernel...</p>"},{"location":"m00-setup/jupyter_basics/#5-executing-code-and-interrupting-the-kernel","title":"5. Executing code and interrupting the Kernel","text":"<p>You can execute code cells by pressing <code>Shift + Enter</code>. If you need to stop the execution of a cell or interrupt the kernel, you can use the following shortcuts: - Interrupt Kernel: Press <code>I</code> twice. You can also click on the stop button in the toolbar. This will stop the execution of the current cell but keep the kernel running. - Restart Kernel: Go to the Kernel menu and select Restart Kernel. Restarting the kernel will clear all variables and outputs.</p>"},{"location":"m00-setup/jupyter_basics/#6-advanced-features","title":"6. Advanced Features","text":""},{"location":"m00-setup/jupyter_basics/#extensions-and-plugins","title":"Extensions and Plugins","text":"<p>Jupyter Lab supports a rich ecosystem of extensions: - Variable Inspector: Monitor variables in your notebook. - Git Integration: Track changes and version control your projects. - Code Formatter: Automatically format your code with tools like Black.</p> <p>You can explore available extensions at the Jupyter Lab Extensions Repository.</p>"},{"location":"m00-setup/jupyter_basics/#terminal-and-text-editor","title":"Terminal and Text Editor","text":"<ul> <li>Terminal: Open an integrated terminal from File &gt; New &gt; Terminal.</li> <li>Text Editor: Edit plain text or configuration files directly within Jupyter Lab.</li> </ul>"},{"location":"m00-setup/jupyter_basics/#customizing-settings","title":"Customizing Settings","text":"<p>Access the Advanced Settings Editor under the Settings menu to customize themes, shortcuts, and other configurations.</p>"},{"location":"m00-setup/jupyter_basics/#7-tips-for-effective-use","title":"7. Tips for Effective Use","text":"<ul> <li>Regularly Save: Use keyboard shortcuts to save frequently. (<code>Ctrl + S</code> or <code>Cmd + S on macOS</code>)</li> <li>Organize Code into Functions: Keep your notebook clean by modularizing your code.</li> <li>Version Control Notebooks: Back up your notebooks using Git.</li> <li>Explore Commands: Use the command palette (<code>Ctrl + Shift + C</code> or <code>Cmd + Shift + C on macOS</code>) to discover powerful quick commands.</li> <li>Utilize Markdown Cells: Enhance your notebooks by using Markdown cells for documentation and notes.</li> <li>Keep Descriptive Titles: Use clear and descriptive titles for your notebooks to make them easily identifiable.</li> </ul>"},{"location":"m00-setup/jupyter_basics/#7-troubleshooting","title":"7. Troubleshooting","text":""},{"location":"m00-setup/jupyter_basics/#common-issues","title":"Common Issues","text":"<ul> <li>Kernel Not Found: Check if the environment corresponding to the kernel is active and all dependencies are installed.</li> <li>Slow Performance: Close unnecessary tabs and clear outputs when working with large datasets.</li> <li>Extension Issues: Disable or update extensions if you experience unexpected behavior.</li> </ul>"},{"location":"m00-setup/jupyter_basics/#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: Refer to the Jupyter Lab Documentation for more details.</li> <li>Community Forums: Engage in community discussions on forums like Stack Overflow for troubleshooting help.</li> <li>Instructor and AIs: Reach out to your instructor or AIs for assistance with specific issues.</li> </ul>"},{"location":"m00-setup/jupyter_basics/#conclusion","title":"Conclusion","text":"<p>Jupyter Lab is a versatile tool that supports interactive coding, combining code execution, rich text elements, and visualizations in a unified interface. Whether using the <code>usableai</code> conda environment or another setup, mastering Jupyter Lab will enhance your productivity in this course and beyond.</p> <p>Happy coding!</p>"},{"location":"m00-setup/terminal_basics/","title":"Terminal Basics","text":"<p>Below is a concise, beginner-friendly guide to using the command line (often called the \u201cterminal\u201d in macOS and Linux). The guide covers the basics for Windows, macOS, and Linux side-by-side. Mastering the terminal is vital for our Usable AI course as it empowers you to efficiently manage files, automate workflows, and interact with various AI tools and scripts.</p>"},{"location":"m00-setup/terminal_basics/#1-opening-the-command-line","title":"1. Opening the Command Line","text":""},{"location":"m00-setup/terminal_basics/#windows","title":"Windows","text":"<ul> <li>Command Prompt: Press <code>Win + R</code>, type <code>cmd</code>, and hit Enter.  </li> <li>PowerShell: Press <code>Win + X</code> and select \u201cWindows PowerShell\u201d (or just search for PowerShell in the Start menu).</li> </ul>"},{"location":"m00-setup/terminal_basics/#macos","title":"macOS","text":"<ul> <li>Terminal: Open \u201cApplications\u201d \u2192 \u201cUtilities\u201d \u2192 \u201cTerminal\u201d, or use Spotlight search (<code>Cmd + Space</code>) and type \u201cTerminal\u201d.</li> </ul>"},{"location":"m00-setup/terminal_basics/#linux","title":"Linux","text":"<ul> <li>Terminal: Typically found in the application menu under \u201cAccessories\u201d or \u201cSystem Tools\u201d. You can also use a keyboard shortcut like <code>Ctrl + Alt + T</code> on many distributions.</li> </ul>"},{"location":"m00-setup/terminal_basics/#2-basic-navigation-commands","title":"2. Basic Navigation Commands","text":"Action Windows macOS / Linux List files/folders <code>dir</code> <code>ls</code> Change directory <code>cd folder_name</code> <code>cd folder_name</code> Go up one directory <code>cd ..</code> <code>cd ..</code> Clear screen <code>cls</code> <code>clear</code>"},{"location":"m00-setup/terminal_basics/#examples","title":"Examples","text":"<ul> <li>List Files:  <ul> <li>Windows: <code>dir</code> </li> <li>macOS/Linux: <code>ls</code> </li> </ul> </li> <li>Change Directory (to <code>Documents</code> folder): <pre><code>cd Documents\n</code></pre></li> <li>Go Up One Level: <pre><code>cd ..\n</code></pre></li> </ul>"},{"location":"m00-setup/terminal_basics/#3-creating-and-managing-foldersfiles","title":"3. Creating and Managing Folders/Files","text":"Action Windows (Command Prompt/PowerShell) macOS / Linux Create a folder <code>mkdir folder_name</code> <code>mkdir folder_name</code> Remove an empty folder <code>rmdir folder_name</code> <code>rmdir folder_name</code> Remove a folder + contents <code>rmdir /s folder_name</code> (CMD)  <code>Remove-Item folder_name -Recurse -Force</code> (PowerShell) <code>rm -r folder_name</code>"},{"location":"m00-setup/terminal_basics/#examples_1","title":"Examples","text":"<ul> <li>Create Folder: <pre><code>mkdir MyProject\n</code></pre></li> <li>Remove Folder:  <ul> <li>Windows Command Prompt:   <pre><code>rmdir /s MyProject\n</code></pre></li> <li>PowerShell:   <pre><code>Remove-Item MyProject -Recurse -Force\n</code></pre></li> <li>macOS/Linux:   <pre><code>rm -r MyProject\n</code></pre></li> </ul> </li> </ul>"},{"location":"m00-setup/terminal_basics/#4-viewing-and-editing-files","title":"4. Viewing and Editing Files","text":"Action Windows (Command Prompt/PowerShell) macOS / Linux Show file contents <code>type filename.txt</code> <code>cat filename.txt</code> Open file in text editor Use Notepad or similar:  <code>notepad filename.txt</code> Use Nano/Vim/gedit etc.:  <code>nano filename.txt</code>"},{"location":"m00-setup/terminal_basics/#examples_2","title":"Examples","text":"<ul> <li>Display Contents:<ul> <li>Windows: <code>type README.txt</code></li> <li>macOS/Linux: <code>cat README.txt</code></li> </ul> </li> <li>Edit File:<ul> <li>Windows: <code>notepad README.txt</code></li> <li>macOS/Linux: <code>nano README.txt</code> (or <code>vim</code>, <code>gedit</code>, etc.)</li> </ul> </li> </ul>"},{"location":"m00-setup/terminal_basics/#5-copying-moving-and-deleting-files","title":"5. Copying, Moving, and Deleting Files","text":"Action Windows macOS / Linux Copy a file <code>copy old.txt new.txt</code> <code>cp old.txt new.txt</code> Move/rename a file <code>move old.txt new.txt</code> <code>mv old.txt new.txt</code> Delete a file <code>del filename.txt</code> <code>rm filename.txt</code>"},{"location":"m00-setup/terminal_basics/#examples_3","title":"Examples","text":"<ul> <li>Copy a File:  <ul> <li>Windows: <code>copy report.txt backup_report.txt</code></li> <li>macOS/Linux: <code>cp report.txt backup_report.txt</code></li> </ul> </li> <li>Move/Rename a File:  <ul> <li>Windows: <code>move data.csv archived_data.csv</code></li> <li>macOS/Linux: <code>mv data.csv archived_data.csv</code></li> </ul> </li> <li>Delete a File:  <ul> <li>Windows: <code>del old_data.csv</code></li> <li>macOS/Linux: <code>rm old_data.csv</code></li> </ul> </li> </ul>"},{"location":"m00-setup/terminal_basics/#6-piping-and-redirection","title":"6. Piping and Redirection","text":"<p>Often, you\u2019ll want to chain commands together or redirect output to a file.</p> Action Windows (Command Prompt/PowerShell) macOS / Linux Redirect output to a file <code>command &gt; output.txt</code> <code>command &gt; output.txt</code> Append output to a file <code>command &gt;&gt; output.txt</code> <code>command &gt;&gt; output.txt</code> Pipe output (send to another command) <code>command1 | command2</code> <code>command1 \\| command2</code>"},{"location":"m00-setup/terminal_basics/#examples_4","title":"Examples","text":"<ul> <li> <p>Redirect output: <pre><code>dir &gt; files_list.txt    # Windows\nls &gt; files_list.txt     # macOS/Linux\n</code></pre>     This saves the output to <code>files_list.txt</code> instead of displaying it on the screen.</p> </li> <li> <p>Pipe output:  </p> <ul> <li>Windows: <code>dir | findstr \".py\"</code> </li> <li>macOS/Linux: <code>ls | grep \".py\"</code> This finds all Python files in the current directory.</li> </ul> </li> </ul>"},{"location":"m00-setup/terminal_basics/#7-additional-tips","title":"7. Additional Tips","text":"<ul> <li>Up/Down Arrow Keys: Cycle through your command history to reuse or edit previous commands.  </li> <li>Tab Completion: Start typing a folder or file name and press <code>Tab</code> to auto-complete it (works in most shells).  </li> <li>Help: Use <code>--help</code> or <code>-h</code> after commands to see help information. Windows commands often use <code>/?</code> instead.</li> <li>Case Sensitivity: Windows commands can be case-insensitive, while macOS/Linux commands are case-sensitive.</li> <li>Historical Commands: Use <code>history</code> (Linux or macOS) or <code>doskey /history</code> (Windows) to view past commands.</li> <li>Permissions: Sometimes you may find permissions issues when running commands. In such cases, you may need to run the command with elevated permissions (e.g., <code>sudo</code> on Linux/macOS or as an administrator on Windows). This should be done with caution, especially when deleting files or changing system settings. You may also need to change the permissions of a file or folder; refer to the documentation of your operating system for more information.<ul> <li>Additional Command: Use <code>chmod</code> (Linux/macOS) to change file permissions as needed.</li> <li>File Ownership Command: Use <code>chown</code> (Linux/macOS) to change file ownership if necessary.</li> </ul> </li> </ul>"},{"location":"m00-setup/terminal_basics/#8-more-resources","title":"8. More Resources","text":"<ul> <li>Windows Command Prompt Documentation</li> <li>macOS Terminal Documentation</li> <li>Linux Command Line Basics</li> <li>Introduction to permissions in Linux</li> </ul>"},{"location":"m00-setup/vscode_basics/","title":"VS Code Start Guide","text":""},{"location":"m00-setup/vscode_basics/#introduction","title":"Introduction","text":"<p>Visual Studio Code is a practical choice for our Usable AI course. It supports running Python notebooks and provides essential tools for project management through its integrated GitHub features. This approach makes managing code and projects straightforward without unnecessary complexity. You can also customize the editor to suit your preferences and workflow and use it across different operating systems, even remotely.</p>"},{"location":"m00-setup/vscode_basics/#1-downloading-vs-code","title":"1. Downloading VS Code","text":"<p>To get started, download the latest version of VS Code from the official website: - Visit https://code.visualstudio.com and select the installer for your operating system. - Follow the installation instructions provided on the site. - To use python, you need to have set up Python on your machine. You can follow the instructions in the Setup Lecture.</p>"},{"location":"m00-setup/vscode_basics/#2-signing-in-for-enhanced-features","title":"2. Signing In for Enhanced Features","text":"<p>After installation: - Open VS Code and use the built-in account manager to sign in with your GitHub account. - Signing in unlocks access to repository management directly within the editor, integrates GitHub Copilot for AI-driven code suggestions, and synchronizes your settings across devices. - For further guidance, check the GitHub Authentication Guide.</p>"},{"location":"m00-setup/vscode_basics/#3-installing-essential-extensions","title":"3. Installing Essential Extensions","text":"<p>Enhance VS Code functionality by installing key extensions: - Python Extension: Provides powerful language support, debugging, and linting. Learn more at Python in VS Code. - Jupyter Extension: Enables interactive coding, allowing you to create, open, and execute Jupyter Notebooks. For further instructions, visit Using Jupyter in VS Code. - You may additionally explore other extensions related to your projects through the marketplace.</p>"},{"location":"m00-setup/vscode_basics/#4-key-features","title":"4. Key Features","text":"<p>VS Code comes with a range of essential features: - Jupyter Notebooks Integration: Work seamlessly with <code>.ipynb</code> files, allowing interactive data exploration and visualization. - Integrated Terminal: Launch a terminal within VS Code to execute commands, run scripts, or manage version control without leaving the editor. - Remote Development: Use extensions like Remote - SSH to connect to remote servers, containers, or WSL, enabling you to develop on powerful machines from a lightweight local editor. - Customizable User Interface: Tailor your workspace with themes, layouts, and keyboard shortcuts to create a personalized development environment.</p>"},{"location":"m00-setup/vscode_basics/#5-additional-resources-guides","title":"5. Additional Resources &amp; Guides","text":"<p>Further reading and resources to help you master VS Code: - VS Code Official Documentation - Getting Started with VS Code - Remote Development in VS Code - VS Code Tips and Tricks</p>"},{"location":"m01-python_basics1/python_basics1/","title":"Module 1 - Python Basics 1","text":"<p>Are you still new to Python? Or your python skill is a bit rusty?</p> <p>Then, feel free to use this to learn or refresh Python basics! You don't have to submit this notebook. It is just for your own reference.</p> <p>We begin by focusing on constructing and executing Python scripts. This fundamental skill allows you to instruct your computer to perform a multitude of tasks. As you master this technique, you will be able to transform abstract ideas into executable programs, opening up a world of possibilities for automation and efficient problem-solving.</p> <p>Moving forward, we will dive into loops and conditional statements. These powerful constructs enable your programs to dynamically react to varying scenarios and diverse data inputs. Leveraging these tools allows for the creation of responsive and adaptive code, which is essential for tackling complex challenges.</p> <p>Towards the end of the module, we'll explore the concept of reusable functions. Crafting well-structured functions not only enhances the modularity and reusability of your code, but also simplifies maintenance. This skill is paramount for reducing redundancy, streamlining debugging, and ensuring that your code remains clear and adaptable.</p> <p>You can comment in Python using <code>#</code>. Comments are ignored by the Python interpreter.</p> <p>To define a variable, assign a value to a variable name. Python will automatically detect the data type. The syntax is <code>variable_name = value</code>. You can print a variable using the <code>print(variable_name)</code> function.</p> In\u00a0[\u00a0]: Copied! <pre># This is a comment and will be ignored by the interpreter\na_variable = \"the value\"\n# This is another comment and will also be ignored by the interpreter\nprint(a_variable)\n</pre> # This is a comment and will be ignored by the interpreter a_variable = \"the value\" # This is another comment and will also be ignored by the interpreter print(a_variable)  <p>Variables can be reassigned with a different value and data type. For example, you can change an integer to a string:</p> In\u00a0[\u00a0]: Copied! <pre>a_variable = 10\nprint(a_variable)  # Output: 10\na_variable = \"Now I'm a string!\"\nprint(a_variable)  # Output: Now I'm a string!\n</pre> a_variable = 10 print(a_variable)  # Output: 10 a_variable = \"Now I'm a string!\" print(a_variable)  # Output: Now I'm a string!  <p>Q: Create two variables and then swap their values. Print the variables before and after the swap.</p> <ul> <li>For example if <code>a = 1</code> and <code>b = 2</code>, after the swap <code>a = 2</code> and <code>b = 1</code>.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># TODO: Write your code here\n</pre> # TODO: Write your code here  In\u00a0[\u00a0]: Copied! <pre>print(\"Hello, World!\")  # Output: Hello, World!\n\n# Example of print with concatenation\na_planet = \"Earth\"\nprint(\"Hello from \" + a_planet + \"!\")\n# Output: Hello from Earth!\n\n# Example of print with comma separated values\na_planet = \"Mars\"\nanother_planet = \"Jupiter\"\nprint(\"Hello from\", a_planet, \"and\", another_planet + \"!\")\n# Output: Hello from Mars and Jupiter!\n</pre> print(\"Hello, World!\")  # Output: Hello, World!  # Example of print with concatenation a_planet = \"Earth\" print(\"Hello from \" + a_planet + \"!\") # Output: Hello from Earth!  # Example of print with comma separated values a_planet = \"Mars\" another_planet = \"Jupiter\" print(\"Hello from\", a_planet, \"and\", another_planet + \"!\") # Output: Hello from Mars and Jupiter!   <p>Another way to format text is using the f-string method. You can use f-string by adding an <code>f</code> before the string and then use curly braces <code>{}</code> to insert variables.</p> In\u00a0[\u00a0]: Copied! <pre># Example of f-string print\na_planet = \"Earth\"\nanother_planet = \"Mars\"\nprint(f\"Hello from {a_planet} and {another_planet}!\")  # Output: Hello from Earth and Mars!\n</pre> # Example of f-string print a_planet = \"Earth\" another_planet = \"Mars\" print(f\"Hello from {a_planet} and {another_planet}!\")  # Output: Hello from Earth and Mars!  In\u00a0[\u00a0]: Copied! <pre># Declaring an integer and a float variable\nx = 10 \ny = 3.0\nk = x + y + 100\n\n# The value stored in the variable can be inspected by using print statement. \n# Type of a variable var can be checked by calling type(var) \nprint(\"The value of x is\", x, \"and it is of type\", type(x))\n\n# f-strings can be used to write more elegant print statement. \nprint(f\"The value of y is {y} and it is of type {type(y)}\")\nprint(f\"The value of k is {k} and it is of type {type(k)}\")\n\n# casting int to float\nprint(f\"x is of type {type(x)}\")\nx = float(x)\nprint(f\"x is of type {type(x)} after casting\")\n</pre> # Declaring an integer and a float variable x = 10  y = 3.0 k = x + y + 100  # The value stored in the variable can be inspected by using print statement.  # Type of a variable var can be checked by calling type(var)  print(\"The value of x is\", x, \"and it is of type\", type(x))  # f-strings can be used to write more elegant print statement.  print(f\"The value of y is {y} and it is of type {type(y)}\") print(f\"The value of k is {k} and it is of type {type(k)}\")  # casting int to float print(f\"x is of type {type(x)}\") x = float(x) print(f\"x is of type {type(x)} after casting\")  <p>Numbers can also be written in different formats. For example, you can use scientific notation: <code>z = 1.23e-4</code> (equivalent to 0.000123).</p> In\u00a0[\u00a0]: Copied! <pre># Numbers can also be written in scientific notation\nx = 1.05e6\nprint(f\"x is {x} and it is of type {type(x)}\")\n\n# You can also use _ to make the number more readable\nx = 1_000_000\nprint(f\"x is {x} and it is of type {type(x)}\")\n</pre> # Numbers can also be written in scientific notation x = 1.05e6 print(f\"x is {x} and it is of type {type(x)}\")  # You can also use _ to make the number more readable x = 1_000_000 print(f\"x is {x} and it is of type {type(x)}\")  <p>You can perform arithmetic operations on numbers. The basic arithmetic operators are <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>//</code> (integer division), and <code>%</code> (modulo).</p> In\u00a0[\u00a0]: Copied! <pre># Arithmetic operators\n\n# Addition\nz = x + y\nprint(f\"Adding x and y gives {z}\")\n\n# Subtraction\nz = x - y\nprint(f\"Subtracting y from x gives {z}\")\n\n# Multiplication\nz = x * y\nprint(f\"Multiplying x and y gives {z}\")\n\n# Division\nz = x / y \nprint(f\"x divided by y gives {z}\") \n\n# Floor Division\nz = x // y \nprint(f\"x divided by y gives {z} as quotient\") \n\n# Modulus Operator\nz = x % y \nprint(f\"x divided by y gives {z} as reminder\")\n\n# Exponentiation\nz = x ** y\nprint(f\"x raised to y gives {z}\") \n\n# self increment by 1\nx = x + 1\n# This is equivalent to x += 1\nprint(f\"x + 1 gives {x}\")\n</pre> # Arithmetic operators  # Addition z = x + y print(f\"Adding x and y gives {z}\")  # Subtraction z = x - y print(f\"Subtracting y from x gives {z}\")  # Multiplication z = x * y print(f\"Multiplying x and y gives {z}\")  # Division z = x / y  print(f\"x divided by y gives {z}\")   # Floor Division z = x // y  print(f\"x divided by y gives {z} as quotient\")   # Modulus Operator z = x % y  print(f\"x divided by y gives {z} as reminder\")  # Exponentiation z = x ** y print(f\"x raised to y gives {z}\")   # self increment by 1 x = x + 1 # This is equivalent to x += 1 print(f\"x + 1 gives {x}\")  In\u00a0[\u00a0]: Copied! <pre># True and False are the key words that represent bool values in python\na = True\nb = False\n\nprint(f\"a is {a} and b is {b}\")\nprint(f\"Type of variable a and b is {type(a)}\")\n\n# None in python represents the absence of something; similar to null value\nc = None \nprint(f\"c is {c} and is of type {type(c)}\")\n\n# Any non-zero integer value is true and zero is false.\n# Also anything with a non-zero length is true and empty sequences are false.\n</pre> # True and False are the key words that represent bool values in python a = True b = False  print(f\"a is {a} and b is {b}\") print(f\"Type of variable a and b is {type(a)}\")  # None in python represents the absence of something; similar to null value c = None  print(f\"c is {c} and is of type {type(c)}\")  # Any non-zero integer value is true and zero is false. # Also anything with a non-zero length is true and empty sequences are false.  <p>You can use logical operators <code>and</code>, <code>or</code>, and <code>not</code> to combine boolean values.</p> In\u00a0[\u00a0]: Copied! <pre># logical operators \n\n# and, or and not operate on bool variables\n# OR operator: Gives True when either of the expressions evaluates to True\n# expr1 or expr2\nprint(f\"a or b is {a or b}\")\nprint(f\"a or a is {a or a}\")\nprint(f\"b or b is {b or b}\")\n\n# AND operator: Gives True when both the expressions evaluates to True\n# expr1 and expr2\nprint(f\"a and b is {a and b}\")\nprint(f\"a and a is {a and a}\")\nprint(f\"b and b is {b and b}\")\n\n# NOT operator: negates a bool\n# not expr1\nprint(f\"Not of a is {not a}\")\n</pre> # logical operators   # and, or and not operate on bool variables # OR operator: Gives True when either of the expressions evaluates to True # expr1 or expr2 print(f\"a or b is {a or b}\") print(f\"a or a is {a or a}\") print(f\"b or b is {b or b}\")  # AND operator: Gives True when both the expressions evaluates to True # expr1 and expr2 print(f\"a and b is {a and b}\") print(f\"a and a is {a and a}\") print(f\"b and b is {b and b}\")  # NOT operator: negates a bool # not expr1 print(f\"Not of a is {not a}\")  <p>Comparison operators result in boolean values. The comparison operators are <code>==</code> (equal), <code>!=</code> (not equal), <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, and <code>&lt;=</code>.</p> <p>You can also combine comparison operators to create more complex conditions, such as <code>x &gt; 5 and x &lt; 10</code>, which checks if <code>x</code> is between 5 and 10.</p> <p>Additionally, logical operators can be used in conjunction with comparison operators to create more complex conditions, for example: <code>if x &gt; 5 and x &lt; 10: print(\"x is between 5 and 10\")</code>.</p> In\u00a0[\u00a0]: Copied! <pre># comparison operators\n\nx = 10\ny = 3.0\nz = 5\n\n# greater that, less than, greater than equal to and lesser than equal to\nx &gt; y\nx &gt;= y\nx &lt; y\nx &lt;= y\n\n# equals and not equals\nx == y\nx != y\n\n# Chained Expressions \nx &gt; y &gt; z \n(x &gt; y) or (x &gt; z)\n</pre> # comparison operators  x = 10 y = 3.0 z = 5  # greater that, less than, greater than equal to and lesser than equal to x &gt; y x &gt;= y x &lt; y x &lt;= y  # equals and not equals x == y x != y  # Chained Expressions  x &gt; y &gt; z  (x &gt; y) or (x &gt; z)  In\u00a0[\u00a0]: Copied! <pre># strings are represented using single or double quotes\nfirst_name = \"Adam\" \nlast_name = 'Eve'\n\n# \\ is used to escape characters\nmiddle_name = 'zero\\'s'\n\n# string concatenation\nfull_name = first_name +' ' + middle_name + ' ' + last_name\n\nprint(f\"Full name is {full_name}\")\nprint(f\"Full name is of type {type(full_name)}\")\n\n# length of string\nlength = len(full_name)\nprint(f\"Length of full name is {length}\")\n</pre> # strings are represented using single or double quotes first_name = \"Adam\"  last_name = 'Eve'  # \\ is used to escape characters middle_name = 'zero\\'s'  # string concatenation full_name = first_name +' ' + middle_name + ' ' + last_name  print(f\"Full name is {full_name}\") print(f\"Full name is of type {type(full_name)}\")  # length of string length = len(full_name) print(f\"Length of full name is {length}\")  <p>Strings can be indexed and sliced similar to lists and tuples.</p> <p>List indexing is discussed in the list section</p> <p>You can 'multiply' a string by an integer to repeat it.</p> In\u00a0[\u00a0]: Copied! <pre>repeated_string = \"Hello \" * 10\n\nprint(f\"repeated_string is {repeated_string}\")\n</pre> repeated_string = \"Hello \" * 10  print(f\"repeated_string is {repeated_string}\")  <p>Some characters have special meanings in strings. For example, <code>\\n</code> represents a newline character, and <code>\\t</code> represents a tab character. You can use <code>r</code> before the string to treat it as a raw string, ignoring special characters.</p> In\u00a0[\u00a0]: Copied! <pre># characters with special meaning\n# \\n - newline\n# \\t - tab\n# \\\\ - backslash\n# examples\nprint(\"Hello\\nWorld\")\nprint(\"Hello\\tWorld\")\nprint(\"Hello\\\\World\")\n</pre> # characters with special meaning # \\n - newline # \\t - tab # \\\\ - backslash # examples print(\"Hello\\nWorld\") print(\"Hello\\tWorld\") print(\"Hello\\\\World\")  <p>Casting is the process of converting one data type to another. You can cast strings to integers or floats, and vice versa. For example, you can convert a string to an integer using <code>int(\"5\")</code>.</p> In\u00a0[\u00a0]: Copied! <pre># casting str to int  \ntotal = int('1') + int('2')\nprint(f\"The value of total is {total} and it is of type {type(total)}\")\n\n# casting int to str\ntotal = str(1) + str(2)\nprint(f\"The value of total is {total} and it is of type {type(total)}\")\n</pre> # casting str to int   total = int('1') + int('2') print(f\"The value of total is {total} and it is of type {type(total)}\")  # casting int to str total = str(1) + str(2) print(f\"The value of total is {total} and it is of type {type(total)}\")  In\u00a0[\u00a0]: Copied! <pre># empty string is considered as False\nempty_string = ''\nbool(empty_string)\n</pre> # empty string is considered as False empty_string = '' bool(empty_string)  <p>To replace a part of a string, you can use the <code>replace()</code> method. To split a string into a list of substrings, you can use the <code>split()</code> method.</p> In\u00a0[\u00a0]: Copied! <pre># replace method\ns = \"Hello, World!\"\ns = s.replace(\"World\", \"Universe\")\nprint(s)\n</pre> # replace method s = \"Hello, World!\" s = s.replace(\"World\", \"Universe\") print(s)  <p>Note that the result of the f-string composition is a string.</p> In\u00a0[\u00a0]: Copied! <pre># f-string composition is a string\nhelloworld = f\"Hello, World!\"\nfull_sentence = f\"This is a full sentence. {helloworld} and it is of type {type(helloworld)}\"\nprint(full_sentence)\n</pre> # f-string composition is a string helloworld = f\"Hello, World!\" full_sentence = f\"This is a full sentence. {helloworld} and it is of type {type(helloworld)}\" print(full_sentence)  <p>Q: Create a sentence about the weather using variables for the temperature and the weather condition.</p> <ul> <li>Print the sentence</li> <li>Example: \"The temperature is 25 degrees and it is sunny.\"</li> <li>Then use replace to change the weather condition to \"cloudy\" and print the new sentence.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># TODO: Write your code here\n</pre> # TODO: Write your code here  In\u00a0[\u00a0]: Copied! <pre># Arrays are implemented as lists in python\n\n# creating empty list\nnames = []\nnames = list()\n\n# list of strings\nnames = ['Zach', 'Jay']\nprint(names)\n\n# list of intergers\nnums = [1, 2, 3, 4, 5]\nprint(nums)\n\n# list of different data types\nl = ['Zach', 1, True, None]\nprint(l)\n\n# list of lists\nll = [[1, 3], [2, 3], [3, 4]]\n\n# finding the length of list\nlength = len(l)\nprint(length)\n</pre> # Arrays are implemented as lists in python  # creating empty list names = [] names = list()  # list of strings names = ['Zach', 'Jay'] print(names)  # list of intergers nums = [1, 2, 3, 4, 5] print(nums)  # list of different data types l = ['Zach', 1, True, None] print(l)  # list of lists ll = [[1, 3], [2, 3], [3, 4]]  # finding the length of list length = len(l) print(length)  <p>You can repeat a list or tuple by multiplying it by an integer.</p> In\u00a0[\u00a0]: Copied! <pre># You can repeat a list or tuple by multiplying it by an integer.\nl = [1, 2, 3] * 3\nprint(l)\n\nt = (1, 2, 3) * 3\nprint(t)\n</pre> # You can repeat a list or tuple by multiplying it by an integer. l = [1, 2, 3] * 3 print(l)  t = (1, 2, 3) * 3 print(t)  <p>You can change the content of a list by assigning new values to its elements. You can also add elements to a list using the <code>append()</code> method or extend a list with another list using the <code>extend()</code> method.</p> In\u00a0[\u00a0]: Copied! <pre># Lists are mutable\n\nnames = names + ['Ravi']\nnames.append('Richard')\nnames.extend(['Abi', 'Kevin'])\nprint(names)\n</pre> # Lists are mutable  names = names + ['Ravi'] names.append('Richard') names.extend(['Abi', 'Kevin']) print(names)  <p>An element or a subset of a list can be accessed using element's index or slice of indices.</p> <p>The same notation applies for strings but at the character level.</p> In\u00a0[\u00a0]: Copied! <pre># some_list[index]\n# some_list[start_index: end_index(not included)]\n\nnumbers = [0, 1, 2, 3, 4, 5, 6]\n\n# indices start from 0 in python\nprint(f'The first element in numbers is {numbers[0]}')\nprint(f'The third element in numbers is {numbers[2]}')\n\nprint(f'Elements from 1st to 5th index are {numbers[1:6]}')\nprint(f'Elements from start to 5th index are {numbers[:6]}')\nprint(f'Elements from 4th index to end are {numbers[4:]}')\n\nprint(f'Last Element is {numbers[-1]}')\nprint(f'Last four element are {numbers[-4:]}')\n\n# changing 1st element in the numbers list\nnumbers[0] = 100\nprint(numbers)\n\n# changing first 3 numbers\nnumbers[0: 3] = [100, 200, 300]\nprint(numbers)\n</pre> # some_list[index] # some_list[start_index: end_index(not included)]  numbers = [0, 1, 2, 3, 4, 5, 6]  # indices start from 0 in python print(f'The first element in numbers is {numbers[0]}') print(f'The third element in numbers is {numbers[2]}')  print(f'Elements from 1st to 5th index are {numbers[1:6]}') print(f'Elements from start to 5th index are {numbers[:6]}') print(f'Elements from 4th index to end are {numbers[4:]}')  print(f'Last Element is {numbers[-1]}') print(f'Last four element are {numbers[-4:]}')  # changing 1st element in the numbers list numbers[0] = 100 print(numbers)  # changing first 3 numbers numbers[0: 3] = [100, 200, 300] print(numbers)  <p>Q: Define a list of four items representing your favorite hobbies/activities.</p> <ul> <li>Print only the second and third items using list slicing.</li> </ul> In\u00a0[1]: Copied! <pre># TODO: Write your code here\n</pre> # TODO: Write your code here  <p>On the other hand, tuples are immutable, meaning that you cannot change the content of a tuple once it is created.</p> In\u00a0[\u00a0]: Copied! <pre># Tuples are immutable lists. They are created using () instead of [].\n\nnames = tuple()\nnames = ('Zach', 'Jay') \nprint(names[0])\n\n# trying to alter the tuple gives an error\nnames[0] = 'Richard'\n\n# similar to tuples, strings are also immutable\n</pre> # Tuples are immutable lists. They are created using () instead of [].  names = tuple() names = ('Zach', 'Jay')  print(names[0])  # trying to alter the tuple gives an error names[0] = 'Richard'  # similar to tuples, strings are also immutable  In\u00a0[\u00a0]: Copied! <pre># empty lists and tuples are considered as False\nemptyList = []\nemptyTuple = ()\nprint(f\"emptyList is {bool(emptyList)} and emptyTuple is {bool(emptyTuple)}\")\n</pre> # empty lists and tuples are considered as False emptyList = [] emptyTuple = () print(f\"emptyList is {bool(emptyList)} and emptyTuple is {bool(emptyTuple)}\") <p>The <code>split()</code> method can be used to split a string into a list of substrings. By default, it splits the string by spaces, but you can specify a different separator.</p> In\u00a0[\u00a0]: Copied! <pre># split method\ns = \"Hello, World!\"\nwords = s.split()\nprint(words)\n\n# split with a different separator\ns = \"apple,banana,orange\"\nfruits = s.split(',')\nprint(fruits)\n</pre> # split method s = \"Hello, World!\" words = s.split() print(words)  # split with a different separator s = \"apple,banana,orange\" fruits = s.split(',') print(fruits)  <p>You can use the <code>join()</code> method to concatenate a list of strings into a single string.</p> In\u00a0[\u00a0]: Copied! <pre># join method\nwords = ['Hello', 'World']\ns = ' '.join(words)\nprint(s)\n</pre> # join method words = ['Hello', 'World'] s = ' '.join(words) print(s)  <p>Q: See what happens when you try to change the content of a tuple.</p> <ul> <li>Since you cannot change the content of a tuple, you need to create a new tuple with the new content.</li> <li>Create a new tuple with 3 elements based on the first three elements of the tuple you defined above.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># TODO: Write your code here\n</pre> # TODO: Write your code here  In\u00a0[\u00a0]: Copied! <pre># hash maps in python are called Dictionaries\n# dict{key: value}\n\n# Empty dictionary\nphonebook = dict()\n\n# contruction dict using sequences of key-value pairs\ndict([('sape', 4139), ('guido', 4127), ('jack', 4098)])\n\n# Dictionary with one item\nphonebook = {'jack': 4098}\n\n# Add another item\nphonebook['guido'] = 4127\n\nprint(phonebook)\nprint(phonebook['jack'])\nprint(phonebook.items())\nprint(phonebook.keys())\nprint(phonebook.values())\n\nprint('jack' in phonebook)\nprint('Kevin' in phonebook)\n\n# Delete an item\ndel phonebook['jack'] \nprint(phonebook)\n</pre> # hash maps in python are called Dictionaries # dict{key: value}  # Empty dictionary phonebook = dict()  # contruction dict using sequences of key-value pairs dict([('sape', 4139), ('guido', 4127), ('jack', 4098)])  # Dictionary with one item phonebook = {'jack': 4098}  # Add another item phonebook['guido'] = 4127  print(phonebook) print(phonebook['jack']) print(phonebook.items()) print(phonebook.keys()) print(phonebook.values())  print('jack' in phonebook) print('Kevin' in phonebook)  # Delete an item del phonebook['jack']  print(phonebook)  In\u00a0[\u00a0]: Copied! <pre># An empty dictionary is considered as False\nemptyDict = {}\nprint(f\"emptyDict is {bool(emptyDict)}\")\n</pre> # An empty dictionary is considered as False emptyDict = {} print(f\"emptyDict is {bool(emptyDict)}\")  <p>Q: Define a dictionary where the keys are the names of the planets and the values are their positions relative to the Sun (like 3 for Earth).</p> <ul> <li>Print the dictionary.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># TODO: Write your code here\n</pre> # TODO: Write your code here  In\u00a0[\u00a0]: Copied! <pre># Sets are unordered collection of unique elements\n# Sets are mutable\n\n# Empty set\ns = set()\n\n# set of integers\ns = {1, 2, 3, 4, 5}\nprint(f\"The set is {s}\")\n\n# set of mixed data types\ns = {1, 2.0, 'three'}\nprint(f\"The set is {s}\")\n\n# set of tuples\ns = {('a', 'b'), ('c', 'd')}\nprint(f\"The set is {s}\")\n\n# set operations\ns1 = {1, 2, 3, 4, 5}\ns2 = {4, 5, 6, 7, 8}\n\nprint(f\"s1 is {s1}\")\nprint(f\"s2 is {s2}\")\n\n# Union\ns3 = s1 | s2\nprint(f\"Union of s1 and s2 is {s3}\")\n\n# Intersection\ns3 = s1 &amp; s2\nprint(f\"Intersection of s1 and s2 is {s3}\")\n\n# Difference\ns3 = s1 - s2\nprint(f\"Difference of s1 and s2 is {s3}\")\n\n# Symmetric Difference\ns3 = s1 ^ s2\nprint(f\"Symmetric Difference of s1 and s2 is {s3}\")\n\n# Empty set is considered as False\nemptySet = set()\nprint(f\"emptySet is {bool(emptySet)}\")\n</pre> # Sets are unordered collection of unique elements # Sets are mutable  # Empty set s = set()  # set of integers s = {1, 2, 3, 4, 5} print(f\"The set is {s}\")  # set of mixed data types s = {1, 2.0, 'three'} print(f\"The set is {s}\")  # set of tuples s = {('a', 'b'), ('c', 'd')} print(f\"The set is {s}\")  # set operations s1 = {1, 2, 3, 4, 5} s2 = {4, 5, 6, 7, 8}  print(f\"s1 is {s1}\") print(f\"s2 is {s2}\")  # Union s3 = s1 | s2 print(f\"Union of s1 and s2 is {s3}\")  # Intersection s3 = s1 &amp; s2 print(f\"Intersection of s1 and s2 is {s3}\")  # Difference s3 = s1 - s2 print(f\"Difference of s1 and s2 is {s3}\")  # Symmetric Difference s3 = s1 ^ s2 print(f\"Symmetric Difference of s1 and s2 is {s3}\")  # Empty set is considered as False emptySet = set() print(f\"emptySet is {bool(emptySet)}\")  <p>Q: Define two sets of strings with some common elements and some unique elements.</p> <ul> <li><p>Perform the following operations on the sets:</p> <ul> <li>Union</li> <li>Intersection</li> <li>Difference</li> </ul> </li> <li><p>Print the results.</p> </li> </ul> In\u00a0[21]: Copied! <pre># TODO: Write your code here\n</pre> # TODO: Write your code here <p>Flow control statements are used to control the flow of a program. They include <code>if</code>, <code>elif</code>, and <code>else</code> statements, <code>for</code> and <code>while</code> loops, and <code>break</code> and <code>continue</code> statements.</p> <p>Differently from other programming languages, Python uses indentation to define code blocks. The code block following an <code>if</code>, <code>elif</code>, <code>else</code>, <code>for</code>, or <code>while</code> statement must be indented.</p> <p>IMPORTANT: The standard indentation is 4 spaces. Always use the same number of spaces for indentation in the same block of code. Avoid using tabs for indentation. Most of the code editors are configured to automatically convert tabs to spaces.</p> In\u00a0[\u00a0]: Copied! <pre># if expr1:\n#     code1\n# elif expr2:\n#     code2\n#   .\n#   .\n#   .\n#   .\n# else:\n#     code_n\n\n# code1 is executed if expr1 is evaluated to true. Else it moves to expr2 and checks for true \n# condition and moves to the next if not true. \n# Finally if all the excpression's are false, code_n is executed\n\nx = int(input(\"Please enter an integer: \"))\n\nif x &lt; 0:\n    x = 0\n    print('Negative changed to zero')\nelif x == 0:\n    print('Zero')\nelif x == 1:\n    print('Single')\nelse:\n    print('More')\n</pre> # if expr1: #     code1 # elif expr2: #     code2 #   . #   . #   . #   . # else: #     code_n  # code1 is executed if expr1 is evaluated to true. Else it moves to expr2 and checks for true  # condition and moves to the next if not true.  # Finally if all the excpression's are false, code_n is executed  x = int(input(\"Please enter an integer: \"))  if x &lt; 0:     x = 0     print('Negative changed to zero') elif x == 0:     print('Zero') elif x == 1:     print('Single') else:     print('More')  <p>Q: Prompt the user to input a number and use an if-elif-else statement to check if the number is negative, zero, or positive.</p> <ul> <li>Print an appropriate message in each case.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># TODO: Write your code here\n</pre> # TODO: Write your code here  <p>If statements can be used in conjunction with comparison operators to create more complex conditions.</p> In\u00a0[\u00a0]: Copied! <pre>x = int(input(\"Please enter an integer: \"))\n\nif x &gt; 0 and x &lt; 10:\n    print(f'x={x} is a positive single digit number')\nelse:\n    print(f'x={x} is not a positive single digit number')\n</pre> x = int(input(\"Please enter an integer: \"))  if x &gt; 0 and x &lt; 10:     print(f'x={x} is a positive single digit number') else:     print(f'x={x} is not a positive single digit number')  In\u00a0[\u00a0]: Copied! <pre># for loop is used to iter over any iterable object\n\n# iterating over list\nfor name in ['Steve', 'Jill', 'Venus']:\n    print(name)\n</pre> # for loop is used to iter over any iterable object  # iterating over list for name in ['Steve', 'Jill', 'Venus']:     print(name)  <p>You can iterate over every character in a string.</p> In\u00a0[\u00a0]: Copied! <pre># iterating over string\nfor char in \"Hellooooo\":\n    print(char)\n</pre> # iterating over string for char in \"Hellooooo\":     print(char)  <p>To iterate over dictionary keys and values, you can use the <code>items()</code> method. The for loop can deal with multiple variables.</p> In\u00a0[\u00a0]: Copied! <pre># iterating over dict keys\nphone_nos = {\"Henry\": 6091237458,\n             \"James\": 1234556789, \n             \"Larry\": 5698327549, \n             \"Rocky\": 8593876589}\n\nfor name, no in phone_nos.items(): # items() returns a list of tuples\n    print(f\"{name}: {no}\")\n</pre> # iterating over dict keys phone_nos = {\"Henry\": 6091237458,              \"James\": 1234556789,               \"Larry\": 5698327549,               \"Rocky\": 8593876589}  for name, no in phone_nos.items(): # items() returns a list of tuples     print(f\"{name}: {no}\")   <p>To iterate over a sequence of numbers we use <code>range()</code> function.</p> <p>The <code>range()</code> function generates a sequence of numbers. It can take one, two, or three parameters. For example, <code>range(5)</code> generates a sequence of numbers from 0 to 4.</p> In\u00a0[\u00a0]: Copied! <pre>for i in range(5):\n    print(i)\n</pre> for i in range(5):     print(i)  <p>However, if you want to iterate over a sequence of numbers with a different starting point, you can use <code>range(start, stop, step)</code>. For example, <code>range(2, 10, 2)</code> generates a sequence of numbers from 2 to 20 with a step of 2:</p> In\u00a0[\u00a0]: Copied! <pre>for i in range(2, 20, 2):\n    print(i)\n</pre>  for i in range(2, 20, 2):     print(i)  <p>You can use the len() function to get the length of a list or a string and use it in the range() function.</p> In\u00a0[\u00a0]: Copied! <pre># using len of list/tuple in range\nnames = ['Steve', 'Rock', 'Harry']\nfor i in range(len(names)):\n    print(names[i])\n</pre> # using len of list/tuple in range names = ['Steve', 'Rock', 'Harry'] for i in range(len(names)):     print(names[i])  <p>Alternatively, if you want to have both the element in a sequences as well as an index, you can use the <code>enumerate()</code> function.</p> In\u00a0[\u00a0]: Copied! <pre># using len of list/tuple in range\nnames = ['Steve', 'Rock', 'Harry']\nfor i in range(len(names)):\n    print(f\"{i}: {names[i]}\")\n</pre> # using len of list/tuple in range names = ['Steve', 'Rock', 'Harry'] for i in range(len(names)):     print(f\"{i}: {names[i]}\")  <p>Q: Using a for loop, print the square of each number in range(1, 6).</p> In\u00a0[\u00a0]: Copied! <pre># TODO: Write your code here\n</pre> # TODO: Write your code here  In\u00a0[\u00a0]: Copied! <pre># While loop executes as long as the condition remains true.\n# while cond1:\n#     pass\n\ni = 0\nwhile i &lt; 10:\n    print(i)\n    i += 1\n</pre> # While loop executes as long as the condition remains true. # while cond1: #     pass  i = 0 while i &lt; 10:     print(i)     i += 1  <p>Forever loop: The below code runs for ever (don't run it)</p> In\u00a0[\u00a0]: Copied! <pre># while True:\n#     print('Forever...')\n</pre> # while True: #     print('Forever...')  In\u00a0[\u00a0]: Copied! <pre># break statement breaks out of the the loop\nwhile True:\n    print('We\u2019re stuck in a loop...')\n    break # Break out of the while loop\nprint(\"not!\")\n</pre> # break statement breaks out of the the loop while True:     print('We\u2019re stuck in a loop...')     break # Break out of the while loop print(\"not!\")  <p>You can skip the current iteration without exiting the loop by using the <code>continue</code> statement.</p> In\u00a0[\u00a0]: Copied! <pre># continue statement skips a loop\nfor i in range(5):\n    continue \n    print(i)\n</pre> # continue statement skips a loop for i in range(5):     continue      print(i)  <p>This is more usefult when you have a condition to skip the current iteration but you don't want to exit the loop. For instance, you can skip the iteration if the number is even.</p> In\u00a0[\u00a0]: Copied! <pre>for i in range(10):\n    if i % 2 == 0: # Calculate the remainder of i divided by 2 and check if it is 0\n        continue\n    print(i)\n</pre> for i in range(10):     if i % 2 == 0: # Calculate the remainder of i divided by 2 and check if it is 0         continue     print(i)  <p>The <code>pass</code> statement is used as a placeholder for future code. When the <code>pass</code> statement is executed, nothing happens, but you avoid getting an error. It allows you to write minimal code structure without causing issues during execution, which is helpful in situations where code needs to be developed incrementally.</p> In\u00a0[\u00a0]: Copied! <pre>for i in range(10):\n    pass\n</pre> for i in range(10):     pass  <p>Q: Ask the user for an input, print that input, and then print the input in reverse.</p> <ul> <li>Use a while loop to repeat this process until the user inputs 'quit'.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># TODO: Write your code here\n</pre> # TODO: Write your code here"},{"location":"m01-python_basics1/python_basics1/#module-1-python-basics-1","title":"Module 1 - Python Basics 1\u00b6","text":""},{"location":"m01-python_basics1/python_basics1/#objectives","title":"Objectives\u00b6","text":"<ul> <li>Construct and Execute Python Scripts: Develop the ability to write and run basic Python scripts that adhere to standard syntax, establishing a firm foundation in programming.</li> <li>Apply Loops and Conditional Statements: Implement loops and conditional constructs to manipulate data effectively and solve intricate problems.</li> <li>Create Reusable Functions: Design and integrate Python functions that promote modularity and reusability, enhancing both code efficiency and maintainability.</li> </ul>"},{"location":"m01-python_basics1/python_basics1/#the-topics-covered-in-this-module","title":"The topics covered in this module:\u00b6","text":"<ul> <li>Variables and built in datatypes</li> <li>Printing and user input</li> <li>Operators</li> <li>Lists and Tuples</li> <li>Dictionaries</li> <li>Sets</li> <li>Flow control</li> </ul>"},{"location":"m01-python_basics1/python_basics1/#1-variables-and-built-in-data-types","title":"1. Variables and built-in data types\u00b6","text":""},{"location":"m01-python_basics1/python_basics1/#11-basic-and-variables","title":"1.1 Basic and variables\u00b6","text":""},{"location":"m01-python_basics1/python_basics1/#12-printing-composed-variables-and-text","title":"1.2 Printing composed variables and text\u00b6","text":"<p>Use the <code>print()</code> function to print variables and text. You can use the <code>+</code> operator to concatenate strings. You can also concatenate variables and strings by separating them with a comma.</p>"},{"location":"m01-python_basics1/python_basics1/#13-int-and-float","title":"1.3 Int and Float\u00b6","text":"<p>Numbers can be stored in variables. Integers are whole numbers, while floats are numbers with decimal points. Example: <code>x = 5</code> or <code>y = 3.14</code>.</p>"},{"location":"m01-python_basics1/python_basics1/#14-booleans-and-none","title":"1.4 Booleans and None\u00b6","text":"<p>Bolean values are <code>True</code> and <code>False</code> and are used to represent binary values. <code>None</code> is a special value that represents the absence of a value, when tested, normally it results to <code>False</code>.</p>"},{"location":"m01-python_basics1/python_basics1/#15-strings","title":"1.5 Strings\u00b6","text":"<p>Strings are sequences of characters, e.g., pieces of text. You can create them by enclosing characters in quotes. Python treats single quotes the same as double quotes.</p> <p>Strings can be concatenated using the <code>+</code> operator. You can also multiply strings by integers to repeat them.</p> <p>You may need to escape special characters in strings using a backslash <code>\\</code>. For example, to include a quote in a string, you can use <code>\\'</code>.</p> <p>You can use the <code>len()</code> function to get the length of a string.</p>"},{"location":"m01-python_basics1/python_basics1/#14-lists-and-tuples","title":"1.4 Lists and Tuples\u00b6","text":"<p>Lists are collections of items that are ordered and changeable. Tuples are collections of items that are ordered and unchangeable.</p> <p>They can contain any type of object, even other lists or tuples. They can be used for instance to store a sequence of numbers, strings, or a mix of both.</p>"},{"location":"m01-python_basics1/python_basics1/#15-dictionary","title":"1.5 Dictionary\u00b6","text":"<p>Dictionary is a collection of key-value pairs. It is unordered, changeable and indexed. Dictionaries are written with curly brackets, and have keys and values.</p> <p>They can be used for example to associate a name with a phone number.</p>"},{"location":"m01-python_basics1/python_basics1/#16-sets","title":"1.6 Sets\u00b6","text":"<p>Sets are unordered collections of unique elements. They are used to store multiple items in a single variable. Sets are written with curly brackets.</p>"},{"location":"m01-python_basics1/python_basics1/#2-flow-control-statements","title":"2. Flow control statements\u00b6","text":""},{"location":"m01-python_basics1/python_basics1/#21-if-elif-else","title":"2.1 if... elif... else...\u00b6","text":"<p>The <code>if</code> statement is used to test a condition. If the condition is true, the code block following the <code>if</code> statement is executed. The <code>elif</code> statement is used to test multiple conditions. If the condition is true, the code block following the <code>elif</code> statement is executed. The <code>else</code> statement is used to execute a block of code if the condition is false.</p> <p>For instance, this is needed if you want to check if a number is positive, negative, or zero and then decide what to do based on the result.</p>"},{"location":"m01-python_basics1/python_basics1/#22-for-loops","title":"2.2 For loops\u00b6","text":"<p>A <code>for</code> loop is used for iterating over a sequence (that is either a list, a tuple, a dictionary, a set, or a string).</p> <p>In general, loops are used to repeat a block of code multiple times. The <code>for</code> loop iterates over a sequence of elements, such as a list or a tuple. The <code>for</code> loop can also iterate over a string, which is a sequence of characters.</p>"},{"location":"m01-python_basics1/python_basics1/#23-while-loop","title":"2.3 While Loop\u00b6","text":"<p>With the <code>while</code> loop we can execute a set of statements as long as a condition is true.</p>"},{"location":"m01-python_basics1/python_basics1/#24-break-continue-and-pass-statements","title":"2.4 Break, Continue and Pass statements\u00b6","text":"<p><code>break</code> statement is used to exit a loop when a condition is met. <code>continue</code> statement is used to skip the current block and return to the <code>for</code> or <code>while</code> statement. <code>pass</code> statement is used as a placeholder for future code.</p>"},{"location":"m01-python_basics1/python_basics1/#3-references-to-other-resources","title":"3. References to other resources\u00b6","text":"<p>You can find more info in the Python documentation or in other tutorials.</p> <ul> <li><p>Python Documentation</p> </li> <li><p>Python Tutorial</p> </li> </ul>"},{"location":"m02-python_basics2/assigment_m02/","title":"Homework 1 Introduction to Python","text":"In\u00a0[\u00a0]: Copied! <pre># TODO - Enter Library Name\nimport _\n</pre> # TODO - Enter Library Name import _ <p>Next, we'll need to put the text files in a known location. The cell below prints out the current path, place the text files in this directory to ensure they are found by your script.</p> In\u00a0[\u00a0]: Copied! <pre># Getting the current path and displaying it\ncurrent_path = os.getcwd()\nprint(current_path)\n</pre> # Getting the current path and displaying it current_path = os.getcwd() print(current_path) <p>You can compose paths using the <code>os.path.join()</code> function. This function takes two arguments, the first being the directory path and the second being the file name. This function will return a string with the path to the file.</p> <p>For example, suppose you have a directory named <code>data</code> and a file named <code>story-1.txt</code>. You can compose the path to the file using the following code:</p> In\u00a0[\u00a0]: Copied! <pre># Composing the path to the file\na_path = os.path.join('data', 'story-1.txt')\nprint(a_path)\n</pre> # Composing the path to the file a_path = os.path.join('data', 'story-1.txt') print(a_path) <p>Now let's create the directory path to the <code>Datasets</code> folder. You can use <code>\"..\"</code> to refer to the parent directory. For example to compose a path from the parent of the current directory</p> <pre>path_to_datasets = os.path.join(\"..\", \"Datasets\")\n</pre> <p>You can use multiple path components to compose a new path. For example to compose a path from the parent of the current directory to the <code>Datasets</code> folder you can use:</p> <pre>path_to_datasets = os.path.join(a_path, \"..\", \"..\",\"a_file.txt\")\n</pre> <p>In the cell below, complete the code to create the path to the <code>Datasets</code> folder. Remember you already have the current path from previous cells.</p> In\u00a0[\u00a0]: Copied! <pre># Defining the datasets directory path\n# TODO - Enter the path to the datasets\ndataset_path = os.path.join(_)\nprint(dataset_path)\n</pre> # Defining the datasets directory path # TODO - Enter the path to the datasets dataset_path = os.path.join(_) print(dataset_path) In\u00a0[\u00a0]: Copied! <pre># TODO - Fill in the stories filename\nfile_path = os.path.join(dataset_path,_)\n# Opening the file\n# TODO - Fill the way to open the file\nwith open(file_path, _) as fp:\n    # TODO - Fill in the variable that represents the file we are working with\n    content = _.read()\n    # now you should have the content of the file in the variable content\nprint(\"Content of the file:\",content)\n</pre> # TODO - Fill in the stories filename file_path = os.path.join(dataset_path,_) # Opening the file # TODO - Fill the way to open the file with open(file_path, _) as fp:     # TODO - Fill in the variable that represents the file we are working with     content = _.read()     # now you should have the content of the file in the variable content print(\"Content of the file:\",content) <p>You may also find some code with try and catch. In our <code>except</code> block, we check if any issues occur, and if they do, we print them to the screen. This is used as a way to catch any errors that may occur during the reading of the file.</p> <p>For instance:</p> <pre>try:\n    with open(\"a_file.txt\", \"rt\") as fp:\n        text = fp.read()\n        print(text) \nexcept Exception as e:\n    print(\"Error reading file:\", e)\n</pre> <p>Add a <code>try</code> and <code>except</code> block to the code that you complete in the previous cell to catch any errors that may occur during the reading of the file.</p> In\u00a0[\u00a0]: Copied! <pre># TODO - Add a try except block to catch the exception\ntry:\n    # TODO - Fill in the file name\n    # Add your previous cell code here\n    ...\nexcept Exception as e:\n    print(\"Error reading file:\", e)\n</pre> # TODO - Add a try except block to catch the exception try:     # TODO - Fill in the file name     # Add your previous cell code here     ... except Exception as e:     print(\"Error reading file:\", e) In\u00a0[\u00a0]: Copied! <pre># TODO - Convert content to lowercase and split it into words\nlower_content = content.lower()\nwords = ... \n\n# The response should be a python list of words like: \"This is a test\" -&gt; ['this', 'is', 'a', 'test']\n</pre> # TODO - Convert content to lowercase and split it into words lower_content = content.lower() words = ...   # The response should be a python list of words like: \"This is a test\" -&gt; ['this', 'is', 'a', 'test'] <p>Now create a dictionary and start counting the words by using a for loop. If the word is not in the dictionary, add it with a count of 1. If the word is already in the dictionary, increment the count by 1.</p> In\u00a0[\u00a0]: Copied! <pre># Now create a dictionary and start counting the words by using a for loop. If the word is not in the dictionary, add it with a count of 1. If the word is already in the dictionary, increment the count by 1.\nword_count = {}\nfor word in words:\n    if word in word_count:\n        # TODO - Increment the count\n        ...\n    else:\n        # TODO - Add the word to the dictionary with a count of 1\n        ...\n\nprint(word_count)\n</pre> # Now create a dictionary and start counting the words by using a for loop. If the word is not in the dictionary, add it with a count of 1. If the word is already in the dictionary, increment the count by 1. word_count = {} for word in words:     if word in word_count:         # TODO - Increment the count         ...     else:         # TODO - Add the word to the dictionary with a count of 1         ...  print(word_count)   <p>Create a function <code>count_words_in_file(file_path)</code> that reads a file and returns a dictionary with the word count. The function should take the file path as an argument.</p> <p>Use your previous codes as a base to create this function. You can copy and paste the code you wrote before into the function.</p> In\u00a0[\u00a0]: Copied! <pre>def count_words_in_file(file_path):\n    # TODO - Copy the code from the previous cells here, make sure that file_path \n    ...\n    return word_count\n</pre> def count_words_in_file(file_path):     # TODO - Copy the code from the previous cells here, make sure that file_path      ...     return word_count <p>Let's test the function with another story file, like <code>story-2.txt</code>.</p> In\u00a0[\u00a0]: Copied! <pre># You should be able to test your function by running the following code:\nfile_path = os.path.join(dataset_path, \"story-2.txt\")\ncounts_dictionary = count_words_in_file(file_path)\nprint(counts_dictionary)\n</pre> # You should be able to test your function by running the following code: file_path = os.path.join(dataset_path, \"story-2.txt\") counts_dictionary = count_words_in_file(file_path) print(counts_dictionary) <p>We can now create a list of the files names to individually loop through.  Notice stories is a <code>list</code>, as it is assigned to values enclosed by brackets (<code>[]</code>).</p> In\u00a0[\u00a0]: Copied! <pre># List of files names to read in the same directory\n# TODO - Enter the file names inside the list\nstories = [...]\n</pre> # List of files names to read in the same directory # TODO - Enter the file names inside the list stories = [...] <p>Let's now loop through the list of files and call the function <code>count_words_in_file</code> for each file. We will store the result in a list of dictionaries called <code>word_counts</code>.</p> In\u00a0[\u00a0]: Copied! <pre>word_counts = []\n\nfor story in _:\n    # TODO - Fill in the file path\n    file_path = os.path.join(dataset_path, story)\n    # TODO - Call the function count_words_in_file with the file_path\n    word_count = ...\n    # TODO - Append the result to the word_counts list\n    ...\n\nprint(word_counts)\n</pre> word_counts = []  for story in _:     # TODO - Fill in the file path     file_path = os.path.join(dataset_path, story)     # TODO - Call the function count_words_in_file with the file_path     word_count = ...     # TODO - Append the result to the word_counts list     ...  print(word_counts) <p>And there you have it! Submit the completed version of this assignment for points. Save the notebook as a PDF or HTML file and submit it via Canvas. Keep the output of the code cells visible in the exported file.</p>"},{"location":"m02-python_basics2/assigment_m02/#homework-1-introduction-to-python","title":"Homework 1 Introduction to Python\u00b6","text":"<p>In this homework, we will be getting more experienced with python through file processing. With this guide, you will be able to:</p> <ul> <li>read from a text (.txt) file</li> <li>process the words of the file</li> <li>output the result of this process</li> </ul>"},{"location":"m02-python_basics2/assigment_m02/#instructions","title":"Instructions\u00b6","text":"<ol> <li>Follow the instructions on how to setup your Python and Jupyter (or VSCode) environment and cloning or downloading our repository. Instructions can be found in the class notes: https://filipinascimento.github.io/usable_ai/m00-setup/class</li> <li>Ensure that you have Python and Jupyter Notebook working. (You can also try using Google Colab. This is not the preferred method for this homework, but it is an option)</li> <li>Load the text files: <code>story-1.txt</code>, <code>story-2.txt</code>, <code>story-3.txt</code>, and <code>story-4.txt</code>, located in the <code>Datasets</code> directory. (If you are using Google Colab, you will need to upload the files to the colab environment)</li> <li>Answer the questions below by writing or completing the code in the provided cells.</li> </ol>"},{"location":"m02-python_basics2/assigment_m02/#dataset-overview","title":"Dataset Overview\u00b6","text":"<p>The dataset consists of four text files, each containing a story. The stories are:</p> <ul> <li><code>story-1.txt</code>: The Monkey and the Crocodile</li> <li><code>story-2.txt</code>: The Musical Donkey</li> <li><code>story-3.txt</code>: A Tale of Three Fish</li> <li><code>story-4.txt</code>: The Foolish Lion and the Clever Rabbit</li> </ul>"},{"location":"m02-python_basics2/assigment_m02/#submission-guidelines","title":"Submission Guidelines\u00b6","text":"<ul> <li>Submit your completed notebook as a HTML export, or a PDF file.</li> </ul> <p>To export to HTML, if you are on Jupyter, select <code>File</code> &gt; <code>Export Notebook As</code> &gt; <code>HTML</code>.</p> <p>If you are on VSCode, you can use the <code>Jupyter: Export to HTML</code> command.</p> <ul> <li>Open the command palette (Ctrl+Shift+P or Cmd+Shift+P on Mac).<ul> <li>Search for <code>Jupyter: Export to HTML</code>.</li> <li>Save the HTML file to your computer and submit it via Canvas.</li> </ul> </li> </ul> <p>Using Generative AI Responsibly</p> <p>You're welcome to use Generative AI to assist your learning, but focus on understanding the concepts rather than just solving the assignment. For example, instead of copying and pasting the question into the model, ask it to explain the concept in the question. Try asking: <code>How can I open a file in Python? Can you give me examples?</code> or <code>What functions and methods can I use to extract the words of a text file? Can you explain how they work with some examples?</code></p> <p>This way, you will learn how the solution works while building your skills. Remember to give context to the generative AI, so it can better assist you. Talk to the instructor and AIs if you have any questions or need insights.</p> <p>To begin, we will first need to make sure the functions (set of instructions) we need to use are included in this file. We'll do this through an import statement, followed by the libraries we need. In this case, we will use the <code>os</code> library. In the cell below, type the library name where the <code>_</code> is. In the remainder of this guide, you will be filling in some value where ever a <code>_</code> or <code>...</code> is.</p>"},{"location":"m02-python_basics2/assigment_m02/#opening-and-reading-one-of-the-text-files","title":"Opening and Reading one of the text files\u00b6","text":"<p>Let's open the first file (<code>stories-1.txt</code>) and read it's content.   We will need to call <code>read()</code> on <code>fp</code> to read in values from the file. </p>"},{"location":"m02-python_basics2/assigment_m02/#counting-words","title":"Counting Words\u00b6","text":"<p>Now we can read the file, we want to count the words that we read into the <code>words</code> variable. Take a look at the lesons and/or python documentation to find out how to split the text into words. I also suggest you use the <code>lower()</code> method to convert all words to lowercase. This way, we can count the words without worrying about the case of the words.</p>"},{"location":"m02-python_basics2/assigment_m02/#repeating-the-process-for-all-files","title":"Repeating the process for all files\u00b6","text":"<p>Now that you have the code to read and count the words in a file, you can repeat this process for all the files. You can create a function that reads a file and returns the word count. Then you can call this function for each file.</p>"},{"location":"m02-python_basics2/python_basics2/","title":"Module 2 - Python Basics 2","text":"<p>In this module we will cover more basic Python concepts. We will start by importing modules and defining functions. We will also cover file I/O, loading data, list comprehensions, lambda functions, and installing packages.</p> In\u00a0[\u00a0]: Copied! <pre># importing modules using import statement\nimport math\n\n# Import under an alias (avoid this pattern except with the most \n# common modules like pandas, numpy, etc.)\nimport math as m\n\n# Access components with pkg.fn\nm.pow(2, 3) \n\n# Import specific submodules/functions (not usually recommended \n# because it can be confusing to know where a function comes from)\nfrom math import pow\npow(2, 3)\n</pre> # importing modules using import statement import math  # Import under an alias (avoid this pattern except with the most  # common modules like pandas, numpy, etc.) import math as m  # Access components with pkg.fn m.pow(2, 3)   # Import specific submodules/functions (not usually recommended  # because it can be confusing to know where a function comes from) from math import pow pow(2, 3)  <p>Q: After importing the math module, can you import the random module and generate five random floating-point numbers between 0 and 1?</p> <ul> <li>Tip: use random.random() in a for loop</li> </ul> In\u00a0[\u00a0]: Copied! <pre># TODO: Write your code here\n</pre> # TODO: Write your code here  In\u00a0[\u00a0]: Copied! <pre># Functions in python are defined using key word \"def\"\n\n# simple function to print greetings `greet_word` is an optional argument with default value 'Hello'\ndef greet(name, greet_word='Hello'):\n    print(f\"{greet_word} {name}. How are you doing?\")\n\n# here greet has 'Hello' as default argument for greet_word\nprint(greet('James'))\nprint(greet(\"Steven\", greet_word=\"Howdy\"))\n\n# Observe that the function by default returns None\n</pre> # Functions in python are defined using key word \"def\"  # simple function to print greetings `greet_word` is an optional argument with default value 'Hello' def greet(name, greet_word='Hello'):     print(f\"{greet_word} {name}. How are you doing?\")  # here greet has 'Hello' as default argument for greet_word print(greet('James')) print(greet(\"Steven\", greet_word=\"Howdy\"))  # Observe that the function by default returns None  <p>To return a value from a function, you can use the <code>return</code> statement.</p> In\u00a0[\u00a0]: Copied! <pre># To return a value from a function, you can use the `return` statement.\ndef multiply_and_subtract_one(x, y):\n    return x * y - 1\n\nprint(multiply_and_subtract_one(2, 3))\n</pre> # To return a value from a function, you can use the `return` statement. def multiply_and_subtract_one(x, y):     return x * y - 1  print(multiply_and_subtract_one(2, 3))  <p>Q: Define a function called \"divide_by_two\" that takes a single parameter and returns half its value. Then test it with both integer and float inputs.</p> In\u00a0[\u00a0]: Copied! <pre># TODO: Write your code here\n</pre> # TODO: Write your code here  <p>Python supports recursive functions. A recursive function is a function that calls itself.</p> In\u00a0[\u00a0]: Copied! <pre># Function to print nth fibonacci number\ndef fib(n):\n    if n &lt;= 1:\n        return 1\n    else:\n        return fib(n-1)+fib(n-2)\nn = 10\nprint(f\"{n}th fibonacci number is {fib(n)}\")\n</pre> # Function to print nth fibonacci number def fib(n):     if n &lt;= 1:         return 1     else:         return fib(n-1)+fib(n-2) n = 10 print(f\"{n}th fibonacci number is {fib(n)}\")  In\u00a0[\u00a0]: Copied! <pre># Writing to a text file\nwith open(\"example.txt\", \"wt\") as f:\n    f.write(\"This is an example.\\n\")\n    f.write(\"This is another example in another line.\\n\")\n</pre> # Writing to a text file with open(\"example.txt\", \"wt\") as f:     f.write(\"This is an example.\\n\")     f.write(\"This is another example in another line.\\n\")   <p>You can load the content of a file using the <code>read()</code> method. You can also iterate over the lines of a file using a <code>for</code> loop. This will progressively load the file line by line.</p> In\u00a0[\u00a0]: Copied! <pre># Reading from a text file\nwith open(\"example.txt\", \"rt\") as f:\n    content = f.read()\n    print(content)\n\n# Read line by line\nwith open(\"example.txt\", \"rt\") as f:\n    for line in f:\n        print(line)\n</pre> # Reading from a text file with open(\"example.txt\", \"rt\") as f:     content = f.read()     print(content)  # Read line by line with open(\"example.txt\", \"rt\") as f:     for line in f:         print(line)   <p>Note that the last print resulted into 2 line breaks because the <code>print()</code> function adds a newline character at the end of the line that already contains a newline character.</p> In\u00a0[\u00a0]: Copied! <pre># Note that the last print resulted into 2 line breaks because the `print()` function adds a newline character at the end of the line that already contains a newline character.\n\n# You can strip the newline character by using the `strip()` method of the string object.\nwith open(\"example.txt\", \"rt\") as f:\n    for line in f:\n        print(line.strip())\n</pre> # Note that the last print resulted into 2 line breaks because the `print()` function adds a newline character at the end of the line that already contains a newline character.  # You can strip the newline character by using the `strip()` method of the string object. with open(\"example.txt\", \"rt\") as f:     for line in f:         print(line.strip())   <p>Append a file: You can append to a file by opening it in append mode. When you open a file in append mode, new data is written to the end of the file.</p> In\u00a0[\u00a0]: Copied! <pre>with open(\"example.txt\", \"at\") as f:\n    f.write(\"This is an appended line.\\n\")\n\nwith open(\"example.txt\", \"rt\") as f:\n    content = f.read()\n    print(content)\n</pre> with open(\"example.txt\", \"at\") as f:     f.write(\"This is an appended line.\\n\")  with open(\"example.txt\", \"rt\") as f:     content = f.read()     print(content)  <p>Q: Append user input to a file named \"user_input.txt\" and then read it again to verify the content was appended.</p> <ul> <li>Use the <code>with open(..., 'at')</code> approach.</li> <li>Then open the file again in read mode and print its contents.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># TODO: Write your code here\n</pre> # TODO: Write your code here  In\u00a0[\u00a0]: Copied! <pre># creating a json file\nimport json\ndata = {\n    \"name\": \"John\",\n    \"age\": 30,\n    \"city\": \"New York\"\n}\nwith open(\"data.json\", \"wt\") as f:\n    json.dump(data, f)\n\n# reading from a json file\nwith open(\"data.json\", \"rt\") as f:\n    data = json.load(f)\n    print(data)\n</pre> # creating a json file import json data = {     \"name\": \"John\",     \"age\": 30,     \"city\": \"New York\" } with open(\"data.json\", \"wt\") as f:     json.dump(data, f)  # reading from a json file with open(\"data.json\", \"rt\") as f:     data = json.load(f)     print(data)  In\u00a0[\u00a0]: Copied! <pre># creating a csv file\nimport csv\nwith open(\"example.csv\", \"w\") as f:\n    writer = csv.writer(f)\n    writer.writerow([\"Name\", \"City\"])\n    writer.writerow([\"John\", \"New York\"])\n    writer.writerow([\"Peter\", \"Los Angeles\"])\n\n# reading from a csv file\nwith open(\"example.csv\", \"r\") as f:\n    reader = csv.reader(f)\n    for row in reader:\n        print(row)\n</pre> # creating a csv file import csv with open(\"example.csv\", \"w\") as f:     writer = csv.writer(f)     writer.writerow([\"Name\", \"City\"])     writer.writerow([\"John\", \"New York\"])     writer.writerow([\"Peter\", \"Los Angeles\"])  # reading from a csv file with open(\"example.csv\", \"r\") as f:     reader = csv.reader(f)     for row in reader:         print(row)    In\u00a0[\u00a0]: Copied! <pre># List comprehension is a concise way to create lists\n# [expr for var in iterable]\n\nresult = [x**2 for x in range(10)]\nprint(f\"x squared is {result}\")\n</pre> # List comprehension is a concise way to create lists # [expr for var in iterable]  result = [x**2 for x in range(10)] print(f\"x squared is {result}\")   <p>You can add a condition to the list comprehension to filter the elements. For example, you can create a list of even numbers from 0 to 9.</p> In\u00a0[\u00a0]: Copied! <pre># List comprehension with condition\nresult = [x**2 for x in range(10) if x % 2 == 0]\nprint(f\"x squared when x is even is {result}\")\n</pre>  # List comprehension with condition result = [x**2 for x in range(10) if x % 2 == 0] print(f\"x squared when x is even is {result}\")  <p>Q: Use a list comprehension to generate the cubes of numbers from 1 to 7.</p> <ul> <li>Then filter out cubes that are divisible by 3 and print the result.</li> </ul> In\u00a0[\u00a0]: Copied! <pre># TODO: Write your code here\n</pre> # TODO: Write your code here  <p>You can have complex nested loops in list comprehensions. For example, you can create a list of tuples by combining elements from two lists.</p> In\u00a0[\u00a0]: Copied! <pre># List comprehension with nested loops\nresult = [(x, y) for x in range(3) for y in range(3)]\nprint(f\"Cartesian product of [0, 1, 2] is {result}\")\n</pre>  # List comprehension with nested loops result = [(x, y) for x in range(3) for y in range(3)] print(f\"Cartesian product of [0, 1, 2] is {result}\")   <p>You can combine nested loops with conditions.</p> In\u00a0[\u00a0]: Copied! <pre># List comprehension with nested loops and condition\nresult = [(x, y) for x in range(3) for y in range(3) if x != y]\nprint(f\"Cartesian product of [0, 1, 2] without diagonal is {result}\")\n</pre>  # List comprehension with nested loops and condition result = [(x, y) for x in range(3) for y in range(3) if x != y] print(f\"Cartesian product of [0, 1, 2] without diagonal is {result}\")  <p>Nested loops can be useful to flatten a list of lists. For example, you can flatten a list of lists into a single list.</p> In\u00a0[\u00a0]: Copied! <pre>list_of_lists = [[1, 2], [3, 4], [5, 6]]\n\nresult = [item for sublist in list_of_lists for item in sublist]\nprint(f\"Flattened list is {result}\")\n</pre> list_of_lists = [[1, 2], [3, 4], [5, 6]]  result = [item for sublist in list_of_lists for item in sublist] print(f\"Flattened list is {result}\")  In\u00a0[\u00a0]: Copied! <pre># Lambda functions are anonymous functions\n# lambda arguments: expression\nf = lambda x: x**2\nprint(f\"Square of 10 is {f(10)}\")\n</pre> # Lambda functions are anonymous functions # lambda arguments: expression f = lambda x: x**2 print(f\"Square of 10 is {f(10)}\")   <p>Lambda functions are used with map, filter and reduce functions.</p> <p>The <code>map()</code> function takes in a function and a list. The function is applied to every item in the list. It returns a list of the results.</p> In\u00a0[\u00a0]: Copied! <pre># map applies a function to all the items in an input list\n# map(function, iterable)\nresult = list(map(lambda x: x**2, range(10)))\nprint(f\"Square of 0 to 9 is {result}\")\n</pre>  # map applies a function to all the items in an input list # map(function, iterable) result = list(map(lambda x: x**2, range(10))) print(f\"Square of 0 to 9 is {result}\")   <p>The <code>filter()</code> function takes in a function and a list. The function is applied to every item in the list. It returns a list of items for which the function returns True.</p> In\u00a0[\u00a0]: Copied! <pre># filter creates a list of elements for which a function returns true\n# filter(function, iterable)\nresult = list(filter(lambda x: x % 2 == 0, range(10)))\nprint(f\"Even numbers in 0 to 9 are {result}\")\n</pre>  # filter creates a list of elements for which a function returns true # filter(function, iterable) result = list(filter(lambda x: x % 2 == 0, range(10))) print(f\"Even numbers in 0 to 9 are {result}\")   <p>The <code>reduce()</code> function is defined in the <code>functools</code> module. It applies a rolling computation to sequential pairs of values in a list. For example, you can use the <code>reduce()</code> function to calculate the sum of a list of numbers.</p> In\u00a0[\u00a0]: Copied! <pre># reduce applies a rolling computation to sequential pairs of values in a list\n# reduce(function, iterable)\nfrom functools import reduce\nresult = reduce(lambda x, y: x + y, range(10))\n\n# The above code is equivalent to sum(range(10))\nprint(f\"Sum of 0 to 9 is {result}\")\n</pre>  # reduce applies a rolling computation to sequential pairs of values in a list # reduce(function, iterable) from functools import reduce result = reduce(lambda x, y: x + y, range(10))  # The above code is equivalent to sum(range(10)) print(f\"Sum of 0 to 9 is {result}\")   <p>Lambda functions have other uses as well, such as in sorting and in defining functions that take functions as arguments.</p> In\u00a0[\u00a0]: Copied! <pre>pairs = [(1, 'one'), (2, 'two'), (3, 'three'), (4, 'four')]\npairs.sort(key=lambda pair: pair[1])\nprint(f\"Sorted pairs based on second element is {pairs}\")\n</pre> pairs = [(1, 'one'), (2, 'two'), (3, 'three'), (4, 'four')] pairs.sort(key=lambda pair: pair[1]) print(f\"Sorted pairs based on second element is {pairs}\")  <p>If you are on colab, you can install extra packages using</p> In\u00a0[\u00a0]: Copied! <pre>!pip install package_name\n</pre> !pip install package_name  <p>If you are on jupyter notebook, you can install extra packages using:</p> In\u00a0[\u00a0]: Copied! <pre>pip install package_name\n</pre> pip install package_name  In\u00a0[\u00a0]: Copied! <pre># TRY IT OUT\n</pre> # TRY IT OUT  In\u00a0[\u00a0]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE  In\u00a0[\u00a0]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE  In\u00a0[\u00a0]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE  In\u00a0[\u00a0]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE  In\u00a0[\u00a0]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE  In\u00a0[\u00a0]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE  In\u00a0[\u00a0]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE  In\u00a0[\u00a0]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE  In\u00a0[\u00a0]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE  In\u00a0[\u00a0]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE  In\u00a0[\u00a0]: Copied! <pre># YOUR SOLUTION HERE\n</pre> # YOUR SOLUTION HERE"},{"location":"m02-python_basics2/python_basics2/#module-2-python-basics-2","title":"Module 2 - Python Basics 2\u00b6","text":""},{"location":"m02-python_basics2/python_basics2/#the-topics-covered-in-this-module","title":"The topics covered in this module:\u00b6","text":"<ul> <li>Importing Modules</li> <li>Defining Functions</li> <li>File I/O</li> <li>Loading Data (JSON, CSV)</li> <li>List Comprehensions</li> <li>Lambda Functions</li> <li>Installing Packages</li> </ul>"},{"location":"m02-python_basics2/python_basics2/#1-importing-modules","title":"1. Importing Modules\u00b6","text":"<p>Python has a lot of built-in modules that you can use. You can import these modules using the <code>import</code> statement.</p> <p>The same command can be used to import installed packages.</p>"},{"location":"m02-python_basics2/python_basics2/#2-defining-functions","title":"2. Defining Functions\u00b6","text":"<p>A function is a block of code that only runs when it is called. You can pass data, known as parameters, into a function. A function can return data as a result.</p>"},{"location":"m02-python_basics2/python_basics2/#3file-io","title":"3.File I/O\u00b6","text":"<p>Python has functions for file handling and manipulation. The key function to work with files in Python is the <code>open()</code> function. The open() function takes two parameters; filename, and mode. The mode parameter specifies whether you want to read, write, or append to the file. Common modes are 'r' for reading, 'w' for writing (which overwrites the file), and 'a' for appending.</p> <p>You can also define the format of the file by adding 't' for text or 'b' for binary.</p>"},{"location":"m02-python_basics2/python_basics2/#4-loading-data","title":"4 Loading Data\u00b6","text":""},{"location":"m02-python_basics2/python_basics2/#41-dealing-with-json-files-in-python","title":"4.1 Dealing with json files in Python\u00b6","text":"<p>JSON (JavaScript Object Notation) is a lightweight data-interchange format. It is easy for humans to read and write. It is easy for machines to parse and generate.</p> <p>An example of a JSON file:</p> <pre>{\n  \"name\": \"John\",\n  \"age\": 30,\n  \"city\": \"New York\"\n  \"children\": [\n    {\n      \"name\": \"Anna\",\n      \"age\": 5\n    },\n    {\n      \"name\": \"Betty\",\n      \"age\": 7\n    }\n  ]\n}\n</pre>"},{"location":"m02-python_basics2/python_basics2/#42-dealing-with-csv-files-in-python","title":"4.2 Dealing with csv files in Python\u00b6","text":"<p>CSV (Comma Separated Values) is a simple file format used to store tabular data, such as a spreadsheet or database. A CSV file stores tabular data (numbers and text) in plain text.</p> <p>NOTE THAT PANDAS IS A BETTER OPTION FOR DEALING WITH CSV FILES. WE WILL DISCUSS PANDAS IN THE NEXT SECTION.</p>"},{"location":"m02-python_basics2/python_basics2/#5-list-comprehensions-advanced","title":"5. List Comprehensions (Advanced)\u00b6","text":"<p>List comprehensions provide a concise way to create lists. Common applications are to make new lists where each element is the result of some operation applied to each member of another sequence or iterable, or to create a subsequence of those elements that satisfy a certain condition.</p>"},{"location":"m02-python_basics2/python_basics2/#6-lambda-functions-advanced","title":"6. Lambda Functions (Advanced)\u00b6","text":"<p>Lambda functions are small anonymous functions. They can have any number of arguments but only one expression. They are defined using the <code>lambda</code> keyword.</p>"},{"location":"m02-python_basics2/python_basics2/#7-installing-packages","title":"7. Installing Packages\u00b6","text":"<p>There are many packages available for Python. Most of them are available on the Python Package Index (PyPI). You can install packages using the <code>pip</code> command. For example, to install the <code>numpy</code> package, you can use the command <code>!pip install numpy</code> (on collab).</p> <p>Check out the PyPI website for more information on available packages.</p> <p>Some advanced packages can only be installed via conda. For example, to install the <code>numpy</code> package, you can use the command <code>!conda install &lt;package_name&gt;</code>. Refer to the conda documentation for more information.</p>"},{"location":"m02-python_basics2/python_basics2/#8-references-to-other-advanced-topics","title":"8. References to other advanced topics\u00b6","text":"<p>Python has a lot of advanced topics that you can learn. For instance it is possible to create classes and objects, use decorators, context managers, and more. These topics are not covered in this course but you can find more information in the Python documentation or in other tutorials.</p> <ul> <li><p>Python Documentation</p> </li> <li><p>Python Tutorial</p> </li> <li><p>Python Library Reference</p> </li> <li><p>YYiki Python</p> </li> <li><p>Real Python</p> </li> </ul>"},{"location":"m02-python_basics2/python_basics2/#extra-questions","title":"Extra questions\u00b6","text":"<p>For each of the following question, first think about the result without running the code. Then test it by running the code. Reach out if you don't understand why!</p>"},{"location":"m02-python_basics2/python_basics2/#whats-the-output","title":"What's the output?\u00b6","text":"<pre>def func(a):\n    a = a + 2\n    a = a * 2\n    return a\n\nprint(func(2))\n</pre>"},{"location":"m02-python_basics2/python_basics2/#true-false-why","title":"True? False? Why?\u00b6","text":"<pre>0.1 + 0.2 == 0.3\n</pre>"},{"location":"m02-python_basics2/python_basics2/#3-what-is-list_1-and-list_2-and-why","title":"3. What is <code>list_1</code> and <code>list_2</code> and why?\u00b6","text":"<pre>list_1 = [1,2,3]\nlist_2 = list_1\nlist_1.append(4)\nlist_2 += [5]\nlist_2 = list_2 + [10]\n</pre>"},{"location":"m02-python_basics2/python_basics2/#whats-the-output","title":"What's the output?\u00b6","text":"<pre>l = [i**2 for i in range(10)]\nl[-4:2:-3]\n</pre>"},{"location":"m02-python_basics2/python_basics2/#what-does-the-code-do-if-the-ordering-doesnt-matter-how-can-it-be-simplified","title":"What does the code do? If the ordering doesn't matter, how can it be simplified?\u00b6","text":"<pre>def func1(lst):\n    a = []\n    for i in lst:\n        if i not in a:\n            a.append(i)\n    return a\n</pre>"},{"location":"m02-python_basics2/python_basics2/#what-would-be-the-output","title":"What would be the output?\u00b6","text":"<pre>val = [0, 10, 15, 20]\ndata = 15\ntry:\n    data = data/val[0]\nexcept ZeroDivisionError:\n    print(\"zero division error - 1\")\nexcept:\n    print(\"zero division error - 2\")\nfinally:\n    print(\"zero division error - 3\")\n\nval = [0, 10, 15, 20]\ndata = 15\n\ntry:\n    data = data/val[4]\nexcept ZeroDivisionError:\n    print(\"zero division error - 1\")\nexcept:\n    print(\"zero division error - 2\")\nfinally:\n    print(\"zero division error - 3\")\n</pre>"},{"location":"m02-python_basics2/python_basics2/#what-does-the-code-do","title":"What does the code do?\u00b6","text":"<pre>def func(s):\n    d = {}\n    for c in s:\n        if c in d:\n            d[n] += 1\n        else:\n            d[n] = 1\n    return d\n</pre> <p>(Btw, the same operation can be done by simply running <code>Counter(s)</code> by using <code>Counter</code> data structure in the <code>collections</code> module.)</p>"},{"location":"m02-python_basics2/python_basics2/#whats-the-output","title":"What's the output?\u00b6","text":"<pre>def func(l):\n    l.append(10)\n    return l\n\na = [1,2,3]\nb = func(a)\na == b\n</pre>"},{"location":"m02-python_basics2/python_basics2/#whats-happening-to-a-in-each-step-why","title":"What's happening to <code>a</code> in each step? Why?\u00b6","text":"<pre># step 1\na = [ [ ] ] * 5\n# step 2\na[0].append(10)\n# step 3\na[1].append(20)\n# step 4\na.append(30)\n</pre>"},{"location":"m02-python_basics2/python_basics2/#whats-the-output","title":"What's the output?\u00b6","text":"<pre>L = list('abcdefghijk')\nL[1] = L[4] = 'x'\nL[3] = L[-3]\nprint(L)\n</pre>"},{"location":"m02-python_basics2/python_basics2/#whats-the-output","title":"What's the output?\u00b6","text":"<pre>y = 8\nz = lambda x : x * y\nprint(z(6))\n</pre>"},{"location":"m02-python_basics2/python_basics2/#whats-the-output","title":"What's the output?\u00b6","text":"<pre>count = 1\n\ndef func(count):\n    for i in (1, 2, 3):\n        count += 1\nfunc(count = 10)\ncount += 5\nprint(count)\n</pre>"},{"location":"m03-numpy_pandas/assignment_m03_sol/","title":"Homework 2: Exploring Solar System Bodies (Pandas introduction)","text":"In\u00a0[\u00a0]: Copied! <pre># if you are running this notebook in your local machine,\n# make sure you have all the dependencies installed\n# uncomment the following lines to install the dependencies\n# This may be needed if you are running this notebook in online\n# environments such as Google Colab\n#\n# !pip install numpy pandas\n#\n# also copy the data file to the same directory as this notebook\n# and update the paths accordingly\n</pre> # if you are running this notebook in your local machine, # make sure you have all the dependencies installed # uncomment the following lines to install the dependencies # This may be needed if you are running this notebook in online # environments such as Google Colab # # !pip install numpy pandas # # also copy the data file to the same directory as this notebook # and update the paths accordingly In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\n\n# Load the dataset\ndata = pd.read_json('../../Datasets/sol_data.json')\n# The ../../ are needed to go back two levels in the directory structure.\n# Note that the path is relative to the location of the notebook file. Double check\n# if the path is correct based on your system\ndata.head()\n</pre> import pandas as pd import numpy as np  # Load the dataset data = pd.read_json('../../Datasets/sol_data.json') # The ../../ are needed to go back two levels in the directory structure. # Note that the path is relative to the location of the notebook file. Double check # if the path is correct based on your system data.head() In\u00a0[\u00a0]: Copied! <pre># Total number of objects\n# Fill in code to calculate total number of objects\n\n# Number of planets\n# Fill in code to calculate number of planets\n\n# Number of moons\n# Fill in code to calculate number of moons\n</pre> # Total number of objects # Fill in code to calculate total number of objects  # Number of planets # Fill in code to calculate number of planets  # Number of moons # Fill in code to calculate number of moons <p>Hint: By moon we mean a natural satellite of a planet or another object in the solar system. Take a look at the columns and see if you can identify the criteria for classifying an object as a moon. Ask the instructor or AIs for help if needed.</p> In\u00a0[\u00a0]: Copied! <pre># Mean density of all planets\n# Fill in code\n\n# Planet with the highest surface gravity\n# Fill in code\n\n# Planets by descending mass\n# Fill in code\n</pre>  # Mean density of all planets # Fill in code  # Planet with the highest surface gravity # Fill in code  # Planets by descending mass # Fill in code  In\u00a0[\u00a0]: Copied! <pre># Number of moons orbiting each planet\n# Fill in code\n\n# Average radius of all moons\n# Fill in code\n\n# Compare average surface gravity of moons vs. planets\n# Fill in code\n</pre> # Number of moons orbiting each planet # Fill in code  # Average radius of all moons # Fill in code  # Compare average surface gravity of moons vs. planets # Fill in code  In\u00a0[\u00a0]: Copied! <pre># Highest orbital eccentricity\n# Fill in code\n\n# Average semi-major axis of planets vs. moons\n# Fill in code\n\n# Moon with the shortest orbital period\n# Fill in code\n</pre> # Highest orbital eccentricity # Fill in code  # Average semi-major axis of planets vs. moons # Fill in code  # Moon with the shortest orbital period # Fill in code In\u00a0[\u00a0]: Copied! <pre># Example of how to parse and clean the strings for the assignment\ndef preprocess_dates(date_string):\n    # conver to YYYY-MM-DD\n    if pd.isna(date_string):\n        return pd.NA\n    \n    # replace ?? by 01\n    date_string = date_string.replace('??', '01')\n\n    # add 01/01 if only year is provided\n    if len(date_string) == 4:\n        date_string = '01/01/' + date_string\n    \n    # transform to YYYY-MM-DD\n    date_splitted = date_string.split('/')\n\n    # but only if the string has 3 parts (day, month, year)\n    if len(date_splitted) == 3:\n        day = date_splitted[0]\n        month = date_splitted[1]\n        year = date_splitted[2]\n        return f\"{year}-{month}-{day}\"\n        # or using pandas Period (pd.Period)\n        # return pd.Period(year=int(year), month=int(month), day=int(day), freq=\"D\")\n    else:\n        return pd.NA\n\ndata['parsedDiscoveryDate'] = data['discoveryDate'].apply(preprocess_dates)\n</pre> # Example of how to parse and clean the strings for the assignment def preprocess_dates(date_string):     # conver to YYYY-MM-DD     if pd.isna(date_string):         return pd.NA          # replace ?? by 01     date_string = date_string.replace('??', '01')      # add 01/01 if only year is provided     if len(date_string) == 4:         date_string = '01/01/' + date_string          # transform to YYYY-MM-DD     date_splitted = date_string.split('/')      # but only if the string has 3 parts (day, month, year)     if len(date_splitted) == 3:         day = date_splitted[0]         month = date_splitted[1]         year = date_splitted[2]         return f\"{year}-{month}-{day}\"         # or using pandas Period (pd.Period)         # return pd.Period(year=int(year), month=int(month), day=int(day), freq=\"D\")     else:         return pd.NA  data['parsedDiscoveryDate'] = data['discoveryDate'].apply(preprocess_dates) In\u00a0[\u00a0]: Copied! <pre># Objects with discovery dates\n# Fill in code\n\n# Oldest discovered moon\n# Fill in code\n</pre> # Objects with discovery dates # Fill in code  # Oldest discovered moon # Fill in code In\u00a0[\u00a0]: Copied! <pre># Average density of moons orbiting planets with mass &gt; Earth\n# Fill in code\n\n# Average orbital eccentricity by orbit_type\n# Fill in code\n\n# Top 3 moons with highest escape velocity\n# Fill in code\n</pre> # Average density of moons orbiting planets with mass &gt; Earth # Fill in code  # Average orbital eccentricity by orbit_type # Fill in code  # Top 3 moons with highest escape velocity # Fill in code In\u00a0[\u00a0]: Copied! <pre># Moons with a mass less than Earth's moon and percentage\n# Fill in code\n\n# Ratio of moons to planets and planet with highest moon to mass ratio\n# Fill in code\n\n# Average density of moons per planet\n# Fill in code\n</pre> # Moons with a mass less than Earth's moon and percentage # Fill in code  # Ratio of moons to planets and planet with highest moon to mass ratio # Fill in code  # Average density of moons per planet # Fill in code"},{"location":"m03-numpy_pandas/assignment_m03_sol/#homework-2-exploring-solar-system-bodies-pandas-introduction","title":"Homework 2: Exploring Solar System Bodies (Pandas introduction)\u00b6","text":"<p>Welcome to Assignment 12!</p> <p>In this assignment, we will analyze data about celestial bodies in the solar system using Python, NumPy, and Pandas. The goals of this assignment are to:</p> <ul> <li>Open a simple dataset formatted as JSON using pandas.</li> <li>Apply simple statistical analysis to real-world data.</li> <li>Refine Python programming skills through hands-on practice.</li> <li>Ensure you can run Python and Python notebook environments (e.g., Jupyter Notebook, JupyterLab, Collab, VSCode) and troubleshoot any setup issues.</li> </ul> <p>A key part of this homework is verifying that you can successfully run Python notebooks. If you encounter any difficulties, seek help from the instructor or AIs. Additionally, use Slack to ask questions or share insights. If you see a classmate struggling, helping them out will be great for a collaborative learning environment (and may count extra points in engagement \ud83d\ude00).</p>"},{"location":"m03-numpy_pandas/assignment_m03_sol/#instructions","title":"Instructions\u00b6","text":"<ol> <li>Follow the instructions on how to setup your Python and Jupyter (or VSCode) environment and cloning or downloading our repository. Instructions can be found in the class notes.</li> <li>Ensure that you have Python, Jupyter Notebook, and the necessary libraries installed (<code>NumPy</code> and <code>Pandas</code>).</li> <li>Load the dataset <code>Datasets/sol_data.json</code> into a Pandas DataFrame.</li> <li>Answer the questions below by writing Python code.</li> <li>No plots or visualizations are required\u2014your insights should come from code-based analysis and outputs.</li> </ol>"},{"location":"m03-numpy_pandas/assignment_m03_sol/#dataset-overview","title":"Dataset Overview\u00b6","text":"<p>The dataset contains information about celestial objects, including:</p> <ul> <li>isPlanet: Indicates whether the object is a planet (<code>True</code> or <code>False</code>).</li> <li>isDwarfPlanet: Indicates whether the object is a dwarf planet (<code>True</code> or <code>False</code>).</li> <li>orbit_type: Classifies the object as \"Primary\" (planets) or \"Secondary\" (moons).</li> <li>Physical and orbital properties, such as mass, density, meanRadius, gravity, sideralOrbit, and more.</li> </ul>"},{"location":"m03-numpy_pandas/assignment_m03_sol/#submission-guidelines","title":"Submission Guidelines\u00b6","text":"<ul> <li>Submit your completed notebook as a HTML export, or a PDF file.</li> </ul> <p>To export to HTML, if you are on Jupyter, select <code>File</code> &gt; <code>Export Notebook As</code> &gt; <code>HTML</code>.</p> <p>If you are on VSCode, you can use the <code>Jupyter: Export to HTML</code> command.</p> <ul> <li>Open the command palette (Ctrl+Shift+P or Cmd+Shift+P on Mac).<ul> <li>Search for <code>Jupyter: Export to HTML</code>.</li> <li>Save the HTML file to your computer and submit it via Canvas.</li> </ul> </li> </ul> <p>Hint: If you are learning pandas, check out our tutorials or the official documentation:</p> <ul> <li>Pandas Getting started</li> <li>Pandas DataFrame API Documentation</li> <li>Our lecture on Pandas</li> </ul> <p>Using Generative AI Responsibly</p> <p>You're welcome to use Generative AI to assist your learning, but focus on understanding the concepts rather than just solving the assignment. For example:</p> <ul> <li>Instead of asking: <code>What's the code to count moons orbiting each planet?</code></li> <li>Try asking: <code>How can I use Pandas to group and count values? Can you provide examples? Can you explain the steps?</code></li> </ul> <p>This way, you will learn how the solution works while building your skills. Remember to give context to the generative AI, so it can better assist you. Talk to the instructor and AIs if you have any questions or need insights.</p>"},{"location":"m03-numpy_pandas/assignment_m03_sol/#1-general-information","title":"1. General Information\u00b6","text":"<ul> <li>How many objects are in the dataset?</li> <li>How many are planets? How many are moons?</li> </ul>"},{"location":"m03-numpy_pandas/assignment_m03_sol/#2-planets","title":"2. Planets\u00b6","text":"<ul> <li>What is the mean density of all planets?</li> <li>Which planet has the highest surface gravity, and what is its gravity value?</li> <li>List all planets in descending order of their mass.</li> </ul>"},{"location":"m03-numpy_pandas/assignment_m03_sol/#3-moons-satellites","title":"3. Moons (Satellites)\u00b6","text":"<ul> <li>How many moons orbit each planet? Present this as a table or dictionary.</li> <li>What is the average radius (meanRadius) of all moons?</li> <li>Compare the average surface gravity of moons to that of planets.</li> </ul>"},{"location":"m03-numpy_pandas/assignment_m03_sol/#4-orbital-properties","title":"4. Orbital Properties\u00b6","text":"<ul> <li>Which object has the highest orbital eccentricity, and what is its value?</li> <li>Calculate the average semi-major axis (semimajorAxis) for planets and compare it to that of moons.</li> <li>Identify the moon with the shortest orbital period (sideralOrbit) and the planet it orbits.</li> </ul>"},{"location":"m03-numpy_pandas/assignment_m03_sol/#5-discovery-dates","title":"5. Discovery Dates\u00b6","text":"<ul> <li>How many objects have recorded discovery dates?</li> <li>Which is the oldest discovered moon (except ours) for which we have recorded discovery dates, and when was it discovered?</li> </ul> <p>Look at the format of dates in the dataset. You will find NA values for objects without recorded discovery dates. Also some dates are just a year, while others are more precise. Complete dates are formatted as <code>DD/MM/YYYY</code> (e.g, 12/04/1997), while years are formatted as <code>YYYY</code>, e.g., <code>1997</code>. Finally some dates may have <code>??</code> in place of day or months, which should be cleaned up. For instance by converting <code>??/??/1997</code> to <code>01/01/1997</code> or <code>??/04/1997</code> to <code>01/04/1997</code>.</p> <p>Hint: Pandas <code>.to_datetime()</code> does not support dates before 1600. I recommend to create a function to clean the dates and use the <code>.apply()</code> to run. For example, first ignore NA values, then convert the valid complete dates while handling the years by padding them to a full date format if needed (like Jan 1st). Alternatively, you can use pd.period.</p>"},{"location":"m03-numpy_pandas/assignment_m03_sol/#6-advanced-analysis","title":"6. Advanced Analysis\u00b6","text":"<ul> <li>Calculate the average density of moons that orbit planets with a mass greater than Earth's mass (<code>5.97e24 kg</code>).</li> <li>Group all objects by their <code>orbit_type</code> and compute the average orbital eccentricity for each group.</li> <li>Identify the top 3 moons with the highest escape velocity (escape).</li> </ul>"},{"location":"m03-numpy_pandas/assignment_m03_sol/#7-extra-questions","title":"7. Extra questions\u00b6","text":"<ol> <li>How many moons have a mass less than 10% of Earth's moon? What percentage of all moons does this represent?</li> <li>Calculate the ratio of moons to planets in the dataset. Which planet has the highest number of moons relative to its mass?</li> <li>Group moons by their host planet and calculate the average density for each group. Which planet hosts moons with the highest average density?</li> </ol>"},{"location":"m03-numpy_pandas/numpy_basics/","title":"NumPy Tutorial","text":"In\u00a0[1]: Copied! <pre># Install numpy using pip (ONLY IF NEEDED)\n#!pip install numpy\n</pre> # Install numpy using pip (ONLY IF NEEDED) #!pip install numpy  In\u00a0[2]: Copied! <pre># Importing NumPy\nimport numpy as np\n</pre> # Importing NumPy import numpy as np  <p>Why NumPy?</p> <ul> <li>Efficiency: NumPy arrays are more memory-efficient than Python lists for large-scale data.</li> <li>Vectorization: Mathematical operations on arrays are optimized in C, making them very fast.</li> <li>Broad Ecosystem: NumPy underpins many other libraries (Pandas, SciPy, TensorFlow, etc.).</li> </ul> In\u00a0[3]: Copied! <pre>arr_list = [1, 2, 3, 4]\narr_np = np.array(arr_list)\nprint(\"Created array:\", arr_np)       # [1 2 3 4]\nprint(\"Data type:\", arr_np.dtype)     # e.g., int64 (depends on platform)\n</pre> arr_list = [1, 2, 3, 4] arr_np = np.array(arr_list) print(\"Created array:\", arr_np)       # [1 2 3 4] print(\"Data type:\", arr_np.dtype)     # e.g., int64 (depends on platform)  <pre>Created array: [1 2 3 4]\nData type: int64\n</pre> In\u00a0[4]: Copied! <pre>zeros_arr = np.zeros(4)       # 1D array of 4 zeros\nones_arr = np.ones(5)              # 1D array of 5 ones\nfours_arr = np.full(6, 4)            # 1D array of 6 fours\nrange_arr = np.arange(0, 10, 2)    # [0, 2, 4, 6, 8]\nlin_arr = np.linspace(0, 1, 5)     # [0., 0.25, 0.5, 0.75, 1.]\n\nprint(\"Zeros array:\\n\", zeros_arr)\nprint(\"Ones array:\\n\", ones_arr)\nprint(\"Fours array:\\n\", fours_arr)\nprint(\"Arange array:\", range_arr)\nprint(\"Linspace array:\", lin_arr)\n</pre> zeros_arr = np.zeros(4)       # 1D array of 4 zeros ones_arr = np.ones(5)              # 1D array of 5 ones fours_arr = np.full(6, 4)            # 1D array of 6 fours range_arr = np.arange(0, 10, 2)    # [0, 2, 4, 6, 8] lin_arr = np.linspace(0, 1, 5)     # [0., 0.25, 0.5, 0.75, 1.]  print(\"Zeros array:\\n\", zeros_arr) print(\"Ones array:\\n\", ones_arr) print(\"Fours array:\\n\", fours_arr) print(\"Arange array:\", range_arr) print(\"Linspace array:\", lin_arr)  <pre>Zeros array:\n [0. 0. 0. 0.]\nOnes array:\n [1. 1. 1. 1. 1.]\nFours array:\n [4 4 4 4 4 4]\nArange array: [0 2 4 6 8]\nLinspace array: [0.   0.25 0.5  0.75 1.  ]\n</pre> In\u00a0[5]: Copied! <pre>a_2d_array = np.array([[1, 2, 3], [4, 5, 6]])\nprint(\"2D array:\\n\", a_2d_array)\nprint(\"Shape of 2D array:\", a_2d_array.shape)  # (2, 3)\n\na_3d_array = np.array([[[1, 2, 3], [3, 4, 5]], [[5, 6, 7], [7, 8, 9]]])\nprint(\"3D array:\\n\", a_3d_array)\nprint(\"Shape of 3D array:\", a_3d_array.shape)\n</pre> a_2d_array = np.array([[1, 2, 3], [4, 5, 6]]) print(\"2D array:\\n\", a_2d_array) print(\"Shape of 2D array:\", a_2d_array.shape)  # (2, 3)  a_3d_array = np.array([[[1, 2, 3], [3, 4, 5]], [[5, 6, 7], [7, 8, 9]]]) print(\"3D array:\\n\", a_3d_array) print(\"Shape of 3D array:\", a_3d_array.shape) <pre>2D array:\n [[1 2 3]\n [4 5 6]]\nShape of 2D array: (2, 3)\n3D array:\n [[[1 2 3]\n  [3 4 5]]\n\n [[5 6 7]\n  [7 8 9]]]\nShape of 3D array: (2, 2, 3)\n</pre> <p>Practice Question 1</p> <p>Without introducing new commands (besides what you\u2019ve seen above), create:</p> <ol> <li><p>A 1D array of 7 elements, all set to 3.</p> </li> <li><p>An array of even numbers from 10 to 20 (exclusive) using <code>np.arange</code>.</p> </li> <li><p>Print both arrays to verify.</p> </li> </ol> In\u00a0[6]: Copied! <pre># Uncomment and complete the code below:\n\n# arr_sevens = ...\n# arr_even = ...\n\n# print(\"Array of sevens:\", arr_sevens)\n# print(\"Array of even numbers:\", arr_even)\n</pre> # Uncomment and complete the code below:  # arr_sevens = ... # arr_even = ...  # print(\"Array of sevens:\", arr_sevens) # print(\"Array of even numbers:\", arr_even)   In\u00a0[7]: Copied! <pre>x = np.array([1, 2, 3])\ny = np.array([10, 20, 30])\n\nprint(\"x + y =\", x + y)       # [11 22 33]\nprint(\"x * 2 =\", x * 2)       # [2 4 6]\nprint(\"x * y =\", x * y)       # [10 40 90]\n</pre> x = np.array([1, 2, 3]) y = np.array([10, 20, 30])  print(\"x + y =\", x + y)       # [11 22 33] print(\"x * 2 =\", x * 2)       # [2 4 6] print(\"x * y =\", x * y)       # [10 40 90]  <pre>x + y = [11 22 33]\nx * 2 = [2 4 6]\nx * y = [10 40 90]\n</pre> <p>Array Statistics</p> <p>Many functions exist for statistical operations:</p> <ul> <li><code>mean()</code>, <code>sum()</code>, <code>max()</code>, <code>min()</code>, <code>std()</code> (standard deviation), <code>var()</code> (variance), etc.</li> </ul> In\u00a0[8]: Copied! <pre>arr = np.array([1, 2, 3, 4, 5])\nprint(\"Mean:\", arr.mean())    # 3.0\nprint(\"Sum:\", arr.sum())      # 15\nprint(\"Max:\", arr.max())      # 5\n</pre> arr = np.array([1, 2, 3, 4, 5]) print(\"Mean:\", arr.mean())    # 3.0 print(\"Sum:\", arr.sum())      # 15 print(\"Max:\", arr.max())      # 5  <pre>Mean: 3.0\nSum: 15\nMax: 5\n</pre> <p>Practice Question 2</p> <ol> <li><p>Create a 1D array of <code>[3, 5, 7, 9]</code> and store it in a variable called <code>test_arr</code>.</p> </li> <li><p>Print out the sum, mean, and standard deviation of <code>test_arr</code>.</p> </li> <li><p>Multiply <code>test_arr</code> by 4 and store it in <code>multiplied_arr</code>. Print <code>multiplied_arr</code>.</p> </li> </ol> In\u00a0[9]: Copied! <pre># Uncomment and complete the code below:\n\n# test_arr = ...\n# print(\"Sum:\", ...)\n# print(\"Mean:\", ...)\n# print(\"Std Dev:\", ...)\n\n# multiplied_arr = ...\n# print(\"multiplied_arr:\", multiplied_arr)\n</pre> # Uncomment and complete the code below:  # test_arr = ... # print(\"Sum:\", ...) # print(\"Mean:\", ...) # print(\"Std Dev:\", ...)  # multiplied_arr = ... # print(\"multiplied_arr:\", multiplied_arr)   In\u00a0[10]: Copied! <pre>mat = np.arange(1, 7)   # [1 2 3 4 5 6]\nmat_2d = mat.reshape(2, 3)\nprint(\"Original array:\", mat)\nprint(\"Reshaped to 2x3:\\n\", mat_2d)\n</pre> mat = np.arange(1, 7)   # [1 2 3 4 5 6] mat_2d = mat.reshape(2, 3) print(\"Original array:\", mat) print(\"Reshaped to 2x3:\\n\", mat_2d)  <pre>Original array: [1 2 3 4 5 6]\nReshaped to 2x3:\n [[1 2 3]\n [4 5 6]]\n</pre> In\u00a0[11]: Copied! <pre># 1D slicing\narr = np.array([10, 20, 30, 40, 50])\nprint(\"arr[1:3] =\", arr[1:3])       # [20 30]\n</pre> # 1D slicing arr = np.array([10, 20, 30, 40, 50]) print(\"arr[1:3] =\", arr[1:3])       # [20 30]   <pre>arr[1:3] = [20 30]\n</pre> <p>For 2D arrays we can use two ranges to slice rows and columns. For instance to slice the following 2D array into the highlighted section:</p> Index 0 1 2 3 0 11 12 13 14 1 21 22 23 24 2 31 32 33 34 In\u00a0[12]: Copied! <pre># 2D slicing\narray_2d = np.array([[11, 12, 13, 14], \n                      [21, 22, 23, 24], \n                      [31, 32, 33, 34]])\n\n# A slice would be\n\na_slice = array_2d[:2, 1:3]\n\nprint(a_slice)\n</pre> # 2D slicing array_2d = np.array([[11, 12, 13, 14],                        [21, 22, 23, 24],                        [31, 32, 33, 34]])  # A slice would be  a_slice = array_2d[:2, 1:3]  print(a_slice) <pre>[[12 13]\n [22 23]]\n</pre> <p>You can also use <code>:</code> to slice all rows or columns. For example, <code>arr[1,:]</code> selects all rows and columns 1 and 2.</p> Index 0 1 2 3 0 11 12 13 14 1 21 22 23 24 2 31 32 33 34 In\u00a0[13]: Copied! <pre>row_slice = array_2d[1,:]\n\nprint(row_slice)\n</pre> row_slice = array_2d[1,:]  print(row_slice) <pre>[21 22 23 24]\n</pre> <p>For slicing whole columns, use arr[:,1]</p> Index 0 1 2 3 0 11 12 13 14 1 21 22 23 24 2 31 32 33 34 In\u00a0[14]: Copied! <pre>column_array = array_2d[:, 1]\nprint(column_array)\n</pre> column_array = array_2d[:, 1] print(column_array) <pre>[12 22 32]\n</pre> <p>It is also possible to reverse slices from the end of the array using negative indices.</p> <ul> <li><code>arr[-1]</code>: last element</li> <li><code>arr[-2:]</code>: last two elements</li> <li><code>arr[:-1]</code>: all but the last element</li> <li><code>arr[-4:-2]</code>: o slice from the end of the array without the last two elements</li> <li><code>arr[:,-1]</code>: last column of a 2D array</li> </ul> In\u00a0[15]: Copied! <pre># 1d advanced slicing\nan_array_1d = np.array([10, 20, 30, 40, 50])\nprint(\"Last element:\", an_array_1d[-1])       # 50\nprint(\"Last two elements:\", an_array_1d[-2:]) # [40 50]\nprint(\"All but the last element:\", an_array_1d[:-1]) # [10 20 30 40]\nprint(\"Last elements except last two:\", an_array_1d[-4:-2]) # [20 30]\nprint(\"Last column of 2D array:\", array_2d[:,-1]) # [14 24 34]\n\n# 2d array slicing\narray_2d = np.array([[11, 12, 13, 14], \n                      [21, 22, 23, 24], \n                      [31, 32, 33, 34]])\n\nprint(\"Last row of 2D array:\", array_2d[-1]) # [31 32 33 34]\nprint(\"Last two rows of 2D array:\\n\", array_2d[-2:]) # [[21 22 23 24] [31 32 33 34]]\nprint(\"Last two rows and last two columns of 2D array:\\n\", array_2d[-2:, -2:]) # [[22 23] [32 33]]\n</pre> # 1d advanced slicing an_array_1d = np.array([10, 20, 30, 40, 50]) print(\"Last element:\", an_array_1d[-1])       # 50 print(\"Last two elements:\", an_array_1d[-2:]) # [40 50] print(\"All but the last element:\", an_array_1d[:-1]) # [10 20 30 40] print(\"Last elements except last two:\", an_array_1d[-4:-2]) # [20 30] print(\"Last column of 2D array:\", array_2d[:,-1]) # [14 24 34]  # 2d array slicing array_2d = np.array([[11, 12, 13, 14],                        [21, 22, 23, 24],                        [31, 32, 33, 34]])  print(\"Last row of 2D array:\", array_2d[-1]) # [31 32 33 34] print(\"Last two rows of 2D array:\\n\", array_2d[-2:]) # [[21 22 23 24] [31 32 33 34]] print(\"Last two rows and last two columns of 2D array:\\n\", array_2d[-2:, -2:]) # [[22 23] [32 33]]  <pre>Last element: 50\nLast two elements: [40 50]\nAll but the last element: [10 20 30 40]\nLast elements except last two: [20 30]\nLast column of 2D array: [14 24 34]\nLast row of 2D array: [31 32 33 34]\nLast two rows of 2D array:\n [[21 22 23 24]\n [31 32 33 34]]\nLast two rows and last two columns of 2D array:\n [[23 24]\n [33 34]]\n</pre> <p>Practice Question 3</p> <ol> <li>Create a 1D array of <code>[5, 10, 15, 20, 25, 30]</code>. Slice out <code>[15, 20, 25]</code> and print it.</li> <li>Create a 2D array of shape (3,4) using <code>np.arange(1, 13)</code> (it will contain 1 to 12). Then:<ul> <li>Reshape it to shape (3, 4).</li> <li>Print the second row only.</li> <li>Print the first two elements of the third row.</li> <li>Print the last two columns of the first two rows.</li> </ul> </li> </ol> In\u00a0[16]: Copied! <pre># Uncomment and complete the code below:\n\n# my_arr_1d = np.array([5, 10, 15, 20, 25, 30])\n# slice_result = ...\n# print(\"Slice result:\", slice_result)\n\n\n# my_arr_2d = ...\n# print(my_arr_2d)\n# print(\"Second row:\", ...)\n# print(\"First two elements of third row:\", ...)\n# print(\"Last two columns of the first two rows.\", ...)\n</pre> # Uncomment and complete the code below:  # my_arr_1d = np.array([5, 10, 15, 20, 25, 30]) # slice_result = ... # print(\"Slice result:\", slice_result)   # my_arr_2d = ... # print(my_arr_2d) # print(\"Second row:\", ...) # print(\"First two elements of third row:\", ...) # print(\"Last two columns of the first two rows.\", ...)   In\u00a0[17]: Copied! <pre>arr = np.array([10, 20, 30, 40, 50])\nidx = np.array([0,0,0,1,1,-1])  # positions to pick\nprint(\"Fancy indexing result:\", arr[idx])   # [30 10 40]\n</pre> arr = np.array([10, 20, 30, 40, 50]) idx = np.array([0,0,0,1,1,-1])  # positions to pick print(\"Fancy indexing result:\", arr[idx])   # [30 10 40]  <pre>Fancy indexing result: [10 10 10 20 20 50]\n</pre> In\u00a0[18]: Copied! <pre>arr = np.array([10, 20, 30, 40, 50])\nmask = arr &gt; 25\nprint(\"Mask array:\", mask)        # [False False  True  True  True]\nprint(\"Filtered:\", arr[mask])     # [30 40 50]\n\narr[arr &lt; 40]\n</pre> arr = np.array([10, 20, 30, 40, 50]) mask = arr &gt; 25 print(\"Mask array:\", mask)        # [False False  True  True  True] print(\"Filtered:\", arr[mask])     # [30 40 50]  arr[arr &lt; 40]  <pre>Mask array: [False False  True  True  True]\nFiltered: [30 40 50]\n</pre> Out[18]: <pre>array([10, 20, 30])</pre> <p>Practice Question 4</p> <ol> <li><p>Create an array of <code>[2, 4, 6, 8, 10, 12]</code>. Use fancy indexing to pick out <code>[10, 4, 2]</code> in that order.</p> </li> <li><p>Create a boolean mask that selects the elements of this array that are greater than 6.</p> </li> </ol> In\u00a0[19]: Copied! <pre># Uncomment and complete the code below:\n\n# test_fancy = np.array([2, 4, 6, 8, 10, 12])\n# fancy_indices = ...\n# print(\"Fancy selection:\", ...)\n\n# print(\"Elements greater than 6:\", ...)\n</pre> # Uncomment and complete the code below:  # test_fancy = np.array([2, 4, 6, 8, 10, 12]) # fancy_indices = ... # print(\"Fancy selection:\", ...)  # print(\"Elements greater than 6:\", ...)   In\u00a0[20]: Copied! <pre>a = np.array([[1, 2, 3],\n              [4, 5, 6]])\nb = np.array([10, 20, 30])\n\nprint(\"Result of a + b:\\n\", a + b)\n# The 1D array b is \"broadcasted\" across each row of a:\n</pre> a = np.array([[1, 2, 3],               [4, 5, 6]]) b = np.array([10, 20, 30])  print(\"Result of a + b:\\n\", a + b) # The 1D array b is \"broadcasted\" across each row of a:   <pre>Result of a + b:\n [[11 22 33]\n [14 25 36]]\n</pre> <p>Practice Question 5</p> <ol> <li><p>Create a 2D array <code>[[1,2],[3,4]]</code>.</p> </li> <li><p>Create a 1D array <code>[10, 20]</code>.</p> </li> <li><p>Add them using broadcasting and print the result.</p> </li> <li><p>(Optional) Predict the shape and values before running the code.</p> </li> </ol> In\u00a0[21]: Copied! <pre># Uncomment and complete the code below:\n\n# arr_2d = np.array([[1,2],[3,4]])\n# arr_1d = np.array([10, 20])\n# result_broadcast = ...\n# print(\"Broadcasting result:\\n\", result_broadcast)\n</pre> # Uncomment and complete the code below:  # arr_2d = np.array([[1,2],[3,4]]) # arr_1d = np.array([10, 20]) # result_broadcast = ... # print(\"Broadcasting result:\\n\", result_broadcast)   In\u00a0[22]: Copied! <pre># Random numbers\nrand_arr = np.random.rand(3, 2)    # uniform in [0,1)\nrandn_arr = np.random.randn(3, 2)  # normal distribution (mean=0, std=1)\nrandint_arr = np.random.randint(5, 15, size=(2,3)) # random ints between 5 and 14\n\nprint(\"Uniform random:\\n\", rand_arr)\nprint(\"Shape:\", rand_arr.shape)\nprint(\"\\nNormal random:\\n\", randn_arr)\nprint(\"Shape:\", randn_arr.shape)\nprint(\"\\nInteger random:\\n\", randint_arr)\nprint(\"Shape:\", randint_arr.shape)\n</pre> # Random numbers rand_arr = np.random.rand(3, 2)    # uniform in [0,1) randn_arr = np.random.randn(3, 2)  # normal distribution (mean=0, std=1) randint_arr = np.random.randint(5, 15, size=(2,3)) # random ints between 5 and 14  print(\"Uniform random:\\n\", rand_arr) print(\"Shape:\", rand_arr.shape) print(\"\\nNormal random:\\n\", randn_arr) print(\"Shape:\", randn_arr.shape) print(\"\\nInteger random:\\n\", randint_arr) print(\"Shape:\", randint_arr.shape)  <pre>Uniform random:\n [[0.14331159 0.49664192]\n [0.2732867  0.3803681 ]\n [0.04899517 0.47154495]]\nShape: (3, 2)\n\nNormal random:\n [[ 0.26105074  2.81767613]\n [-0.04306999 -1.44551721]\n [-0.64460383 -0.66513559]]\nShape: (3, 2)\n\nInteger random:\n [[10  6  6]\n [12 12 11]]\nShape: (2, 3)\n</pre> <p>Practice Question 6</p> <ol> <li><p>Generate a 1D array of 5 random integers between 0 and 10.</p> </li> <li><p>Print its shape, and number of dimensions.</p> </li> <li><p>Reshape it to <code>(5, 1)</code> and print the new shape.</p> </li> </ol> In\u00a0[23]: Copied! <pre># Uncomment and complete the code below:\n\n# rand_ints = ...\n# print(rand_ints)\n# print(\"Shape of rand_ints:\", ...)\n# print(\"Number of dimensions:\", ...)\n\n# reshaped = rand_ints.reshape(5,1)\n# print(\"New shape:\", reshaped.shape)\n# print(reshaped)\n</pre> # Uncomment and complete the code below:  # rand_ints = ... # print(rand_ints) # print(\"Shape of rand_ints:\", ...) # print(\"Number of dimensions:\", ...)  # reshaped = rand_ints.reshape(5,1) # print(\"New shape:\", reshaped.shape) # print(reshaped)    In\u00a0[24]: Copied! <pre># Your solution:\n# 1. Create a 1D NumPy array of the first 10 positive integers\n\n# 2. Create a 2x5 array of zeros\n\n# 3. Create a 4x4 identity matrix\n</pre> # Your solution: # 1. Create a 1D NumPy array of the first 10 positive integers  # 2. Create a 2x5 array of zeros  # 3. Create a 4x4 identity matrix   In\u00a0[25]: Copied! <pre># Your solution:\narr = np.array([5, 10, 15, 20, 25])\n\n# 1. Multiply each element by 3\n\n# 2. Subtract 5 from each element\n\n# 3. Compute the sum of all elements\n</pre> # Your solution: arr = np.array([5, 10, 15, 20, 25])  # 1. Multiply each element by 3  # 2. Subtract 5 from each element  # 3. Compute the sum of all elements   In\u00a0[26]: Copied! <pre># Your solution:\n\n# 1. Create a 1D array of integers from 1 to 12\n\n# 2. Reshape it into a 3x4 matrix\n\n# 3. Extract the element in the 2nd row, 3rd column\n\n# 4. Extract the first column as a 1D array\n</pre> # Your solution:  # 1. Create a 1D array of integers from 1 to 12  # 2. Reshape it into a 3x4 matrix  # 3. Extract the element in the 2nd row, 3rd column  # 4. Extract the first column as a 1D array   In\u00a0[27]: Copied! <pre># Your solution:\narr = np.array([100, 200, 300, 400, 500])\nindices = np.array([4, 0, 2])\n\n# 1. Use fancy indexing to extract elements\n\n# 2. Rearrange the extracted elements into descending order\n</pre> # Your solution: arr = np.array([100, 200, 300, 400, 500]) indices = np.array([4, 0, 2])  # 1. Use fancy indexing to extract elements  # 2. Rearrange the extracted elements into descending order   In\u00a0[28]: Copied! <pre># Your solution:\narr = np.array([1, 4, 7, 10, 13, 16])\n\n# 1. Create a boolean mask for elements greater than 8\n\n# 2. Use the mask to extract those elements\n\n# 3. Compute the mean of the extracted elements\n</pre> # Your solution: arr = np.array([1, 4, 7, 10, 13, 16])  # 1. Create a boolean mask for elements greater than 8  # 2. Use the mask to extract those elements  # 3. Compute the mean of the extracted elements   In\u00a0[29]: Copied! <pre># Your solution:\na = np.array([[1, 2, 3], [4, 5, 6]])\nb = np.array([10, 20, 30])\n\n# 1. Add `b` to each row of `a`\n\n# 2. Multiply each element of `a` by 2\n\n# 3. Compute the sum of all elements\n</pre> # Your solution: a = np.array([[1, 2, 3], [4, 5, 6]]) b = np.array([10, 20, 30])  # 1. Add `b` to each row of `a`  # 2. Multiply each element of `a` by 2  # 3. Compute the sum of all elements   In\u00a0[30]: Copied! <pre># Your solution:\n\n# 1. Generate a 3x3 array of random values uniformly distributed between 0 and 1\n\n# 2. Generate a 3x3 array of random values drawn from a standard normal distribution\n\n# 3. Compute the mean and standard deviation of each array\n</pre> # Your solution:  # 1. Generate a 3x3 array of random values uniformly distributed between 0 and 1  # 2. Generate a 3x3 array of random values drawn from a standard normal distribution  # 3. Compute the mean and standard deviation of each array"},{"location":"m03-numpy_pandas/numpy_basics/#numpy-tutorial","title":"NumPy Tutorial\u00b6","text":"<p>This tutorial provides a detailed introduction to NumPy, one of the most widely used libraries for numerical computing in Python. By the end, you should have a good grasp on essential NumPy operations, including array creation, reshaping, indexing, fancy indexing, boolean masking, broadcasting, and random utilities.</p> <p>Throughout, we\u2019ve added extra explanations and short practice questions to help you test your knowledge. The questions require code-based solutions, so simply uncomment and fill them in as you go.</p>"},{"location":"m03-numpy_pandas/numpy_basics/#1-installation-and-import","title":"1. Installation and Import\u00b6","text":"<p>NumPy doesn't come bundled with Python, so you need to install it (unless you\u2019re using a distribution like Anaconda, which comes with NumPy pre-installed). If you set up your Python environment with our previous tutorial, you should already have NumPy installed.</p> <p>1.1 Install NumPy (if you haven\u2019t already):</p> <pre>pip install numpy\n</pre> <p>This downloads and installs the NumPy package from the Python Package Index (PyPI).</p> <p>1.2 Import NumPy:</p> <p>The <code>as np</code> alias is a common convention that makes it easier to reference NumPy throughout your code.</p>"},{"location":"m03-numpy_pandas/numpy_basics/#2-creating-arrays","title":"2. Creating Arrays\u00b6","text":"<p>NumPy arrays (<code>ndarray</code> objects) are the core data structure for fast numerical computations in Python. They are more efficient than standard Python lists, especially for operations on large datasets.</p>"},{"location":"m03-numpy_pandas/numpy_basics/#21-from-python-lists","title":"2.1 From Python Lists\u00b6","text":"<p>You can create a NumPy array simply by passing a Python list to <code>np.array()</code>:</p>"},{"location":"m03-numpy_pandas/numpy_basics/#22-using-built-in-numpy-functions","title":"2.2 Using Built-in NumPy Functions\u00b6","text":"<p>NumPy provides several functions to generate arrays of specific shapes and values:</p> <ul> <li><code>np.zeros(shape)</code>: Creates an array filled with zeros.</li> <li><code>np.ones(shape)</code>: Creates an array filled with ones.</li> <li><code>np.full(shape, fill_value)</code>: Creates an array filled with a specified value.</li> <li><code>np.arange(start, stop, step)</code>: Creates a range of values from start (inclusive) to stop (exclusive) with a given step.</li> <li><code>np.linspace(start, stop, num)</code>: Creates a set of evenly spaced numbers between start and stop.</li> </ul>"},{"location":"m03-numpy_pandas/numpy_basics/#23-array-dimensions-shape","title":"2.3 Array dimensions (shape)\u00b6","text":"<p>Numpy supports multi-dimensional arrays, which are arrays with more than one dimension. For example, a 2D array can be thought of as a matrix with rows and columns, while a 3D array can be thought of as a stack of matrices.</p> <p>The shape of an array is a tuple that indicates the size of each dimension. For example, a 2D array with 3 rows and 4 columns has a shape of <code>(3, 4)</code>. Additionally, you can use <code>array.shape</code> to retrieve the shape of a NumPy array, where <code>array</code> is your NumPy array instance.</p>"},{"location":"m03-numpy_pandas/numpy_basics/#3-basic-operations","title":"3. Basic Operations\u00b6","text":"<p>NumPy arrays support element-wise arithmetic. This means that when you perform operations on arrays of the same shape, the operation is applied element by element.</p>"},{"location":"m03-numpy_pandas/numpy_basics/#4-reshaping-and-indexing","title":"4. Reshaping and Indexing\u00b6","text":"<p>Reshaping allows you to change the shape of an array without altering its data. Indexing and slicing let you access or modify portions of an array.</p>"},{"location":"m03-numpy_pandas/numpy_basics/#41-reshape","title":"4.1 Reshape\u00b6","text":"<p><code>reshape(new_shape)</code> rearranges the elements to fit into the new shape.</p>"},{"location":"m03-numpy_pandas/numpy_basics/#42-slicing","title":"4.2 Slicing\u00b6","text":"<p>Slicing in NumPy works similarly to Python lists, but you can do more advanced slicing for multi-dimensional arrays.</p> <ul> <li><p><code>arr[start:end]</code>: slices from <code>start</code> to <code>end-1</code> in one dimension.</p> </li> <li><p>For multi-dimensional arrays: <code>arr[row_start:row_end, col_start:col_end]</code>.</p> </li> </ul>"},{"location":"m03-numpy_pandas/numpy_basics/#5-fancy-indexing-and-boolean-masking","title":"5. Fancy Indexing and Boolean Masking\u00b6","text":""},{"location":"m03-numpy_pandas/numpy_basics/#51-fancy-indexing","title":"5.1 Fancy Indexing\u00b6","text":"<p>You can pass an array of indices to directly pick elements in the order you want.</p>"},{"location":"m03-numpy_pandas/numpy_basics/#52-boolean-masking","title":"5.2 Boolean Masking\u00b6","text":"<p>Boolean masking lets you filter an array based on a condition, returning only the elements that satisfy that condition.</p>"},{"location":"m03-numpy_pandas/numpy_basics/#6-broadcasting","title":"6. Broadcasting\u00b6","text":"<p>Broadcasting describes how NumPy automatically handles arithmetic between arrays of different shapes. When possible, NumPy \u201cbroadcasts\u201d the smaller array so that it matches the dimensions of the larger array.</p> <p>For instance, adding a 1D array to a 2D array along a compatible axis:</p>"},{"location":"m03-numpy_pandas/numpy_basics/#7-random-and-useful-utilities","title":"7. Random and Useful Utilities\u00b6","text":"<p>NumPy offers a variety of random number generation functions in the <code>np.random</code> module:</p> <ul> <li><p><code>np.random.rand(shape)</code>: Uniform distribution over <code>[0, 1)</code>.</p> </li> <li><p><code>np.random.randn(shape)</code>: Normal distribution with mean=0, std=1.</p> </li> <li><p><code>np.random.randint(low, high, size)</code>: Random integers from <code>low</code> to <code>high-1</code>.</p> </li> </ul> <p>Inspecting shapes and dimensions:</p> <ul> <li><p><code>.shape</code>: Returns the shape (rows, columns).</p> </li> <li><p><code>.ndim</code>: Returns the number of dimensions.</p> </li> <li><p><code>.size</code>: Returns the total number of elements.</p> </li> </ul>"},{"location":"m03-numpy_pandas/numpy_basics/#questions","title":"Questions\u00b6","text":"<p>Solve the following exercises to practice NumPy basics. Follow the instructions and fill in the blanks where indicated.</p> <p>There is no need to submit this notebook; it's for your practice only.</p>"},{"location":"m03-numpy_pandas/numpy_basics/#1-array-creation-and-initialization","title":"1. Array Creation and Initialization\u00b6","text":"<p>Task:</p> <ol> <li><p>Create a 1D NumPy array of the first 10 positive integers.</p> </li> <li><p>Create a 2x5 array of zeros.</p> </li> <li><p>Create a 4x4 identity matrix.</p> </li> </ol>"},{"location":"m03-numpy_pandas/numpy_basics/#2-array-operations","title":"2. Array Operations\u00b6","text":"<p>Task:</p> <p>Given the array <code>arr = np.array([5, 10, 15, 20, 25])</code>, do the following:</p> <ol> <li><p>Multiply each element by 3.</p> </li> <li><p>Subtract 5 from each element.</p> </li> <li><p>Compute the sum of all elements in the resulting array.</p> </li> </ol>"},{"location":"m03-numpy_pandas/numpy_basics/#3-reshaping-and-indexing","title":"3. Reshaping and Indexing\u00b6","text":"<p>Task:</p> <ol> <li><p>Create a 1D array of integers from 1 to 12.</p> </li> <li><p>Reshape it into a 3x4 matrix.</p> </li> <li><p>Extract the element in the 2nd row, 3rd column.</p> </li> <li><p>Extract the first column as a 1D array.</p> </li> </ol>"},{"location":"m03-numpy_pandas/numpy_basics/#4-fancy-indexing","title":"4. Fancy Indexing\u00b6","text":"<p>Task:</p> <p>Given <code>arr = np.array([100, 200, 300, 400, 500])</code> and <code>indices = np.array([4, 0, 2])</code>:</p> <ol> <li><p>Use fancy indexing to extract the elements at positions defined by <code>indices</code>.</p> </li> <li><p>Rearrange the extracted elements into descending order.</p> </li> </ol>"},{"location":"m03-numpy_pandas/numpy_basics/#5-boolean-masking","title":"5. Boolean Masking\u00b6","text":"<p>Task:</p> <p>Given <code>arr = np.array([1, 4, 7, 10, 13, 16])</code>:</p> <ol> <li><p>Create a boolean mask for elements greater than 8.</p> </li> <li><p>Use the mask to extract those elements.</p> </li> <li><p>Compute the mean of the extracted elements.</p> </li> </ol>"},{"location":"m03-numpy_pandas/numpy_basics/#6-broadcasting","title":"6. Broadcasting\u00b6","text":"<p>Task:</p> <p>Given the 2D array <code>a = np.array([[1, 2, 3], [4, 5, 6]])</code>:</p> <ol> <li><p>Add the 1D array <code>b = np.array([10, 20, 30])</code> to each row of <code>a</code>.</p> </li> <li><p>Multiply each element of <code>a</code> by 2.</p> </li> <li><p>Compute the sum of all elements in the resulting array.</p> </li> </ol>"},{"location":"m03-numpy_pandas/numpy_basics/#7-random-array-utilities","title":"7. Random Array Utilities\u00b6","text":"<p>Task:</p> <ol> <li><p>Generate a 3x3 array of random values uniformly distributed between 0 and 1.</p> </li> <li><p>Generate a 3x3 array of random values drawn from a standard normal distribution.</p> </li> <li><p>Compute the mean and standard deviation of each array.</p> </li> </ol>"},{"location":"m03-numpy_pandas/pandas_basics/","title":"Pandas Tutorial","text":"In\u00a0[1]: Copied! <pre># No need to install pandas, it is already included in our environment.\n# However if you are not using our environment, you can install pandas using the command:\n\n#!pip install pandas\n</pre> # No need to install pandas, it is already included in our environment. # However if you are not using our environment, you can install pandas using the command:  #!pip install pandas   In\u00a0[2]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd  In\u00a0[3]: Copied! <pre># Creating a Pandas Series from a list\ndata_series = pd.Series([10, 20, 30, 40])\ndisplay(\"Pandas Series:\")\ndisplay(data_series)\n\n# # # Demonstrate that DataFrame columns are Series\ndf_series_example = pd.DataFrame({\n    \"Numbers\": data_series, \n    \"Squared\": data_series ** 2\n})\ndisplay(\"\\nDataFrame constructed from Series:\")\ndisplay(df_series_example)\n</pre> # Creating a Pandas Series from a list data_series = pd.Series([10, 20, 30, 40]) display(\"Pandas Series:\") display(data_series)  # # # Demonstrate that DataFrame columns are Series df_series_example = pd.DataFrame({     \"Numbers\": data_series,      \"Squared\": data_series ** 2 }) display(\"\\nDataFrame constructed from Series:\") display(df_series_example)  <pre>'Pandas Series:'</pre> <pre>0    10\n1    20\n2    30\n3    40\ndtype: int64</pre> <pre>'\\nDataFrame constructed from Series:'</pre> Numbers Squared 0 10 100 1 20 400 2 30 900 3 40 1600 In\u00a0[4]: Copied! <pre>data = {\n    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"Age\": [25, 30, 35],\n    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"]\n}\n\ndf = pd.DataFrame(data)\ndisplay(df)\n</pre> data = {     \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],     \"Age\": [25, 30, 35],     \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"] }  df = pd.DataFrame(data) display(df)  Name Age City 0 Alice 25 New York 1 Bob 30 Los Angeles 2 Charlie 35 Chicago In\u00a0[5]: Copied! <pre>data_list = [\n    {\"Name\": \"Alice\",   \"Age\": 25, \"City\": \"New York\"},\n    {\"Name\": \"Bob\",     \"Age\": 30, \"City\": \"Los Angeles\"},\n    {\"Name\": \"Charlie\", \"Age\": 35, \"City\": \"Chicago\"},\n    {\"Name\": \"Vijay\", \"Age\": 28, \"City\": \"St. Louis\"},\n    {\"Name\": \"Jin\", \"Age\": 35, \"City\": \"Orlando\"},\n    {\"Name\": \"Lucas\", \"Age\": 31, \"City\": \"Bloomington\"}\n]\ndf2 = pd.DataFrame(data_list)\ndisplay(df2)\n</pre> data_list = [     {\"Name\": \"Alice\",   \"Age\": 25, \"City\": \"New York\"},     {\"Name\": \"Bob\",     \"Age\": 30, \"City\": \"Los Angeles\"},     {\"Name\": \"Charlie\", \"Age\": 35, \"City\": \"Chicago\"},     {\"Name\": \"Vijay\", \"Age\": 28, \"City\": \"St. Louis\"},     {\"Name\": \"Jin\", \"Age\": 35, \"City\": \"Orlando\"},     {\"Name\": \"Lucas\", \"Age\": 31, \"City\": \"Bloomington\"} ] df2 = pd.DataFrame(data_list) display(df2)   Name Age City 0 Alice 25 New York 1 Bob 30 Los Angeles 2 Charlie 35 Chicago 3 Vijay 28 St. Louis 4 Jin 35 Orlando 5 Lucas 31 Bloomington In\u00a0[6]: Copied! <pre>print(\"First 2 rows:\")\ndisplay(df.head(2))       # First 5 rows (use df.head(10) for the first 10)\nprint(\"\\nLast 2 rows:\")\ndisplay(df.tail(2))       # Last 5 rows\nprint(\"\\nShape of DataFrame:\")\ndisplay(df.shape)        # (rows, columns)\nprint(\"\\nColumn Names:\")\ndisplay(df.columns)      # List of column names\nprint(\"\\nDataFrame Info:\")\ndisplay(df.info())       # Summary of the DataFrame (types, non-null counts)\nprint(\"\\nStatistical Summary:\")\ndisplay(df.describe())   # Basic statistics for numeric columns\n</pre> print(\"First 2 rows:\") display(df.head(2))       # First 5 rows (use df.head(10) for the first 10) print(\"\\nLast 2 rows:\") display(df.tail(2))       # Last 5 rows print(\"\\nShape of DataFrame:\") display(df.shape)        # (rows, columns) print(\"\\nColumn Names:\") display(df.columns)      # List of column names print(\"\\nDataFrame Info:\") display(df.info())       # Summary of the DataFrame (types, non-null counts) print(\"\\nStatistical Summary:\") display(df.describe())   # Basic statistics for numeric columns  <pre>First 2 rows:\n</pre> Name Age City 0 Alice 25 New York 1 Bob 30 Los Angeles <pre>\nLast 2 rows:\n</pre> Name Age City 1 Bob 30 Los Angeles 2 Charlie 35 Chicago <pre>\nShape of DataFrame:\n</pre> <pre>(3, 3)</pre> <pre>\nColumn Names:\n</pre> <pre>Index(['Name', 'Age', 'City'], dtype='object')</pre> <pre>\nDataFrame Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   Name    3 non-null      object\n 1   Age     3 non-null      int64 \n 2   City    3 non-null      object\ndtypes: int64(1), object(2)\nmemory usage: 204.0+ bytes\n</pre> <pre>None</pre> <pre>\nStatistical Summary:\n</pre> Age count 3.0 mean 30.0 std 5.0 min 25.0 25% 27.5 50% 30.0 75% 32.5 max 35.0 In\u00a0[7]: Copied! <pre># Your solution here:\n# 1. Print the first 3 rows.\n# 2. Print the list of column names.\n# 3. Display the DataFrame information.\n</pre> # Your solution here: # 1. Print the first 3 rows. # 2. Print the list of column names. # 3. Display the DataFrame information.  In\u00a0[8]: Copied! <pre># Dot notation (for simple column names without spaces/special chars)\ndisplay(\"Using dot notation to access 'Age':\")\ndisplay(df.Age)\n\n# # Bracket notation\ndisplay(\"\\nUsing bracket notation to access 'Age':\")\ndisplay(df[\"Age\"])\n</pre> # Dot notation (for simple column names without spaces/special chars) display(\"Using dot notation to access 'Age':\") display(df.Age)  # # Bracket notation display(\"\\nUsing bracket notation to access 'Age':\") display(df[\"Age\"])  <pre>\"Using dot notation to access 'Age':\"</pre> <pre>0    25\n1    30\n2    35\nName: Age, dtype: int64</pre> <pre>\"\\nUsing bracket notation to access 'Age':\"</pre> <pre>0    25\n1    30\n2    35\nName: Age, dtype: int64</pre> In\u00a0[9]: Copied! <pre>df = pd.DataFrame({\n    \"Name\": [\"Alice\", \"Bob\", \"Charlie\", \"Dave\"],\n    \"Age\": [25, 30, 35, 28],\n    \"City\": [\"NY\", \"LA\", \"Chicago\", \"NY\"]\n}, index=[\"row1\", \"row2\", \"row3\", \"row4\"])  # custom index labels\n\ndisplay(df)\ndisplay(\"Using .loc (label-based):\")\ndisplay(df.loc[\"row2\"])               # Entire row labeled 'row2'\ndisplay(df.loc[\"row2\", \"Age\"])        # Specific cell (row2, Age)\ndisplay(df.loc[\"row1\":\"row3\"])        # Slice multiple rows by label\ndisplay(df.loc[:, [\"Name\", \"City\"]])  # All rows, only these columns\n\ndisplay(\"\\nUsing .iloc (integer-based):\")\ndisplay(df.iloc[1])                   # 2nd row (since indexing starts at 0)\ndisplay(df.iloc[1, 1])                # Cell at row index 1, col index 1\ndisplay(df.iloc[0:2])                 # Rows 0 to 1\ndisplay(df.iloc[:, [0, 2]])           # All rows, columns 0 and 2\n</pre> df = pd.DataFrame({     \"Name\": [\"Alice\", \"Bob\", \"Charlie\", \"Dave\"],     \"Age\": [25, 30, 35, 28],     \"City\": [\"NY\", \"LA\", \"Chicago\", \"NY\"] }, index=[\"row1\", \"row2\", \"row3\", \"row4\"])  # custom index labels  display(df) display(\"Using .loc (label-based):\") display(df.loc[\"row2\"])               # Entire row labeled 'row2' display(df.loc[\"row2\", \"Age\"])        # Specific cell (row2, Age) display(df.loc[\"row1\":\"row3\"])        # Slice multiple rows by label display(df.loc[:, [\"Name\", \"City\"]])  # All rows, only these columns  display(\"\\nUsing .iloc (integer-based):\") display(df.iloc[1])                   # 2nd row (since indexing starts at 0) display(df.iloc[1, 1])                # Cell at row index 1, col index 1 display(df.iloc[0:2])                 # Rows 0 to 1 display(df.iloc[:, [0, 2]])           # All rows, columns 0 and 2  Name Age City row1 Alice 25 NY row2 Bob 30 LA row3 Charlie 35 Chicago row4 Dave 28 NY <pre>'Using .loc (label-based):'</pre> <pre>Name    Bob\nAge      30\nCity     LA\nName: row2, dtype: object</pre> <pre>30</pre> Name Age City row1 Alice 25 NY row2 Bob 30 LA row3 Charlie 35 Chicago Name City row1 Alice NY row2 Bob LA row3 Charlie Chicago row4 Dave NY <pre>'\\nUsing .iloc (integer-based):'</pre> <pre>Name    Bob\nAge      30\nCity     LA\nName: row2, dtype: object</pre> <pre>30</pre> Name Age City row1 Alice 25 NY row2 Bob 30 LA Name City row1 Alice NY row2 Bob LA row3 Charlie Chicago row4 Dave NY In\u00a0[10]: Copied! <pre># Show only rows where Age &gt; 28\nmask = df[\"Age\"] &gt; 28\ndisplay(mask)\nolder_than_28 = df[mask]\ndisplay(\"Rows where Age &gt; 28:\")\ndisplay(older_than_28)\n</pre> # Show only rows where Age &gt; 28 mask = df[\"Age\"] &gt; 28 display(mask) older_than_28 = df[mask] display(\"Rows where Age &gt; 28:\") display(older_than_28)  <pre>row1    False\nrow2     True\nrow3     True\nrow4    False\nName: Age, dtype: bool</pre> <pre>'Rows where Age &gt; 28:'</pre> Name Age City row2 Bob 30 LA row3 Charlie 35 Chicago In\u00a0[11]: Copied! <pre># People older than 25 AND living in NY\ndf_filtered = df[(df[\"Age\"] &gt; 25) &amp; (df[\"City\"] == \"NY\")]\ndisplay(\"Rows where Age &gt; 25 and City is NY:\")\ndisplay(df_filtered)\n</pre> # People older than 25 AND living in NY df_filtered = df[(df[\"Age\"] &gt; 25) &amp; (df[\"City\"] == \"NY\")] display(\"Rows where Age &gt; 25 and City is NY:\") display(df_filtered)  <pre>'Rows where Age &gt; 25 and City is NY:'</pre> Name Age City row4 Dave 28 NY <p>Alternatively, you can use the <code>query()</code> method for more complex filtering.</p> <p>Check the official documentation for more details.</p> <p>https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#query</p> In\u00a0[12]: Copied! <pre>df_filtered_query = df.query(\"Age &gt; 25 and City == 'NY'\")\ndisplay(\"Rows where Age &gt; 25 and City is NY (using query):\")\ndisplay(df_filtered_query)\n</pre> df_filtered_query = df.query(\"Age &gt; 25 and City == 'NY'\") display(\"Rows where Age &gt; 25 and City is NY (using query):\") display(df_filtered_query)  <pre>'Rows where Age &gt; 25 and City is NY (using query):'</pre> Name Age City row4 Dave 28 NY In\u00a0[13]: Copied! <pre># Your solution here:\n# For example, create your mask and apply it to df.\n</pre> # Your solution here: # For example, create your mask and apply it to df.  In\u00a0[14]: Copied! <pre>display(df)\ndf.loc[\"row1\", \"Age\"] = 26\ndisplay(\"After modifying using .loc:\")\ndisplay(df)\n</pre> display(df) df.loc[\"row1\", \"Age\"] = 26 display(\"After modifying using .loc:\") display(df)  Name Age City row1 Alice 25 NY row2 Bob 30 LA row3 Charlie 35 Chicago row4 Dave 28 NY <pre>'After modifying using .loc:'</pre> Name Age City row1 Alice 26 NY row2 Bob 30 LA row3 Charlie 35 Chicago row4 Dave 28 NY In\u00a0[15]: Copied! <pre>df.iloc[0, 1] = 27\ndisplay(\"After modifying using .iloc:\")\ndisplay(df)\n</pre> df.iloc[0, 1] = 27 display(\"After modifying using .iloc:\") display(df)  <pre>'After modifying using .iloc:'</pre> Name Age City row1 Alice 27 NY row2 Bob 30 LA row3 Charlie 35 Chicago row4 Dave 28 NY In\u00a0[16]: Copied! <pre># Increase everyone's Age by 1\ndf[\"Age\"] = df[\"Age\"] + 1\ndisplay(\"After increasing Age by 1:\")\ndisplay(df)\n</pre> # Increase everyone's Age by 1 df[\"Age\"] = df[\"Age\"] + 1 display(\"After increasing Age by 1:\") display(df)  <pre>'After increasing Age by 1:'</pre> Name Age City row1 Alice 28 NY row2 Bob 31 LA row3 Charlie 36 Chicago row4 Dave 29 NY In\u00a0[17]: Copied! <pre>df[\"Age_squared\"] = df[\"Age\"].apply(lambda x: x*x)\ndisplay(\"After applying lambda function to square Age:\")\ndisplay(df)\n\n# ### 6.5. Creating new columns\n#\n# Create a new column based on existing columns.\n</pre> df[\"Age_squared\"] = df[\"Age\"].apply(lambda x: x*x) display(\"After applying lambda function to square Age:\") display(df)  # ### 6.5. Creating new columns # # Create a new column based on existing columns. <pre>'After applying lambda function to square Age:'</pre> Name Age City Age_squared row1 Alice 28 NY 784 row2 Bob 31 LA 961 row3 Charlie 36 Chicago 1296 row4 Dave 29 NY 841 In\u00a0[18]: Copied! <pre>df[\"Age in 5 years\"] = df[\"Age\"] + 5\ndisplay(\"After creating new column 'Age in 5 years':\")\ndisplay(df)\n</pre> df[\"Age in 5 years\"] = df[\"Age\"] + 5 display(\"After creating new column 'Age in 5 years':\") display(df)  <pre>\"After creating new column 'Age in 5 years':\"</pre> Name Age City Age_squared Age in 5 years row1 Alice 28 NY 784 33 row2 Bob 31 LA 961 36 row3 Charlie 36 Chicago 1296 41 row4 Dave 29 NY 841 34 <p>Sometimes this direct assignment may lead to problems. In particular if you are modifying a view of a DataFrame, it may not behave as expected.</p> <p>In this case you should use <code>df.loc[]</code> to ensure you are modifying the original DataFrame.</p> <p>An alternative is to copy the DataFrame first using <code>df.copy()</code>.</p> In\u00a0[19]: Copied! <pre># For example if you modify a slice of a DataFrame, it may not behave as expected.\ndf_slice = df.query(\"Age &gt; 30\")\ndf_slice[\"Age plus 10\"] = df_slice[\"Age\"] + 10\ndisplay(\"After modifying a slice of the DataFrame:\")\ndisplay(df_slice)\ndisplay(df)\n</pre> # For example if you modify a slice of a DataFrame, it may not behave as expected. df_slice = df.query(\"Age &gt; 30\") df_slice[\"Age plus 10\"] = df_slice[\"Age\"] + 10 display(\"After modifying a slice of the DataFrame:\") display(df_slice) display(df)  <pre>/var/folders/jh/xkyk5yn976z_y46xvbg2kjjm0000gn/T/ipykernel_65888/1240736968.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_slice[\"Age plus 10\"] = df_slice[\"Age\"] + 10\n</pre> <pre>'After modifying a slice of the DataFrame:'</pre> Name Age City Age_squared Age in 5 years Age plus 10 row2 Bob 31 LA 961 36 41 row3 Charlie 36 Chicago 1296 41 46 Name Age City Age_squared Age in 5 years row1 Alice 28 NY 784 33 row2 Bob 31 LA 961 36 row3 Charlie 36 Chicago 1296 41 row4 Dave 29 NY 841 34 <p>The correct way to do this is to use <code>df.loc[]</code> or copy the DataFrame first.</p> In\u00a0[20]: Copied! <pre># Modify the original DataFrame using .loc\nmask = df.Age&gt;30\ndf.loc[mask, \"Age plus 10\"] = df_slice[\"Age\"] + 10\ndisplay(\"After modifying the original DataFrame using .loc:\")\ndisplay(df)\n\n# Or copy the DataFrame first\ndf_slice = df.query(\"Age &gt; 30\").copy()\ndf_slice[\"Age plus 10\"] = df_slice[\"Age\"] + 10\ndisplay(\"After modifying a copy of the slice of the DataFrame:\")\ndisplay(df_slice)\n</pre> # Modify the original DataFrame using .loc mask = df.Age&gt;30 df.loc[mask, \"Age plus 10\"] = df_slice[\"Age\"] + 10 display(\"After modifying the original DataFrame using .loc:\") display(df)  # Or copy the DataFrame first df_slice = df.query(\"Age &gt; 30\").copy() df_slice[\"Age plus 10\"] = df_slice[\"Age\"] + 10 display(\"After modifying a copy of the slice of the DataFrame:\") display(df_slice)   <pre>'After modifying the original DataFrame using .loc:'</pre> Name Age City Age_squared Age in 5 years Age plus 10 row1 Alice 28 NY 784 33 NaN row2 Bob 31 LA 961 36 41.0 row3 Charlie 36 Chicago 1296 41 46.0 row4 Dave 29 NY 841 34 NaN <pre>'After modifying a copy of the slice of the DataFrame:'</pre> Name Age City Age_squared Age in 5 years Age plus 10 row2 Bob 31 LA 961 36 41 row3 Charlie 36 Chicago 1296 41 46 In\u00a0[21]: Copied! <pre>display(\"Average Age:\", df[\"Age\"].mean())  # Average age\ndisplay(\"Max Age:\", df[\"Age\"].max())         # Maximum age\ndisplay(\"Min Age:\", df[\"Age\"].min())         # Minimum age\n</pre> display(\"Average Age:\", df[\"Age\"].mean())  # Average age display(\"Max Age:\", df[\"Age\"].max())         # Maximum age display(\"Min Age:\", df[\"Age\"].min())         # Minimum age  <pre>'Average Age:'</pre> <pre>31.0</pre> <pre>'Max Age:'</pre> <pre>36</pre> <pre>'Min Age:'</pre> <pre>28</pre> In\u00a0[22]: Copied! <pre>city_counts = df[\"City\"].value_counts()\ndisplay(\"City counts:\")\ndisplay(city_counts)\n</pre> city_counts = df[\"City\"].value_counts() display(\"City counts:\") display(city_counts)  <pre>'City counts:'</pre> <pre>City\nNY         2\nLA         1\nChicago    1\nName: count, dtype: int64</pre> In\u00a0[23]: Copied! <pre>data = {\n    \"Name\": [\"Alice\", \"Bob\", \"Charlie\", \"Dave\"],\n    \"Age\": [25, 30, 35, 28],\n    \"City\": [\"NY\", \"LA\", \"NY\", \"LA\"],\n    \"Salary\": [70000, 80000, 120000, 95000]\n}\ndf = pd.DataFrame(data)\n\n# Group by 'City' and calculate mean Salary\ngrouped = df.groupby(\"City\")[\"Age\"].std()\ndisplay(\"Mean Age by City:\")\ndisplay(grouped)\n</pre> data = {     \"Name\": [\"Alice\", \"Bob\", \"Charlie\", \"Dave\"],     \"Age\": [25, 30, 35, 28],     \"City\": [\"NY\", \"LA\", \"NY\", \"LA\"],     \"Salary\": [70000, 80000, 120000, 95000] } df = pd.DataFrame(data)  # Group by 'City' and calculate mean Salary grouped = df.groupby(\"City\")[\"Age\"].std() display(\"Mean Age by City:\") display(grouped)  <pre>'Mean Age by City:'</pre> <pre>City\nLA    1.414214\nNY    7.071068\nName: Age, dtype: float64</pre> In\u00a0[24]: Copied! <pre>df_left = pd.DataFrame({\n    \"PersonID\": [1, 2, 3],\n    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"]\n})\n\ndf_right = pd.DataFrame({\n    \"PersonID\": [1, 2, 4],\n    \"City\": [\"NY\", \"LA\", \"Houston\"]\n})\n\nmerged_df = pd.merge(df_left, df_right, on=\"PersonID\", how=\"outer\")\ndisplay(\"Merged DataFrame (inner join):\")\ndisplay(merged_df)\n</pre> df_left = pd.DataFrame({     \"PersonID\": [1, 2, 3],     \"Name\": [\"Alice\", \"Bob\", \"Charlie\"] })  df_right = pd.DataFrame({     \"PersonID\": [1, 2, 4],     \"City\": [\"NY\", \"LA\", \"Houston\"] })  merged_df = pd.merge(df_left, df_right, on=\"PersonID\", how=\"outer\") display(\"Merged DataFrame (inner join):\") display(merged_df)  <pre>'Merged DataFrame (inner join):'</pre> PersonID Name City 0 1 Alice NY 1 2 Bob LA 2 3 Charlie NaN 3 4 NaN Houston In\u00a0[25]: Copied! <pre># Example: Uncomment and modify the following line if your DataFrames have different key names.\n# pd.merge(df_left, df_right, left_on=\"PersonID\", right_on=\"ID\")\n</pre> # Example: Uncomment and modify the following line if your DataFrames have different key names. # pd.merge(df_left, df_right, left_on=\"PersonID\", right_on=\"ID\")   In\u00a0[26]: Copied! <pre># Save DataFrame to CSV\ndf.to_csv(\"saved_data.csv\", index=False)\ndisplay(\"DataFrame saved to CSV file: saved_data.csv\")\n</pre> # Save DataFrame to CSV df.to_csv(\"saved_data.csv\", index=False) display(\"DataFrame saved to CSV file: saved_data.csv\")  <pre>'DataFrame saved to CSV file: saved_data.csv'</pre> In\u00a0[27]: Copied! <pre># Save DataFrame to Feather format (ensure you have pyarrow installed: pip install pyarrow)\ndf.to_feather(\"saved_data.feather\")\ndisplay(\"DataFrame saved to Feather file: saved_data.feather\")\n</pre> # Save DataFrame to Feather format (ensure you have pyarrow installed: pip install pyarrow) df.to_feather(\"saved_data.feather\") display(\"DataFrame saved to Feather file: saved_data.feather\")  <pre>'DataFrame saved to Feather file: saved_data.feather'</pre> In\u00a0[28]: Copied! <pre># Loading the saved CSV file\ndf_loaded_csv = pd.read_csv(\"saved_data.csv\")\ndisplay(\"CSV file loaded:\")\ndisplay(df_loaded_csv)\n</pre> # Loading the saved CSV file df_loaded_csv = pd.read_csv(\"saved_data.csv\") display(\"CSV file loaded:\") display(df_loaded_csv)  <pre>'CSV file loaded:'</pre> Name Age City Salary 0 Alice 25 NY 70000 1 Bob 30 LA 80000 2 Charlie 35 NY 120000 3 Dave 28 LA 95000 In\u00a0[29]: Copied! <pre># Loading the saved Feather file\ndf_loaded_feather = pd.read_feather(\"saved_data.feather\")\ndisplay(\"Feather file loaded:\")\ndisplay(df_loaded_feather)\n</pre> # Loading the saved Feather file df_loaded_feather = pd.read_feather(\"saved_data.feather\") display(\"Feather file loaded:\") display(df_loaded_feather)  <pre>'Feather file loaded:'</pre> Name Age City Salary 0 Alice 25 NY 70000 1 Bob 30 LA 80000 2 Charlie 35 NY 120000 3 Dave 28 LA 95000 In\u00a0[30]: Copied! <pre># 1. Create a DataFrame from a dictionary of lists.\n</pre> # 1. Create a DataFrame from a dictionary of lists.  In\u00a0[31]: Copied! <pre># 2. Load a CSV file and inspect its first few rows.\n</pre> # 2. Load a CSV file and inspect its first few rows.  In\u00a0[32]: Copied! <pre># 3. Filter rows where a numeric column exceeds a threshold.\n</pre> # 3. Filter rows where a numeric column exceeds a threshold.  In\u00a0[33]: Copied! <pre># 4. Perform a group-by operation and calculate the mean of another column.\n</pre> # 4. Perform a group-by operation and calculate the mean of another column.  In\u00a0[34]: Copied! <pre># 5. Merge two DataFrames on a common key.\n</pre> # 5. Merge two DataFrames on a common key."},{"location":"m03-numpy_pandas/pandas_basics/#pandas-tutorial","title":"Pandas Tutorial\u00b6","text":"<p>A comprehensive yet beginner-friendly tutorial on pandas, a popular Python library for data manipulation and analysis.</p> <p>In this tutorial, we will cover:</p> <pre><code>- Installation and import of the pandas library.\n- An introduction to Pandas Series, highlighting its similarity to NumPy arrays.\n- Creating DataFrames from various data sources.\n- Basic data inspection, selection, indexing, and filtering.\n- Modifying DataFrames and performing calculations.\n- Grouping, merging, and finally saving/loading data in different formats.</code></pre> <p>Note: Remember that a pandas DataFrame can be thought of as a collection of Series objects, where each column is a Series.</p>"},{"location":"m03-numpy_pandas/pandas_basics/#1-installation-and-import","title":"1. Installation and Import\u00b6","text":"<p>First, install pandas if it is not already installed, then import it into your Python environment.</p>"},{"location":"m03-numpy_pandas/pandas_basics/#11-pandas-series-an-introduction","title":"1.1. Pandas Series: An Introduction\u00b6","text":"<p>A Pandas Series is a one-dimensional labeled array capable of holding any data type. If you're already familiar with NumPy arrays, you'll notice that a Series behaves similarly but with added flexibility through indexing (labels for each element).</p> <p>In fact, a DataFrame is essentially a collection of Series objects (each column is a Series), which means many operations applicable to arrays can also be performed on Series.</p>"},{"location":"m03-numpy_pandas/pandas_basics/#2-creating-dataframes","title":"2. Creating DataFrames\u00b6","text":"<p>A DataFrame is the core data structure in pandas \u2014 think of it as a table with rows and columns. You can create a DataFrame from various sources. Below are a few common methods:</p>"},{"location":"m03-numpy_pandas/pandas_basics/#21-from-a-dictionary-of-lists","title":"2.1. From a Dictionary of Lists\u00b6","text":"<p>Here, each key in the dictionary represents a column name, and the corresponding value is a list of data for that column.</p>"},{"location":"m03-numpy_pandas/pandas_basics/#22-from-a-list-of-dictionaries","title":"2.2. From a List of Dictionaries\u00b6","text":"<p>In this approach, each dictionary in the list represents a row of data.</p>"},{"location":"m03-numpy_pandas/pandas_basics/#3-basic-data-inspection","title":"3. Basic Data Inspection\u00b6","text":"<p>After creating or loading a DataFrame, it's important to inspect your data. Common methods include:</p> <ul> <li><p><code>df.head()</code>: View the first few rows.</p> </li> <li><p><code>df.tail()</code>: View the last few rows.</p> </li> <li><p><code>df.shape</code>: Get the number of rows and columns.</p> </li> <li><p><code>df.columns</code>: List all column names.</p> </li> <li><p><code>df.info()</code>: Get a summary including data types and non-null counts.</p> </li> <li><p><code>df.describe()</code>: Compute basic statistics for numerical columns.</p> </li> </ul>"},{"location":"m03-numpy_pandas/pandas_basics/#knowledge-check-dataframe-inspection","title":"Knowledge Check: DataFrame Inspection\u00b6","text":"<p>Consider the DataFrame you just inspected. Write code to:</p> <ol> <li><p>Print the first 3 rows using an alternative method.</p> </li> <li><p>Retrieve the list of column names.</p> </li> <li><p>Summarize the DataFrame using <code>.info()</code>.</p> </li> </ol> <p>Hint: Use the appropriate DataFrame methods to achieve these tasks.</p>"},{"location":"m03-numpy_pandas/pandas_basics/#4-selecting-and-indexing-data","title":"4. Selecting and Indexing Data\u00b6","text":"<p>Pandas offers multiple ways to select or filter data within a DataFrame.</p>"},{"location":"m03-numpy_pandas/pandas_basics/#41-dot-notation-bracket-notation","title":"4.1. Dot Notation / Bracket Notation\u00b6","text":"<ul> <li><p>Dot Notation: Simplifies access for columns with simple names.</p> </li> <li><p>Bracket Notation: More flexible; it supports column names with spaces or special characters.</p> </li> </ul>"},{"location":"m03-numpy_pandas/pandas_basics/#42-row-selection-with-loc-and-iloc","title":"4.2. Row Selection with <code>.loc</code> and <code>.iloc</code>\u00b6","text":"<ul> <li><p><code>.loc</code> selects rows and columns by label.</p> </li> <li><p><code>.iloc</code> selects rows and columns by integer position.</p> </li> </ul>"},{"location":"m03-numpy_pandas/pandas_basics/#5-filtering-rows","title":"5. Filtering Rows\u00b6","text":"<p>Filtering rows lets you extract data based on specific conditions.</p>"},{"location":"m03-numpy_pandas/pandas_basics/#51-boolean-masking","title":"5.1. Boolean Masking\u00b6","text":"<p>Create a boolean condition that returns <code>True/False</code> for each row, then use that mask to filter the DataFrame.</p>"},{"location":"m03-numpy_pandas/pandas_basics/#52-multiple-conditions","title":"5.2. Multiple Conditions\u00b6","text":"<p>Combine conditions using bitwise operators:</p> <ul> <li><p><code>&amp;</code> for AND</p> </li> <li><p><code>|</code> for OR</p> </li> <li><p><code>~</code> for NOT</p> </li> </ul>"},{"location":"m03-numpy_pandas/pandas_basics/#knowledge-check-filtering-rows","title":"Knowledge Check: Filtering Rows\u00b6","text":"<p>Using the DataFrame <code>df</code>:</p> <ol> <li><p>Create a boolean mask to filter rows where the 'Age' is between 26 and 32 (inclusive).</p> </li> <li><p>Additionally, filter rows where the 'City' starts with either 'C' or 'N'.</p> </li> <li><p>Print the resulting DataFrame.</p> </li> </ol> <p>Hint: Use string methods like <code>.str.startswith()</code> on the 'City' column along with logical operators.</p>"},{"location":"m03-numpy_pandas/pandas_basics/#6-changing-values","title":"6. Changing Values\u00b6","text":"<p>You can modify DataFrame values using various methods:</p>"},{"location":"m03-numpy_pandas/pandas_basics/#61-assigning-with-loc","title":"6.1. Assigning with <code>.loc</code>\u00b6","text":"<p>Modify values by referencing labels.</p>"},{"location":"m03-numpy_pandas/pandas_basics/#62-assigning-with-iloc","title":"6.2. Assigning with <code>.iloc</code>\u00b6","text":"<p>Modify values by referencing integer positions.</p>"},{"location":"m03-numpy_pandas/pandas_basics/#63-vectorized-assignments","title":"6.3. Vectorized Assignments\u00b6","text":"<p>Apply operations across entire columns efficiently.</p>"},{"location":"m03-numpy_pandas/pandas_basics/#64-using-apply","title":"6.4. Using <code>apply()</code>\u00b6","text":"<p>Apply a function to each element in a Series or DataFrame and return a new Series or DataFrame.</p>"},{"location":"m03-numpy_pandas/pandas_basics/#7-calculating-simple-statistics-and-value-counts","title":"7. Calculating Simple Statistics and Value Counts\u00b6","text":"<p>Pandas provides simple methods to compute statistics and count occurrences:</p>"},{"location":"m03-numpy_pandas/pandas_basics/#71-simple-statistics","title":"7.1. Simple Statistics\u00b6","text":"<p>Calculate basic statistics such as mean, maximum, and minimum.</p>"},{"location":"m03-numpy_pandas/pandas_basics/#72-value_counts","title":"7.2. <code>value_counts()</code>\u00b6","text":"<p>Count the occurrence of unique values in a Series.</p>"},{"location":"m03-numpy_pandas/pandas_basics/#8-grouping-and-aggregation","title":"8. Grouping and Aggregation\u00b6","text":"<p>Use <code>.groupby()</code> to split data into groups based on certain criteria, apply functions to each group, and combine the results.</p> <p>In the example below, we group the DataFrame by 'City' and calculate the mean Salary for each group.</p>"},{"location":"m03-numpy_pandas/pandas_basics/#9-merging-joining-dataframes","title":"9. Merging / Joining DataFrames\u00b6","text":"<p>Merge or join multiple DataFrames using pandas methods.</p>"},{"location":"m03-numpy_pandas/pandas_basics/#91-the-merge-method","title":"9.1. The <code>merge()</code> Method\u00b6","text":"<p>Merge two DataFrames on a common key.</p>"},{"location":"m03-numpy_pandas/pandas_basics/#92-joins-on-different-column-names","title":"9.2. Joins on Different Column Names\u00b6","text":"<p>If the key column has different names in each DataFrame, use the <code>left_on</code> and <code>right_on</code> parameters.</p>"},{"location":"m03-numpy_pandas/pandas_basics/#10-saving-and-loading-data","title":"10. Saving and Loading Data\u00b6","text":"<p>Pandas allows you to easily save DataFrames to various file formats and load them back into your program. Below are examples for saving to CSV and Feather formats:</p> <ul> <li><p>CSV Format: A widely used text-based format.</p> </li> <li><p>Feather Format: A fast, lightweight, language-independent binary format (requires <code>pyarrow</code>).</p> </li> </ul> <p>Pandas supports many other formats as well, including Excel, JSON, and SQL!</p>"},{"location":"m03-numpy_pandas/pandas_basics/#11-exercises","title":"11. Exercises\u00b6","text":"<p>Practice what you have learned with the following exercises:</p> <ol> <li><p>Create a DataFrame from a dictionary of lists with at least three columns.</p> </li> <li><p>Load a CSV file into a DataFrame and inspect its first few rows.</p> </li> <li><p>Filter rows where a numeric column exceeds a certain threshold.</p> </li> <li><p>Perform a group-by operation and calculate the mean of another column.</p> </li> <li><p>Merge two DataFrames on a common key.</p> </li> </ol> <p>For each exercise, write your code in the provided cells.</p>"},{"location":"m04-tidy_databases/assignment_m04_billboard/","title":"Homework 4 - Tidy and Process the Billboard Dataset","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n\n# 1. Load the Billboard dataset\ndf_bill = pd.read_csv(\"../../Datasets/billboard.csv\")\n\n# Let's check a few columns to see the structure.\ndf_bill.head()\n</pre> import pandas as pd  # 1. Load the Billboard dataset df_bill = pd.read_csv(\"../../Datasets/billboard.csv\")  # Let's check a few columns to see the structure. df_bill.head() <p>The dataset has columns like:</p> <ul> <li><p>year, artist.inverted, track, time, genre \u2026 (song info)</p> </li> <li><p>date.entered, date.peaked \u2026 (chart-related dates)</p> </li> <li><p>x1st.week through x76th.week \u2026 (chart positions over 76 weeks)</p> </li> </ul> <p>We want to melt these weekly columns into a single <code>week</code> and <code>rank</code> column.</p> In\u00a0[\u00a0]: Copied! <pre># Your code here\n</pre> # Your code here <p>Notice how each row is now one song in one week. However, the <code>week</code> column currently contains strings like <code>\"x1st.week\"</code>, <code>\"x2nd.week\"</code>, etc. Let's clean those up and create a numeric week column.</p> In\u00a0[\u00a0]: Copied! <pre># Your code here\n</pre> # Your code here <p>Now, <code>week = 1, 2, 3, ... 76</code>. Next, we want to calculate the exact date on the chart for each row by adding <code>week * 7</code> days to <code>date.entered</code>. Create a column named \"date\" to hold the result. See the expected result in our lecture materials for tidy data.</p> In\u00a0[\u00a0]: Copied! <pre># Note that after doing that, you should have a new column called date\n# Your code here\n</pre> # Note that after doing that, you should have a new column called date # Your code here  In\u00a0[\u00a0]: Copied! <pre># Your code here\n</pre> # Your code here <p>Next, we merge this <code>song_id</code> back into our <code>df_tidy</code> so we can create the positions table.</p> In\u00a0[\u00a0]: Copied! <pre># Your code here\n</pre> # Your code here  In\u00a0[\u00a0]: Copied! <pre># Your code here\n</pre> # Your code here  In\u00a0[\u00a0]: Copied! <pre># Your code here\n</pre> # Your code here  <p>You may want to remove duplicates to get a list of unique songs that reached the top 10. See <code>df.drop_duplicates()</code> for more details.</p> In\u00a0[\u00a0]: Copied! <pre># Your code here\n</pre> # Your code here  In\u00a0[\u00a0]: Copied! <pre># Your code here\n</pre> # Your code here  In\u00a0[\u00a0]: Copied! <pre># Your code here\n</pre> # Your code here  In\u00a0[\u00a0]: Copied! <pre># Your code here\n</pre> # Your code here"},{"location":"m04-tidy_databases/assignment_m04_billboard/#homework-4-tidy-and-process-the-billboard-dataset","title":"Homework 4 - Tidy and Process the Billboard Dataset\u00b6","text":"<p>The Billboard dataset comes with 76 columns corresponding to the chart position of each song from <code>x1st.week</code> through <code>x76th.week</code>. This is a classic example of wide data that needs to be melted (unpivoted) into a long (tidy) format.</p>"},{"location":"m04-tidy_databases/assignment_m04_billboard/#instructions","title":"Instructions\u00b6","text":"<ol> <li>Follow the instructions on how to setup your Python and Jupyter (or VSCode) environment and cloning or downloading our repository. Instructions can be found in the class notes.</li> <li>Fill the missing pieces of code in the provided notebook.</li> <li>Run the notebook and make sure everything works.</li> </ol>"},{"location":"m04-tidy_databases/assignment_m04_billboard/#dataset-overview","title":"Dataset Overview\u00b6","text":"<p>The dataset consists of songs and their weekly chart positions on the Billboard Hot 100. The dataset contains the following columns:</p> <ul> <li><code>year</code>: The year the song entered the chart.</li> <li><code>artist</code>: The artist of the song.</li> <li><code>track</code>: The title of the song.</li> <li><code>time</code>: The duration of the song.</li> <li><code>date.entered</code>: The date the song entered the chart.</li> <li><code>x1st.week</code> to <code>x76th.week</code>: The chart position of the song for each week.</li> </ul>"},{"location":"m04-tidy_databases/assignment_m04_billboard/#goals","title":"Goals\u00b6","text":"<ol> <li>Load the Billboard dataset from CSV.</li> <li>Tidy the data so each row represents one song in one week.</li> <li>Calculate the actual date for each week using <code>date.entered + week * 7 days</code>.</li> <li>Split the data into two tables:<ul> <li>A songs table with static song information.</li> <li>A positions table with <code>(song_id, week, rank, date)</code>.</li> </ul> </li> <li>Save the tidy data to Feather format in the same directory with <code>_tidy</code> suffix.</li> </ol>"},{"location":"m04-tidy_databases/assignment_m04_billboard/#submission-guidelines","title":"Submission Guidelines\u00b6","text":"<ul> <li>Submit your completed notebook as a HTML export, or a PDF file.</li> </ul> <p>To export to HTML, if you are on Jupyter, select <code>File</code> &gt; <code>Export Notebook As</code> &gt; <code>HTML</code>.</p> <p>If you are on VSCode, you can use the <code>Jupyter: Export to HTML</code> command.</p> <ul> <li>Open the command palette (Ctrl+Shift+P or Cmd+Shift+P on Mac).<ul> <li>Search for <code>Jupyter: Export to HTML</code>.</li> <li>Save the HTML file to your computer and submit it via Canvas.</li> </ul> </li> </ul>"},{"location":"m04-tidy_databases/assignment_m04_billboard/#split-into-two-tables","title":"Split into Two Tables\u00b6","text":"<p>Why split? We often separate the static song info (e.g., artist, track, time, genre) from the weekly chart performance (week, rank, date).</p> <ul> <li><p>Songs Table: Contains unique identifiers for each song plus basic metadata.</p> </li> <li><p>Positions Table: Contains <code>(song_id, week, rank, date)</code>, referencing the song_id from the songs table.</p> </li> </ul>"},{"location":"m04-tidy_databases/assignment_m04_billboard/#create-the-positions-table","title":"Create the Positions Table\u00b6","text":"<p>We only keep the relevant columns for weekly positions: <code>song_id</code>, <code>week</code>, <code>rank</code>, and <code>date</code>.</p>"},{"location":"m04-tidy_databases/assignment_m04_billboard/#8playing-with-the-data","title":"8.Playing with the data\u00b6","text":"<p>Now that we have our data in a tidy format, let's do some analysis.</p>"},{"location":"m04-tidy_databases/assignment_m04_billboard/#only-songs-that-reached-top-10","title":"Only songs that reached top 10\u00b6","text":"<p>We can use <code>query()</code> to filter the data for songs that reached the top 10 at least once. We will merge this back to the songs table to get the song details.</p> <p>Get a dataframe with the top 10 songs and their details.</p>"},{"location":"m04-tidy_databases/assignment_m04_billboard/#how-long-did-each-song-stay-in-the-top-10","title":"How long did each song stay in the top 10?\u00b6","text":"<p>Create add to the current dataframe or create a new dataframe with the following columns:</p> <ul> <li><code>song_id</code> : the song id</li> <li><code>weeks_in_top_10</code> : the number of weeks the song was in the top 10</li> </ul>"},{"location":"m04-tidy_databases/assignment_m04_billboard/#in-which-week-did-each-song-reach-the-top-10","title":"In which week did each song reach the top 10?\u00b6","text":"<p>Create or add to a new dataframe with the following columns:</p> <ul> <li><code>week_reached_top_10</code> : the week in which the song reached the top 10 for the first time</li> </ul>"},{"location":"m04-tidy_databases/assignment_m04_billboard/#9-save-tidy-data-to-feather","title":"9. Save Tidy Data to Feather\u00b6","text":"<p>We want to save:</p> <ul> <li><p>The tidy DataFrame (<code>df_tidy</code>) to a single file with the suffix <code>_tidy</code>.</p> </li> <li><p>(Optionally) Also save songs and positions as separate Feather files if needed.</p> </li> </ul>"},{"location":"m04-tidy_databases/assignment_m04_databases/","title":"Homework 3 - Databases and SQL","text":"In\u00a0[28]: Copied! <pre>import #Input needed Library here\nimport os\n</pre> import #Input needed Library here import os <p>Now we have our functions ready to go, let's get the current path and connect to our database. For <code>dbPath</code> variable, create a db with name <code>mydb.sqlite</code>.</p> In\u00a0[\u00a0]: Copied! <pre>data_path = 'Datasets' # Path to the datasets folder\ndbpath = # Select a path to save the database file\nconn = sqlite3.connect(dbpath) \n</pre> data_path = 'Datasets' # Path to the datasets folder dbpath = # Select a path to save the database file conn = sqlite3.connect(dbpath)  <p>Now connected, we can create our cursor variable using the <code>cursor()</code> function.</p> In\u00a0[30]: Copied! <pre>cur = conn._()\n</pre> cur = conn._() <p>Using <code>cur</code>, we want to add in our new tables. However, if we add them multiple times, we will receive an error. So, we first need to <code>DROP</code> the tables if they exist already.Enter your <code>DROP</code> queries below as an argument to the <code>execute</code> methods. Do this twice, once for the <code>stories</code> table and one for the <code>word_counts</code> table.</p> In\u00a0[\u00a0]: Copied! <pre>drop_stories_query  = # Input DROP query for stories table here\ndrop_info_query = # Input DROP query for information table here\n\n#Now we run both queries\ncur.execute(drop_stories_query)\ncur.execute(drop_info_query)\n</pre> drop_stories_query  = # Input DROP query for stories table here drop_info_query = # Input DROP query for information table here  #Now we run both queries cur.execute(drop_stories_query) cur.execute(drop_info_query) <p>We now have a clean slate set up to create our new tables! Now, design the queries to create new tables for <code>stories</code> and <code>word_counts</code>.  The <code>stories</code> table only needs a primary key <code>story_id</code> that is an integer and a text field called <code>story</code> to store the corrosoponding story text.  The <code>word_counts</code> table will need a little more. Please include the following:</p> <ul> <li><code>word_id</code>: primary key, integer</li> <li><code>word</code>: text</li> <li><code>count</code>: integer</li> <li><code>story_id</code>: integer, foreign key to <code>stories</code> table</li> </ul> In\u00a0[40]: Copied! <pre>create_stories_query = # Insert CREATE TABLE query for stories table here\ncreate_info_query =  # Insert CREATE TABLE query for information table here\ncur.execute(create_stories_query)\ncur.execute(create_info_query)\nconn.commit()\n</pre> create_stories_query = # Insert CREATE TABLE query for stories table here create_info_query =  # Insert CREATE TABLE query for information table here cur.execute(create_stories_query) cur.execute(create_info_query) conn.commit() <p>With our new empty tables ready, we can now loop through the stories and store the word counts. Similar to how we looped through the stories in HW 1, we now have an additional step of inserting this data. In the cell below, please add the two queries for inserting these data rows.  The first insert is for storing the story text in the <code>stories</code> table while reading from the file. The second insert is after counting up all the words, and is for inserting those values into the <code>word_counts</code> table. Remember to pass these query string variables into the <code>execute()</code> methods.</p> In\u00a0[\u00a0]: Copied! <pre>stories = [\"/story-1.txt\", \"/story-2.txt\",\"/story-3.txt\",\"/story-4.txt\"]\n\nfor story in stories:\n    words = []\n    count_of_each_word = {}\n    story_id = \"\"\n    try:\n        # Open the file\n        story_path = os.path.join(data_path, story)\n        with open(story_path,\"r\", encoding='utf-8') as fp:\n            # reading data from file and splitting into words\n            # and storing them in a list\n            story_text = fp.read()\n            \n            # For the below query, you will need to use a '?' to\n            #     represent where you want the story_text to be inputed.\n            #     The actual text in story_text is passed in a tuple as \n            #     second parameter of the execute command().\n            \n            insert_story_query = # Input INSERT query for the stories table here\n            cur.execute(insert_story_query, (story_text,))\n            \n            #Grabbing the last id inserted, so we can use it when inserting values into the word_counts table\n            story_id = cur.lastrowid\n            conn.commit()\n            words = story_text.split()\n            \n            # Close the file\n            fp.close()\n            \n    except Exception as e:\n        print(\"Unable to open the file: \" + str(e))\n\n    # Just like before, we are iterating over each word and using a dictonary to store the word counts\n    for word in words:\n        if(word in count_of_each_word):\n            count_of_each_word[word] += 1\n        else:\n            count_of_each_word[word] = 1\n\n    for key in count_of_each_word:\n        insert_count_query = # Input query here, using '?' again in the VALUES () portion\n        \n        cur.execute(insert_count_query, (key, count_of_each_word[key], story_id))\n        conn.commit()\n</pre> stories = [\"/story-1.txt\", \"/story-2.txt\",\"/story-3.txt\",\"/story-4.txt\"]  for story in stories:     words = []     count_of_each_word = {}     story_id = \"\"     try:         # Open the file         story_path = os.path.join(data_path, story)         with open(story_path,\"r\", encoding='utf-8') as fp:             # reading data from file and splitting into words             # and storing them in a list             story_text = fp.read()                          # For the below query, you will need to use a '?' to             #     represent where you want the story_text to be inputed.             #     The actual text in story_text is passed in a tuple as              #     second parameter of the execute command().                          insert_story_query = # Input INSERT query for the stories table here             cur.execute(insert_story_query, (story_text,))                          #Grabbing the last id inserted, so we can use it when inserting values into the word_counts table             story_id = cur.lastrowid             conn.commit()             words = story_text.split()                          # Close the file             fp.close()                  except Exception as e:         print(\"Unable to open the file: \" + str(e))      # Just like before, we are iterating over each word and using a dictonary to store the word counts     for word in words:         if(word in count_of_each_word):             count_of_each_word[word] += 1         else:             count_of_each_word[word] = 1      for key in count_of_each_word:         insert_count_query = # Input query here, using '?' again in the VALUES () portion                  cur.execute(insert_count_query, (key, count_of_each_word[key], story_id))         conn.commit()  <p>Finally! Our tables are filled and we can now run SELECT queries against it to pull the data we want. There are two queries you will need to run.</p> In\u00a0[\u00a0]: Copied! <pre>query_one = #Input SELECT query one here\ncur.execute(query_one)\nrecords = cur.fetchall()\nfor record in records:\n    print(record)\n    \n</pre> query_one = #Input SELECT query one here cur.execute(query_one) records = cur.fetchall() for record in records:     print(record)       In\u00a0[\u00a0]: Copied! <pre>query_two = #Input SELECT query two here\ncur.execute(query_two)\nrecords = cur.fetchall()\nfor record in records:\n    print(record)\n</pre> query_two = #Input SELECT query two here cur.execute(query_two) records = cur.fetchall() for record in records:     print(record) In\u00a0[\u00a0]: Copied! <pre>dataset_path = \"../../Datasets/fifa_soccer_dataset.sqlite\" # Fix your path accordingly\n\nimport sqlite3\nconn = sqlite3.connect(dataset_path)\n</pre> dataset_path = \"../../Datasets/fifa_soccer_dataset.sqlite\" # Fix your path accordingly  import sqlite3 conn = sqlite3.connect(dataset_path)  <p>If you are using pandas, import it and read in the database. For instance:</p> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n# get all tables\ndf_tables = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table';\", conn)\ndisplay(df_tables)\n# get all players from Players table\ndf_players = pd.read_sql_query(\"SELECT * FROM Player\", conn)\ndisplay(df_players)\n</pre> import pandas as pd # get all tables df_tables = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table';\", conn) display(df_tables) # get all players from Players table df_players = pd.read_sql_query(\"SELECT * FROM Player\", conn) display(df_players)  <p>If you prefer to use just sqlite3, you can do that as well. Just make sure to import the library and connect to the database:</p> In\u00a0[\u00a0]: Copied! <pre>conn = sqlite3.connect(dataset_path)\ncur = conn.cursor()\n# get all tables\ncur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\ntables = cur.fetchall()\nfor table in tables:\n    print(table)\n# get all players from Players table\ncur.execute(\"SELECT * FROM Player\")\nplayers = cur.fetchall()\nfor player in players:\n    print(player)\n</pre> conn = sqlite3.connect(dataset_path) cur = conn.cursor() # get all tables cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\") tables = cur.fetchall() for table in tables:     print(table) # get all players from Players table cur.execute(\"SELECT * FROM Player\") players = cur.fetchall() for player in players:     print(player) <p>QUESTION 1</p> <p>Print the birthday of the player whose name is \u201cAaron Kuhl\u201d. Hint: Use \u2018Player\u2019 table</p> In\u00a0[\u00a0]: Copied! <pre># YOUR CODE HERE\n</pre> # YOUR CODE HERE <p>QUESTION 2</p> <p>Print the number of times the team_fifa_api_id \u2018673\u2019 appeared in Team_attribute table. Hint: Apply GROUP BY clause on team_fifa_api_id attribute</p> In\u00a0[\u00a0]: Copied! <pre># YOUR CODE HERE\n</pre> # YOUR CODE HERE <p>QUESTION 3</p> <p>Print country name and league name that have matches on \u201c2014-04-20 00:00:00\u201d. Hint: Apply join on Match Table and Country table, then Match Table and League Table</p> In\u00a0[\u00a0]: Copied! <pre># YOUR CODE HERE\n</pre> # YOUR CODE HERE"},{"location":"m04-tidy_databases/assignment_m04_databases/#homework-3-databases-and-sql","title":"Homework 3 - Databases and SQL\u00b6","text":"<p>In this guide, we will be connecting to the sqlite database created from the lecture, fill it with values, then run a few queries. You will be using the data files from the previous homework.</p>"},{"location":"m04-tidy_databases/assignment_m04_databases/#instructions","title":"Instructions\u00b6","text":"<ol> <li>Follow the instructions on how to setup your Python and Jupyter (or VSCode) environment and cloning or downloading our repository. Instructions can be found in the class notes.</li> <li>Fill the missing pieces of code in the provided notebook.</li> <li>Answer the questions in the notebook through code.</li> <li>Run the notebook and make sure everything works.</li> </ol>"},{"location":"m04-tidy_databases/assignment_m04_databases/#dataset-overview","title":"Dataset Overview\u00b6","text":"<p>We will use two datasets for this assignment. The first one is the same as used in HW1, which consists of four text files, each containing a story. Files are in the <code>Datasets</code> directory of this repository. The stories are:</p> <ul> <li><code>story-1.txt</code>: The Monkey and the Crocodile</li> <li><code>story-2.txt</code>: The Musical Donkey</li> <li><code>story-3.txt</code>: A Tale of Three Fish</li> <li><code>story-4.txt</code>: The Foolish Lion and the Clever Rabbit</li> </ul> <p>The second dataset covers information about soccer players in sqlite format. This file is located in the <code>Datasets</code> directory of this repository. The file is called <code>fifa_soccer_dataset.sqlite.gz</code>.</p> <p>IMPORTANT The database is compressed and needs to be decompressed before use. You can do this by running the following command in your terminal on Linux or MacOS:</p> <pre>gunzip Datasets/fifa_soccer_dataset.sqlite.gz\n</pre> <p>If you are using Windows, you can use the following command in your powershell:</p> <pre>$sourceFile = \"$PWD\\Datasets\\fifa_soccer_dataset.sqlite.gz\"\n$destinationFile = \"$PWD\\Datasets\\fifa_soccer_dataset.sqlite\"\n\n$inputStream = [System.IO.File]::OpenRead($sourceFile)\n$outputStream = [System.IO.File]::Create($destinationFile)\n$gzipStream = New-Object System.IO.Compression.GzipStream($inputStream, [System.IO.Compression.CompressionMode]::Decompress)\n$gzipStream.CopyTo($outputStream)\n\n$gzipStream.Close()\n$outputStream.Close()\n$inputStream.Close()\n</pre> <p>Alternatively, you can extract the file using the GUI of your operating system.</p>"},{"location":"m04-tidy_databases/assignment_m04_databases/#submission-guidelines","title":"Submission Guidelines\u00b6","text":"<ul> <li>Submit your completed notebook as a HTML export, or a PDF file.</li> </ul> <p>To export to HTML, if you are on Jupyter, select <code>File</code> &gt; <code>Export Notebook As</code> &gt; <code>HTML</code>.</p> <p>If you are on VSCode, you can use the <code>Jupyter: Export to HTML</code> command.</p> <ul> <li>Open the command palette (Ctrl+Shift+P or Cmd+Shift+P on Mac).<ul> <li>Search for <code>Jupyter: Export to HTML</code>.</li> <li>Save the HTML file to your computer and submit it via Canvas.</li> </ul> </li> </ul>"},{"location":"m04-tidy_databases/assignment_m04_databases/#part-1-story-analysis","title":"Part 1 Story Analysis\u00b6","text":"<p>First, we need to import the correct library to use sqlite functions. Can you figure out which library are we going to use?</p>"},{"location":"m04-tidy_databases/assignment_m04_databases/#select-query-one","title":"SELECT Query One:\u00b6","text":"<p>Grab all rows from <code>word_counts</code> where the word is \"the\" and the count is greater than 1.</p>"},{"location":"m04-tidy_databases/assignment_m04_databases/#select-query-two","title":"SELECT Query Two:\u00b6","text":"<p>Grab the <code>story_id</code>, <code>story</code>, and <code>count</code> columns where the word is \"the\". You should use a JOIN statement for this query, and only need to include <code>story_id</code> from one table.</p>"},{"location":"m04-tidy_databases/assignment_m04_databases/#part-2-soccer-database","title":"Part 2 - Soccer Database\u00b6","text":"<p>Now that we have our first database filled with data, we can move on to the second one. We will be using the <code>fifa_soccer_dataset.sqlite</code> file. Feel free to use either sqlite3 or pandas to run your queries! If you plan to use pandas, check the <code>pandas</code> documentation for how to read in a sqlite database. In particular you can load a sqlite database into a pandas dataframe using the <code>read_sql_query</code> function.</p>"},{"location":"m04-tidy_databases/more_tidy_data/","title":"Module 04: More on Tidy Data","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd In\u00a0[2]: Copied! <pre>wide_df = pd.DataFrame({\n    \"Year\":  [2020, 2021, 2022, 2023],\n    \"North\": [100, 150, 200, 250],\n    \"South\": [ 90, 130, 170, 220],\n    \"East\":  [ 80, 120, 160, 210],\n    \"West\":  [ 70, 110, 150, 200]\n})\n\ndisplay(wide_df)\n</pre> wide_df = pd.DataFrame({     \"Year\":  [2020, 2021, 2022, 2023],     \"North\": [100, 150, 200, 250],     \"South\": [ 90, 130, 170, 220],     \"East\":  [ 80, 120, 160, 210],     \"West\":  [ 70, 110, 150, 200] })  display(wide_df)  Year North South East West 0 2020 100 90 80 70 1 2021 150 130 120 110 2 2022 200 170 160 150 3 2023 250 220 210 200 In\u00a0[3]: Copied! <pre>tidy_df = wide_df.melt(\n    id_vars=[\"Year\"], \n    var_name=\"Region\", \n    value_name=\"Sales\"\n)\n\ndisplay(tidy_df)\n</pre> tidy_df = wide_df.melt(     id_vars=[\"Year\"],      var_name=\"Region\",      value_name=\"Sales\" )  display(tidy_df)  Year Region Sales 0 2020 North 100 1 2021 North 150 2 2022 North 200 3 2023 North 250 4 2020 South 90 5 2021 South 130 6 2022 South 170 7 2023 South 220 8 2020 East 80 9 2021 East 120 10 2022 East 160 11 2023 East 210 12 2020 West 70 13 2021 West 110 14 2022 West 150 15 2023 West 200 In\u00a0[4]: Copied! <pre>east_after_2021_wide = wide_df.loc[wide_df[\"Year\"] &gt; 2021, [\"Year\", \"East\"]]\neast_after_2021_wide = east_after_2021_wide.rename(columns={\"East\": \"Sales\"})\ndisplay(east_after_2021_wide)\n</pre> east_after_2021_wide = wide_df.loc[wide_df[\"Year\"] &gt; 2021, [\"Year\", \"East\"]] east_after_2021_wide = east_after_2021_wide.rename(columns={\"East\": \"Sales\"}) display(east_after_2021_wide)  Year Sales 2 2022 160 3 2023 210 In\u00a0[5]: Copied! <pre>east_after_2021_tidy = tidy_df.query(\"Region == 'East' and Year &gt; 2021\")\n\ndisplay(east_after_2021_tidy)\n</pre> east_after_2021_tidy = tidy_df.query(\"Region == 'East' and Year &gt; 2021\")  display(east_after_2021_tidy)  Year Region Sales 10 2022 East 160 11 2023 East 210 In\u00a0[6]: Copied! <pre>import numpy as np\n\nmean_north = wide_df[\"North\"].mean()\nmean_south = wide_df[\"South\"].mean()\nmean_east  = wide_df[\"East\"].mean()\nmean_west  = wide_df[\"West\"].mean()\n\nscaled_wide = wide_df.copy()\nscaled_wide[\"North\"] = scaled_wide[\"North\"] / mean_north\nscaled_wide[\"South\"] = scaled_wide[\"South\"] / mean_south\nscaled_wide[\"East\"]  = scaled_wide[\"East\"]  / mean_east\nscaled_wide[\"West\"]  = scaled_wide[\"West\"]  / mean_west\n\n# a for could be used to avoid repetition, but it's still cumbersome\n\ndisplay(scaled_wide)\n</pre> import numpy as np  mean_north = wide_df[\"North\"].mean() mean_south = wide_df[\"South\"].mean() mean_east  = wide_df[\"East\"].mean() mean_west  = wide_df[\"West\"].mean()  scaled_wide = wide_df.copy() scaled_wide[\"North\"] = scaled_wide[\"North\"] / mean_north scaled_wide[\"South\"] = scaled_wide[\"South\"] / mean_south scaled_wide[\"East\"]  = scaled_wide[\"East\"]  / mean_east scaled_wide[\"West\"]  = scaled_wide[\"West\"]  / mean_west  # a for could be used to avoid repetition, but it's still cumbersome  display(scaled_wide)  Year North South East West 0 2020 0.571429 0.590164 0.561404 0.528302 1 2021 0.857143 0.852459 0.842105 0.830189 2 2022 1.142857 1.114754 1.122807 1.132075 3 2023 1.428571 1.442623 1.473684 1.509434 In\u00a0[7]: Copied! <pre>scaled_tidy = tidy_df.copy()\nscaled_tidy[\"Sales_Scaled\"] = scaled_tidy.groupby(\"Region\")[\"Sales\"] \\\n                                         .transform(lambda x: x / x.mean())\n\ndisplay(scaled_tidy)\n</pre> scaled_tidy = tidy_df.copy() scaled_tidy[\"Sales_Scaled\"] = scaled_tidy.groupby(\"Region\")[\"Sales\"] \\                                          .transform(lambda x: x / x.mean())  display(scaled_tidy)  Year Region Sales Sales_Scaled 0 2020 North 100 0.571429 1 2021 North 150 0.857143 2 2022 North 200 1.142857 3 2023 North 250 1.428571 4 2020 South 90 0.590164 5 2021 South 130 0.852459 6 2022 South 170 1.114754 7 2023 South 220 1.442623 8 2020 East 80 0.561404 9 2021 East 120 0.842105 10 2022 East 160 1.122807 11 2023 East 210 1.473684 12 2020 West 70 0.528302 13 2021 West 110 0.830189 14 2022 West 150 1.132075 15 2023 West 200 1.509434 In\u00a0[8]: Copied! <pre>import altair as alt\n</pre> import altair as alt In\u00a0[9]: Copied! <pre>chart_north = alt.Chart(wide_df).mark_line(stroke='blue').encode(\n    x=\"Year:O\",\n    y=\"North:Q\"\n).properties(title=\"North\")\n\nchart_south = alt.Chart(wide_df).mark_line(stroke='red').encode(\n    x=\"Year:O\",\n    y=\"South:Q\"\n).properties(title=\"South\")\n\nchart_east = alt.Chart(wide_df).mark_line(stroke='green').encode(\n    x=\"Year:O\",\n    y=\"East:Q\"\n).properties(title=\"East\")\n\nchart_west = alt.Chart(wide_df).mark_line(stroke='orange').encode(\n    x=\"Year:O\",\n    y=\"West:Q\"\n).properties(title=\"West\")\n\n# compose the charts\nchart_wide_layered = alt.layer(chart_north, chart_south, chart_east, chart_west).properties(\n    width=400,\n    height=300\n)\n\n# Adding the legends would be a bit more work\n\nchart_wide_layered\n</pre> chart_north = alt.Chart(wide_df).mark_line(stroke='blue').encode(     x=\"Year:O\",     y=\"North:Q\" ).properties(title=\"North\")  chart_south = alt.Chart(wide_df).mark_line(stroke='red').encode(     x=\"Year:O\",     y=\"South:Q\" ).properties(title=\"South\")  chart_east = alt.Chart(wide_df).mark_line(stroke='green').encode(     x=\"Year:O\",     y=\"East:Q\" ).properties(title=\"East\")  chart_west = alt.Chart(wide_df).mark_line(stroke='orange').encode(     x=\"Year:O\",     y=\"West:Q\" ).properties(title=\"West\")  # compose the charts chart_wide_layered = alt.layer(chart_north, chart_south, chart_east, chart_west).properties(     width=400,     height=300 )  # Adding the legends would be a bit more work  chart_wide_layered  Out[9]: <ul> <li><p>We had to manually define a chart for each column (region).</p> </li> <li><p>Adding new regions or removing one requires extra lines of code.</p> </li> <li><p>Legends and other customizations would be more complex.</p> </li> </ul> In\u00a0[10]: Copied! <pre>chart_wide_pivot = (\n    alt.Chart(wide_df)\n    .transform_fold( # this is like pd.melt()!!!!\n        fold=[\"North\",\"South\",\"East\",\"West\"],  # must list every region\n        as_=[\"Region\",\"Sales\"]\n    )\n    .mark_line()\n    .encode(\n        x=\"Year:O\",\n        y=\"Sales:Q\",\n        color=\"Region:N\"\n    )\n    .properties(\n        width=400,\n        height=300\n    )\n)\n\nchart_wide_pivot\n</pre> chart_wide_pivot = (     alt.Chart(wide_df)     .transform_fold( # this is like pd.melt()!!!!         fold=[\"North\",\"South\",\"East\",\"West\"],  # must list every region         as_=[\"Region\",\"Sales\"]     )     .mark_line()     .encode(         x=\"Year:O\",         y=\"Sales:Q\",         color=\"Region:N\"     )     .properties(         width=400,         height=300     ) )  chart_wide_pivot  Out[10]: In\u00a0[11]: Copied! <pre>chart_tidy = alt.Chart(tidy_df).mark_line().encode(\n    x=\"Year:O\",\n    y=\"Sales:Q\",\n    color=\"Region:N\"\n).properties(\n    width=400,\n    height=300\n).interactive()\n\nchart_tidy\n</pre> chart_tidy = alt.Chart(tidy_df).mark_line().encode(     x=\"Year:O\",     y=\"Sales:Q\",     color=\"Region:N\" ).properties(     width=400,     height=300 ).interactive()  chart_tidy  Out[11]:"},{"location":"m04-tidy_databases/more_tidy_data/#module-04-more-on-tidy-data","title":"Module 04: More on Tidy Data\u00b6","text":"<p>With this I plan to convince you that tidy data is the way to go.</p> <p>This notebook demonstrates:</p> <ol> <li><p>How to convert wide to tidy format.</p> </li> <li><p>How filtering, grouping, and scaling is more complicated in wide format compared to tidy.</p> </li> <li><p>How visualization in Altair is also simpler with tidy data.</p> </li> </ol>"},{"location":"m04-tidy_databases/more_tidy_data/#1-imports-and-setup","title":"1. Imports and Setup\u00b6","text":""},{"location":"m04-tidy_databases/more_tidy_data/#2-creating-the-example-dataset-wide-format","title":"2. Creating the Example Dataset (Wide Format)\u00b6","text":"<p>We start with a wide DataFrame: each region (<code>North</code>, <code>South</code>, <code>East</code>, <code>West</code>) is in its own column.</p>"},{"location":"m04-tidy_databases/more_tidy_data/#3-converting-wide-to-tidy","title":"3. Converting Wide to Tidy\u00b6","text":"<p>We use <code>pd.melt()</code> to unpivot the data so that each row represents a <code>(Year, Region, Sales)</code> combination.</p>"},{"location":"m04-tidy_databases/more_tidy_data/#4-filtering-grouping","title":"4. Filtering &amp; Grouping\u00b6","text":"<p>We'll do two demonstrations:</p> <ol> <li><p>Filtering data for a specific region and year range.</p> </li> <li><p>Scaling each region's values by its mean sales.</p> </li> </ol>"},{"location":"m04-tidy_databases/more_tidy_data/#41-filtering-for-the-east-region-where-year-2021","title":"4.1 Filtering for the East region where <code>Year &gt; 2021</code>\u00b6","text":""},{"location":"m04-tidy_databases/more_tidy_data/#411-wide-format-complicated","title":"4.1.1 Wide Format (Complicated)\u00b6","text":"<p>Since there's no direct \"Region\" column, we must manually pick the column (<code>East</code>) and rename it:</p>"},{"location":"m04-tidy_databases/more_tidy_data/#412-tidy-format-easy","title":"4.1.2 Tidy Format (Easy)\u00b6","text":"<p>Just use <code>query(\"Region == 'East' and Year &gt; 2021\")</code>:</p>"},{"location":"m04-tidy_databases/more_tidy_data/#42-grouping-scaling-by-the-mean","title":"4.2 Grouping &amp; Scaling by the Mean\u00b6","text":"<p>Task: Divide each region's sales by that region's mean sales.</p>"},{"location":"m04-tidy_databases/more_tidy_data/#421-wide-format","title":"4.2.1 Wide Format\u00b6","text":"<p>Compute the mean of each column and then manually scale:</p>"},{"location":"m04-tidy_databases/more_tidy_data/#422-tidy-format","title":"4.2.2 Tidy Format\u00b6","text":"<p>A single <code>.groupby(\"Region\")</code> and <code>.transform()</code> handles all regions:</p>"},{"location":"m04-tidy_databases/more_tidy_data/#5-visualization-in-altair","title":"5. Visualization in Altair\u00b6","text":"<p>We'll create a simple line plot of Sales over Year for each region.</p> <p>You may need to install altair for this part of the tutorial</p> <pre><code>pip install altair\n</code></pre>"},{"location":"m04-tidy_databases/more_tidy_data/#51-wide-format","title":"5.1 Wide Format\u00b6","text":""},{"location":"m04-tidy_databases/more_tidy_data/#option-a-layer-each-regions-line-separately","title":"Option A: Layer each region's line separately\u00b6","text":""},{"location":"m04-tidy_databases/more_tidy_data/#option-b-use-transform_fold-to-pivot-columns-inside-altair","title":"Option B: Use <code>transform_fold</code> to pivot columns inside Altair\u00b6","text":"<p>This is effectively using Altair to do what <code>melt()</code> does, but you must list all columns:</p>"},{"location":"m04-tidy_databases/more_tidy_data/#52-tidy-format-so-much-simpler","title":"5.2 Tidy Format (So Much Simpler)\u00b6","text":"<p>A single command, no need to list the regions. Altair automatically creates multiple lines, coloring by <code>Region</code>.</p>"},{"location":"m04-tidy_databases/more_tidy_data/#6-final-comparison","title":"6. Final Comparison\u00b6","text":"Action Wide Format Tidy Format Filtering Must pick &amp; rename columns, no direct 'Region' column Simple <code>query(\"Region=='X'\")</code> Grouping or Scaling Repeat operations for each column (or do complex loops) Single <code>.groupby(\"Region\")</code> call Adding New Categories Must add new columns and update code Rows just grow, existing code continues to work Visualization Either layer each column manually or use <code>transform_fold</code> One-liner with <code>color=\"Region:N\"</code> Code Complexity High (lots of repeated steps, listing columns) Low (concise, flexible) <p>Takeaway:</p> <ul> <li><p>Tidy format is recommended for most data science tasks because it avoids repetitive code, makes grouping/filtering straightforward, and integrates smoothly with visualization libraries like Altair, seaborn, etc.</p> </li> <li><p>Wide format can be okay for quick tasks or certain machine learning APIs, but it often becomes cumbersome when you need to filter, group, scale, or plot multiple categories.</p> </li> </ul> <p>Whenever possible, convert to tidy to save time and headaches!</p>"},{"location":"m04-tidy_databases/tidy_data/","title":"Module 04 - Tidy data","text":"In\u00a0[2]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd In\u00a0[5]: Copied! <pre># population over time\ndf = pd.DataFrame({\n    \"country\": [\"USA\", \"Canada\", \"Brazil\"],\n    \"1990\": [253, 28, 149],\n    \"2000\": [282, 31, 170],\n    \"2010\": [309, 34, 192],\n    \"2020\": [339, 38, 209],\n    \"continent\": [\"North America\", \"North America\", \"South America\"],\n})\n\n# Wide format\ndisplay(df)\n</pre> # population over time df = pd.DataFrame({     \"country\": [\"USA\", \"Canada\", \"Brazil\"],     \"1990\": [253, 28, 149],     \"2000\": [282, 31, 170],     \"2010\": [309, 34, 192],     \"2020\": [339, 38, 209],     \"continent\": [\"North America\", \"North America\", \"South America\"], })  # Wide format display(df)  country 1990 2000 2010 2020 continent 0 USA 253 282 309 339 North America 1 Canada 28 31 34 38 North America 2 Brazil 149 170 192 209 South America In\u00a0[7]: Copied! <pre>df_tidy = df.melt(id_vars=[\"country\",\"continent\"], var_name=\"year\", value_name=\"population\")\ndisplay(df_tidy)\n</pre> df_tidy = df.melt(id_vars=[\"country\",\"continent\"], var_name=\"year\", value_name=\"population\") display(df_tidy)  country continent year population 0 USA North America 1990 253 1 Canada North America 1990 28 2 Brazil South America 1990 149 3 USA North America 2000 282 4 Canada North America 2000 31 5 Brazil South America 2000 170 6 USA North America 2010 309 7 Canada North America 2010 34 8 Brazil South America 2010 192 9 USA North America 2020 339 10 Canada North America 2020 38 11 Brazil South America 2020 209 <p>Notice how each row now represents one country in one year, and each column is a single variable.</p> In\u00a0[8]: Copied! <pre>df_wide = df_tidy.pivot(index=\"country\", columns=\"year\", values=\"population\")\ndisplay(df_wide)\n</pre> df_wide = df_tidy.pivot(index=\"country\", columns=\"year\", values=\"population\") display(df_wide)  year 1990 2000 2010 2020 country Brazil 149 170 192 209 Canada 28 31 34 38 USA 253 282 309 339 <p>Here, each row is a country, and each column is a year\u2014back to wide format.</p> In\u00a0[9]: Copied! <pre>df_year_mean = df_tidy.groupby(\"year\")[\"population\"].mean()\ndisplay(df_year_mean)\n# \n</pre> df_year_mean = df_tidy.groupby(\"year\")[\"population\"].mean() display(df_year_mean) #  <pre>year\n1990    143.333333\n2000    161.000000\n2010    178.333333\n2020    195.333333\nName: population, dtype: float64</pre> In\u00a0[11]: Copied! <pre># for key,data in df_tidy.groupby(\"year\"):\n#     display(key)\n#     display(data)\n</pre> # for key,data in df_tidy.groupby(\"year\"): #     display(key) #     display(data) In\u00a0[12]: Copied! <pre>df_year_country_sum = df_tidy.groupby([\"year\", \"country\"])[\"population\"].sum()\ndisplay(df_year_country_sum)\n</pre> df_year_country_sum = df_tidy.groupby([\"year\", \"country\"])[\"population\"].sum() display(df_year_country_sum)  <pre>year  country\n1990  Brazil     149\n      Canada      28\n      USA        253\n2000  Brazil     170\n      Canada      31\n      USA        282\n2010  Brazil     192\n      Canada      34\n      USA        309\n2020  Brazil     209\n      Canada      38\n      USA        339\nName: population, dtype: int64</pre> <p>This returns a multi-index Series, showing the population by year and by country.</p> In\u00a0[13]: Copied! <pre>df_agg = df_tidy.groupby(\"year\").agg({\"population\": [\"mean\", \"max\",\"sum\",\"median\"]})\ndisplay(df_agg)\n</pre> df_agg = df_tidy.groupby(\"year\").agg({\"population\": [\"mean\", \"max\",\"sum\",\"median\"]}) display(df_agg)  population mean max sum median year 1990 143.333333 253 430 149.0 2000 161.000000 282 483 170.0 2010 178.333333 309 535 192.0 2020 195.333333 339 586 209.0 <p>This shows the average (<code>mean</code>) population and the maximum (<code>max</code>) population in each year.</p> In\u00a0[14]: Copied! <pre># Create a copy with artificially introduced NaNs\ndf_missing = df_tidy.copy()\ndf_missing.loc[(df_missing[\"country\"] == \"Brazil\") &amp; (df_missing[\"year\"] == \"2020\"), \"population\"] = None\n\ndisplay(df_missing)\n</pre> # Create a copy with artificially introduced NaNs df_missing = df_tidy.copy() df_missing.loc[(df_missing[\"country\"] == \"Brazil\") &amp; (df_missing[\"year\"] == \"2020\"), \"population\"] = None  display(df_missing)  country continent year population 0 USA North America 1990 253.0 1 Canada North America 1990 28.0 2 Brazil South America 1990 149.0 3 USA North America 2000 282.0 4 Canada North America 2000 31.0 5 Brazil South America 2000 170.0 6 USA North America 2010 309.0 7 Canada North America 2010 34.0 8 Brazil South America 2010 192.0 9 USA North America 2020 339.0 10 Canada North America 2020 38.0 11 Brazil South America 2020 NaN In\u00a0[15]: Copied! <pre>df_dropped = df_missing.dropna(subset=[\"population\"])\ndisplay(df_dropped)\n</pre> df_dropped = df_missing.dropna(subset=[\"population\"]) display(df_dropped)  country continent year population 0 USA North America 1990 253.0 1 Canada North America 1990 28.0 2 Brazil South America 1990 149.0 3 USA North America 2000 282.0 4 Canada North America 2000 31.0 5 Brazil South America 2000 170.0 6 USA North America 2010 309.0 7 Canada North America 2010 34.0 8 Brazil South America 2010 192.0 9 USA North America 2020 339.0 10 Canada North America 2020 38.0 <p>Brazil's 2020 row is completely removed because of the missing population.</p> In\u00a0[16]: Copied! <pre>df_filled = df_missing.fillna(0)\ndisplay(df_filled)\n</pre> df_filled = df_missing.fillna(0) display(df_filled)  country continent year population 0 USA North America 1990 253.0 1 Canada North America 1990 28.0 2 Brazil South America 1990 149.0 3 USA North America 2000 282.0 4 Canada North America 2000 31.0 5 Brazil South America 2000 170.0 6 USA North America 2010 309.0 7 Canada North America 2010 34.0 8 Brazil South America 2010 192.0 9 USA North America 2020 339.0 10 Canada North America 2020 38.0 11 Brazil South America 2020 0.0 <p>Now, the missing value is replaced with <code>0</code>.</p> In\u00a0[17]: Copied! <pre>gdp_data = pd.DataFrame({\n    \"country\": [\"USA\", \"Canada\", \"Brazil\"],\n    \"year\": [\"2020\", \"2020\", \"2020\"],\n    \"gdp\": [21439, 1736, 1445],  # GDP in billions (fictitious or approximate)\n})\n\n# Merging on both country and year\ndf_merged = df_tidy.merge(gdp_data, on=[\"country\", \"year\"], how=\"left\")\ndisplay(df_merged)\n</pre> gdp_data = pd.DataFrame({     \"country\": [\"USA\", \"Canada\", \"Brazil\"],     \"year\": [\"2020\", \"2020\", \"2020\"],     \"gdp\": [21439, 1736, 1445],  # GDP in billions (fictitious or approximate) })  # Merging on both country and year df_merged = df_tidy.merge(gdp_data, on=[\"country\", \"year\"], how=\"left\") display(df_merged)  country continent year population gdp 0 USA North America 1990 253 NaN 1 Canada North America 1990 28 NaN 2 Brazil South America 1990 149 NaN 3 USA North America 2000 282 NaN 4 Canada North America 2000 31 NaN 5 Brazil South America 2000 170 NaN 6 USA North America 2010 309 NaN 7 Canada North America 2010 34 NaN 8 Brazil South America 2010 192 NaN 9 USA North America 2020 339 21439.0 10 Canada North America 2020 38 1736.0 11 Brazil South America 2020 209 1445.0 <p>We used <code>how=\"left\"</code> so that all rows from <code>df_tidy</code> are preserved, even if some may not match in <code>gdp_data</code>.</p> <ul> <li><p><code>how=\"inner\"</code> would only keep matching rows.</p> </li> <li><p><code>how=\"outer\"</code> keeps all rows from both DataFrames.</p> </li> </ul> In\u00a0[18]: Copied! <pre># Sort by population descending\ndf_sorted = df_tidy.sort_values(\"population\", ascending=False)\ndisplay(df_sorted)\n</pre> # Sort by population descending df_sorted = df_tidy.sort_values(\"population\", ascending=False) display(df_sorted)  country continent year population 9 USA North America 2020 339 6 USA North America 2010 309 3 USA North America 2000 282 0 USA North America 1990 253 11 Brazil South America 2020 209 8 Brazil South America 2010 192 5 Brazil South America 2000 170 2 Brazil South America 1990 149 10 Canada North America 2020 38 7 Canada North America 2010 34 4 Canada North America 2000 31 1 Canada North America 1990 28 In\u00a0[20]: Copied! <pre>df_filtered = df_tidy.query(\"population &gt; 300 and country == 'USA'\")\ndisplay(df_filtered)\n</pre> df_filtered = df_tidy.query(\"population &gt; 300 and country == 'USA'\") display(df_filtered)  country continent year population 6 USA North America 2010 309 9 USA North America 2020 339"},{"location":"m04-tidy_databases/tidy_data/#module-04-tidy-data","title":"Module 04 - Tidy data\u00b6","text":"<p>What is Tidy Data?</p> <p>Tidy data is a structured format where:</p> <pre><code> - Each row represents one observation (e.g., a country in a given year).\n - Each column is a variable (e.g., GDP, life expectancy).\n - Each table represents a dataset (e.g., economic statistics).</code></pre> <p>\ud83d\udca1 Why use tidy data?</p> <pre><code> - Easier to analyze: Works well with `groupby()`, `agg()`, and visualization libraries like Seaborn.\n - More readable: No redundant columns.\n - Plays nicely with Pandas and Seaborn.</code></pre>"},{"location":"m04-tidy_databases/tidy_data/#wide-format-to-tidy-long-format","title":"Wide Format to Tidy (Long) Format\u00b6","text":"<p>In the dataset below, each year's population is in a separate column, which makes it a wide format.</p> <p>We can convert it to tidy format using <code>pd.melt()</code>.</p>"},{"location":"m04-tidy_databases/tidy_data/#pdmelt","title":"<code>pd.melt()</code>\u00b6","text":"<ul> <li><p><code>id_vars</code>: The columns that stay the same (identifiers).</p> </li> <li><p><code>var_name</code>: Name of the new column that will hold the old column headers (years).</p> </li> <li><p><code>value_name</code>: Name of the new column that will store the values (population in this case).</p> </li> </ul>"},{"location":"m04-tidy_databases/tidy_data/#converting-tidy-long-format-back-to-wide-format","title":"Converting Tidy (Long) Format Back to Wide Format\u00b6","text":"<ul> <li>If you ever need to go back to wide format, you can use <code>pivot()</code> or <code>pivot_table()</code>.</li> </ul>"},{"location":"m04-tidy_databases/tidy_data/#summarizing-tidy-data-with-groupby","title":"Summarizing Tidy Data with <code>groupby()</code>\u00b6","text":"<p>Tidy data makes it straightforward to group and summarize.</p>"},{"location":"m04-tidy_databases/tidy_data/#groupbyyearpopulationmean","title":"<code>groupby(\"year\")[\"population\"].mean()</code>\u00b6","text":"<p>This computes the mean population for each year across all countries.</p>"},{"location":"m04-tidy_databases/tidy_data/#grouping-by-multiple-columns","title":"Grouping by Multiple Columns\u00b6","text":"<p>We can also group by both <code>year</code> and <code>country</code>.</p>"},{"location":"m04-tidy_databases/tidy_data/#agg-for-multiple-summaries","title":"<code>agg()</code> for Multiple Summaries\u00b6","text":"<p>The <code>agg()</code> function lets us apply multiple aggregations at once.</p> <p>For instance, we can find the mean and the max population per year.</p>"},{"location":"m04-tidy_databases/tidy_data/#handling-missing-data","title":"Handling Missing Data\u00b6","text":"<p>Let's introduce some missing values to demonstrate <code>dropna()</code> and <code>fillna()</code>.</p>"},{"location":"m04-tidy_databases/tidy_data/#dropna","title":"<code>dropna()</code>\u00b6","text":"<ul> <li>Removes rows with missing values.</li> </ul>"},{"location":"m04-tidy_databases/tidy_data/#fillna","title":"<code>fillna()</code>\u00b6","text":"<ul> <li>Fills missing values with a specified value or method.</li> </ul>"},{"location":"m04-tidy_databases/tidy_data/#combining-data-with-merge","title":"Combining Data with <code>merge()</code>\u00b6","text":"<p>Often, you'll have multiple DataFrames that need to be joined.</p> <p>Below is an example for merging a GDP dataset with our population dataset.</p>"},{"location":"m04-tidy_databases/tidy_data/#example-sort_values-and-query","title":"Example: <code>sort_values()</code> and <code>query()</code>\u00b6","text":"<p>Tidy data also makes it easy to sort and filter.</p>"},{"location":"m04-tidy_databases/tidy_data/#query","title":"<code>query()</code>\u00b6","text":"<p>An alternative way to filter rows:</p> <pre>df.query(\"population &gt; 200 and country == 'USA'\")\n</pre> <p>is equivalent to</p> <pre>df[(df[\"population\"] &gt; 200) &amp; (df[\"country\"] == \"USA\")]\n</pre>"},{"location":"m05-ml_eda/assignment_m05_eda/","title":"Homework 5: Exploratory Data Analysis (EDA)","text":"<p>Create your cells below this one. Hint: start by imporint the necessary libraries and loading your dataset.</p>"},{"location":"m05-ml_eda/assignment_m05_eda/#homework-5-exploratory-data-analysis-eda","title":"Homework 5: Exploratory Data Analysis (EDA)\u00b6","text":"<p>In this assignment, you are going to perform exploratory data analysis (EDA) on a small dataset of your choice. You can choose any dataset you like, but you are encouraged to pick a dataset that you are interested in. You can use the datasets you have used in the previous assignments or you can choose a new dataset. If you don't have a dataset in mind, you can choose one from the datasets in the <code>Datasets</code> folder of the course repository.</p>"},{"location":"m05-ml_eda/assignment_m05_eda/#instructions","title":"Instructions\u00b6","text":"<ol> <li>Follow the instructions on how to setup your Python and Jupyter (or VSCode) environment and cloning or downloading our repository. Instructions can be found in the class notes: https://filipinascimento.github.io/usable_ai/m00-setup/class</li> <li>Ensure that you have Python and Jupyter Notebook working. (You can also try using Google Colab. This is not the preferred method for this homework, but it is an option)</li> <li>Load the dataset of your choice into a Pandas dataframe</li> <li>Perform exploratory data analysis (EDA) on the dataset. Your analysis should include the following:<ul> <li>Summary statistics of the dataset</li> <li>Data cleaning and preprocessing</li> <li>Data visualization (e.g., histograms, scatterplots, etc.)</li> <li>You should write a brief summary of the insights and conclusions you have drawn from your analysis. You can use the exploratory_data_analysis.ipynb as a reference.</li> </ul> </li> <li>Important: Create both code and markdown cells in your notebook to document your analysis.</li> <li>Submit your completed notebook as a HTML export, or a PDF file.</li> </ol>"},{"location":"m05-ml_eda/assignment_m05_eda/#submission-guidelines","title":"Submission Guidelines\u00b6","text":"<ul> <li>Submit your completed notebook as a HTML export, or a PDF file.</li> </ul> <p>To export to HTML, if you are on Jupyter, select <code>File</code> &gt; <code>Export Notebook As</code> &gt; <code>HTML</code>.</p> <p>If you are on VSCode, you can use the <code>Jupyter: Export to HTML</code> command.</p> <ul> <li>Open the command palette (Ctrl+Shift+P or Cmd+Shift+P on Mac).<ul> <li>Search for <code>Jupyter: Export to HTML</code>.</li> <li>Save the HTML file to your computer and submit it via Canvas.</li> </ul> </li> </ul> <p>Using Generative AI Responsibly</p> <p>You're welcome to use Generative AI to assist your learning, but focus on understanding the concepts rather than just solving the assignment. For example, instead of copying and pasting the question into the model, ask it to explain the concept in the question. Try asking: <code>How can I open a file in Python? Can you give me examples?</code> or <code>What functions and methods can I use to extract the words of a text file? Can you explain how they work with some examples?</code></p> <p>This way, you will learn how the solution works while building your skills. Remember to give context to the generative AI, so it can better assist you. Talk to the instructor and AIs if you have any questions or need insights.</p>"},{"location":"m05-ml_eda/exploratory_data_analysis/","title":"Module 05 - Exploratory Data Analysis","text":"In\u00a0[2]: Copied! <pre>from tqdm.auto import tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport seaborn as sns\n</pre> from tqdm.auto import tqdm import numpy as np import pandas as pd import matplotlib.pyplot as plt from scipy import stats import seaborn as sns In\u00a0[3]: Copied! <pre>df_cars = pd.read_csv('../../Datasets/carfeatures.csv')\n# Adapt your path to the location of the dataset\n</pre> df_cars = pd.read_csv('../../Datasets/carfeatures.csv') # Adapt your path to the location of the dataset In\u00a0[4]: Copied! <pre>df_cars.head()\n</pre> df_cars.head() Out[4]: Make Model Year Engine Fuel Type Engine HP Engine Cylinders Transmission Type Driven_Wheels Number of Doors Market Category Vehicle Size Vehicle Style highway MPG city mpg Popularity MSRP 0 BMW 1 Series M 2011 premium unleaded (required) 335.0 6.0 MANUAL rear wheel drive 2.0 Factory Tuner,Luxury,High-Performance Compact Coupe 26 19 3916 46135 1 BMW 1 Series 2011 premium unleaded (required) 300.0 6.0 MANUAL rear wheel drive 2.0 Luxury,Performance Compact Convertible 28 19 3916 40650 2 BMW 1 Series 2011 premium unleaded (required) 300.0 6.0 MANUAL rear wheel drive 2.0 Luxury,High-Performance Compact Coupe 28 20 3916 36350 3 BMW 1 Series 2011 premium unleaded (required) 230.0 6.0 MANUAL rear wheel drive 2.0 Luxury,Performance Compact Coupe 28 18 3916 29450 4 BMW 1 Series 2011 premium unleaded (required) 230.0 6.0 MANUAL rear wheel drive 2.0 Luxury Compact Convertible 28 18 3916 34500 <p>You can use <code>.info()</code> to get a concise summary of the dataset, including the data types and the number of non-null entries. This method is useful for quickly assessing the dataset's size and structure.</p> In\u00a0[5]: Copied! <pre>df_cars.info()\n</pre> df_cars.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 11914 entries, 0 to 11913\nData columns (total 16 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   Make               11914 non-null  object \n 1   Model              11914 non-null  object \n 2   Year               11914 non-null  int64  \n 3   Engine Fuel Type   11911 non-null  object \n 4   Engine HP          11845 non-null  float64\n 5   Engine Cylinders   11884 non-null  float64\n 6   Transmission Type  11914 non-null  object \n 7   Driven_Wheels      11914 non-null  object \n 8   Number of Doors    11908 non-null  float64\n 9   Market Category    8172 non-null   object \n 10  Vehicle Size       11914 non-null  object \n 11  Vehicle Style      11914 non-null  object \n 12  highway MPG        11914 non-null  int64  \n 13  city mpg           11914 non-null  int64  \n 14  Popularity         11914 non-null  int64  \n 15  MSRP               11914 non-null  int64  \ndtypes: float64(3), int64(5), object(8)\nmemory usage: 1.5+ MB\n</pre> <p>You may also want to check the range of your data. You can use min(), max(), or describe() to get the minimum and maximum values of each column. This information can help you identify potential outliers or errors in the data. For instance, let's look at the Year range in the car dataset.</p> In\u00a0[6]: Copied! <pre>df_cars[\"Year\"].min(),df_cars[\"Year\"].max()\n</pre> df_cars[\"Year\"].min(),df_cars[\"Year\"].max() Out[6]: <pre>(1990, 2017)</pre> <p>What about the different makes? We can use unique to get the unique values in the Make column.</p> In\u00a0[7]: Copied! <pre>df_cars[\"Make\"].unique()\n</pre> df_cars[\"Make\"].unique() Out[7]: <pre>array(['BMW', 'Audi', 'FIAT', 'Mercedes-Benz', 'Chrysler', 'Nissan',\n       'Volvo', 'Mazda', 'Mitsubishi', 'Ferrari', 'Alfa Romeo', 'Toyota',\n       'McLaren', 'Maybach', 'Pontiac', 'Porsche', 'Saab', 'GMC',\n       'Hyundai', 'Plymouth', 'Honda', 'Oldsmobile', 'Suzuki', 'Ford',\n       'Cadillac', 'Kia', 'Bentley', 'Chevrolet', 'Dodge', 'Lamborghini',\n       'Lincoln', 'Subaru', 'Volkswagen', 'Spyker', 'Buick', 'Acura',\n       'Rolls-Royce', 'Maserati', 'Lexus', 'Aston Martin', 'Land Rover',\n       'Lotus', 'Infiniti', 'Scion', 'Genesis', 'HUMMER', 'Tesla',\n       'Bugatti'], dtype=object)</pre> <p>What about Vehicle Style?</p> In\u00a0[8]: Copied! <pre>df_cars[\"Vehicle Style\"].unique()\n</pre> df_cars[\"Vehicle Style\"].unique() Out[8]: <pre>array(['Coupe', 'Convertible', 'Sedan', 'Wagon', '4dr Hatchback',\n       '2dr Hatchback', '4dr SUV', 'Passenger Minivan', 'Cargo Minivan',\n       'Crew Cab Pickup', 'Regular Cab Pickup', 'Extended Cab Pickup',\n       '2dr SUV', 'Cargo Van', 'Convertible SUV', 'Passenger Van'],\n      dtype=object)</pre> In\u00a0[9]: Copied! <pre>df_cars[\"highway MPG\"].mean()\n</pre> df_cars[\"highway MPG\"].mean() Out[9]: <pre>26.637485311398354</pre> In\u00a0[10]: Copied! <pre>df_cars[\"city mpg\"].mean()\n</pre> df_cars[\"city mpg\"].mean() Out[10]: <pre>19.73325499412456</pre> In\u00a0[11]: Copied! <pre>df_cars[\"highway MPG\"].std()\n</pre> df_cars[\"highway MPG\"].std() Out[11]: <pre>8.863000766979432</pre> In\u00a0[12]: Copied! <pre>df_cars[\"city mpg\"].std()\n</pre> df_cars[\"city mpg\"].std() Out[12]: <pre>8.987798160299246</pre> In\u00a0[13]: Copied! <pre>df_cars[\"city mpg\"].quantile(0.5)\n</pre> df_cars[\"city mpg\"].quantile(0.5) Out[13]: <pre>18.0</pre> In\u00a0[14]: Copied! <pre>df_cars.describe().transpose()\n</pre> df_cars.describe().transpose() Out[14]: count mean std min 25% 50% 75% max Year 11914.0 2010.384338 7.579740 1990.0 2007.0 2015.0 2016.00 2017.0 Engine HP 11845.0 249.386070 109.191870 55.0 170.0 227.0 300.00 1001.0 Engine Cylinders 11884.0 5.628829 1.780559 0.0 4.0 6.0 6.00 16.0 Number of Doors 11908.0 3.436093 0.881315 2.0 2.0 4.0 4.00 4.0 highway MPG 11914.0 26.637485 8.863001 12.0 22.0 26.0 30.00 354.0 city mpg 11914.0 19.733255 8.987798 7.0 16.0 18.0 22.00 137.0 Popularity 11914.0 1554.911197 1441.855347 2.0 549.0 1385.0 2009.00 5657.0 MSRP 11914.0 40594.737032 60109.103604 2000.0 21000.0 29995.0 42231.25 2065902.0 In\u00a0[15]: Copied! <pre>fig = plt.figure(figsize=(5, 3))\nplt.hist(df_cars['city mpg'],bins=100)\nplt.xlabel('city mpg')\nplt.ylabel('Frequency')\nplt.title('Histogram of city mpg')\nplt.tight_layout()\nplt.show()\n</pre> fig = plt.figure(figsize=(5, 3)) plt.hist(df_cars['city mpg'],bins=100) plt.xlabel('city mpg') plt.ylabel('Frequency') plt.title('Histogram of city mpg') plt.tight_layout() plt.show() <p>Do we see any outliers? Let's try z-scoring (standardizing)...</p> In\u00a0[16]: Copied! <pre>zscore_city_mpg = stats.zscore(df_cars['city mpg'])\n# zscore_city_mpg = (df_cars['city mpg'] - df_cars['city mpg'].mean())/df_cars['city mpg'].std()\nfig = plt.figure(figsize=(5, 3))\nplt.hist(zscore_city_mpg,bins=100)\nplt.xlabel('city mpg (z-score)')\nplt.ylabel('Frequency')\nplt.title('Histogram of city mpg (z-score)')\nplt.tight_layout()\nplt.show()\n</pre> zscore_city_mpg = stats.zscore(df_cars['city mpg']) # zscore_city_mpg = (df_cars['city mpg'] - df_cars['city mpg'].mean())/df_cars['city mpg'].std() fig = plt.figure(figsize=(5, 3)) plt.hist(zscore_city_mpg,bins=100) plt.xlabel('city mpg (z-score)') plt.ylabel('Frequency') plt.title('Histogram of city mpg (z-score)') plt.tight_layout() plt.show() <p>What is the problem?</p> In\u00a0[17]: Copied! <pre>df_cars[zscore_city_mpg&gt;3]\n</pre> df_cars[zscore_city_mpg&gt;3] Out[17]: Make Model Year Engine Fuel Type Engine HP Engine Cylinders Transmission Type Driven_Wheels Number of Doors Market Category Vehicle Size Vehicle Style highway MPG city mpg Popularity MSRP 539 FIAT 500e 2015 electric NaN 0.0 DIRECT_DRIVE front wheel drive 2.0 Hatchback Compact 2dr Hatchback 108 122 819 31800 540 FIAT 500e 2016 electric NaN 0.0 DIRECT_DRIVE front wheel drive 2.0 Hatchback Compact 2dr Hatchback 103 121 819 31800 541 FIAT 500e 2017 electric NaN 0.0 DIRECT_DRIVE front wheel drive 2.0 Hatchback Compact 2dr Hatchback 103 121 819 31800 1202 Honda Accord Hybrid 2014 regular unleaded 195.0 4.0 AUTOMATIC front wheel drive 4.0 Hybrid Midsize Sedan 45 50 2202 31905 1203 Honda Accord Hybrid 2014 regular unleaded 195.0 4.0 AUTOMATIC front wheel drive 4.0 Hybrid Midsize Sedan 45 50 2202 29155 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 9868 Chevrolet Spark EV 2014 electric 140.0 0.0 DIRECT_DRIVE front wheel drive 4.0 Hatchback Compact 4dr Hatchback 109 128 1385 27010 9869 Chevrolet Spark EV 2015 electric 140.0 0.0 DIRECT_DRIVE front wheel drive 4.0 Hatchback Compact 4dr Hatchback 109 128 1385 25170 9870 Chevrolet Spark EV 2015 electric 140.0 0.0 DIRECT_DRIVE front wheel drive 4.0 Hatchback Compact 4dr Hatchback 109 128 1385 25560 9871 Chevrolet Spark EV 2016 electric 140.0 0.0 DIRECT_DRIVE front wheel drive 4.0 Hatchback Compact 4dr Hatchback 109 128 1385 25510 9872 Chevrolet Spark EV 2016 electric 140.0 0.0 DIRECT_DRIVE front wheel drive 4.0 Hatchback Compact 4dr Hatchback 109 128 1385 25120 <p>112 rows \u00d7 16 columns</p> <p>So let's remove the outliers and see the distribution again.</p> In\u00a0[18]: Copied! <pre>df_cars_filtered = df_cars[~((df_cars['Engine Fuel Type'] == 'electric') |\n                        (df_cars['Market Category'].str.contains('Hybrid')))]\n\nfig = plt.figure(figsize=(5, 3))\nplt.hist(df_cars_filtered['city mpg'], bins=30)\nplt.xlabel('city mpg')\nplt.ylabel('Frequency')\nplt.title('Histogram of city mpg (without Electric and Hybrid)')\nplt.tight_layout()\nplt.show()\n</pre> df_cars_filtered = df_cars[~((df_cars['Engine Fuel Type'] == 'electric') |                         (df_cars['Market Category'].str.contains('Hybrid')))]  fig = plt.figure(figsize=(5, 3)) plt.hist(df_cars_filtered['city mpg'], bins=30) plt.xlabel('city mpg') plt.ylabel('Frequency') plt.title('Histogram of city mpg (without Electric and Hybrid)') plt.tight_layout() plt.show() In\u00a0[19]: Copied! <pre># calculate the correlation between city mpg and highway mpg\n\ndf_cars_filtered['city mpg'].corr(df_cars_filtered['highway MPG'])\n</pre> # calculate the correlation between city mpg and highway mpg  df_cars_filtered['city mpg'].corr(df_cars_filtered['highway MPG']) Out[19]: <pre>0.8520452086020569</pre> <p>We can also create a table of correlation. You can use seaborn for that.</p> In\u00a0[20]: Copied! <pre>numeric_variables = df_cars_filtered.select_dtypes(include=np.number)\ncorrelation_table = numeric_variables.corr()\ncorrelation_table\n\n\n# Create heatmap\nplt.figure(figsize=(6, 4))\nsns.heatmap(correlation_table, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\nplt.title('Correlation Matrix')\nplt.tight_layout()\nplt.show()\n</pre> numeric_variables = df_cars_filtered.select_dtypes(include=np.number) correlation_table = numeric_variables.corr() correlation_table   # Create heatmap plt.figure(figsize=(6, 4)) sns.heatmap(correlation_table, annot=True, cmap='coolwarm', vmin=-1, vmax=1) plt.title('Correlation Matrix') plt.tight_layout() plt.show() <p>For non-linear relationships, we can use the Spearman correlation coefficient.</p> In\u00a0[21]: Copied! <pre># same but for Spearman\ncorrelation_table = numeric_variables.corr(method='spearman')\n\n# Create heatmap\nplt.figure(figsize=(6, 4))\nsns.heatmap(correlation_table, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\nplt.title('Spearman Corr. Table')\nplt.tight_layout()\nplt.show()\n</pre> # same but for Spearman correlation_table = numeric_variables.corr(method='spearman')  # Create heatmap plt.figure(figsize=(6, 4)) sns.heatmap(correlation_table, annot=True, cmap='coolwarm', vmin=-1, vmax=1) plt.title('Spearman Corr. Table') plt.tight_layout() plt.show()  <p>For instance, how the 'city mpg' changed over the years?</p> In\u00a0[22]: Copied! <pre># Check trends of city MPG over time (average over time)\nplt.figure(figsize=(5, 3))\ndf_cars_filtered.groupby(\"Year\")[\"city mpg\"].mean().plot()\nplt.ylabel('Average City MPG')\nplt.title('Average City MPG over Time')\nplt.tight_layout()\nplt.show()\n</pre> # Check trends of city MPG over time (average over time) plt.figure(figsize=(5, 3)) df_cars_filtered.groupby(\"Year\")[\"city mpg\"].mean().plot() plt.ylabel('Average City MPG') plt.title('Average City MPG over Time') plt.tight_layout() plt.show()  <p>We can also look at the City MPG across different makes.</p> In\u00a0[23]: Copied! <pre># plot brands with highest average city MPG after 2010\nplt.figure(figsize=(5, 3))\ncars_means = df_cars_filtered[df_cars_filtered['Year'] &gt;= 2010].groupby(\"Make\")[\"city mpg\"].mean()\ncars_means = cars_means.sort_values(ascending=False).head(10)\nax = cars_means.plot(kind=\"barh\")\nplt.xlabel('Average City MPG')\nplt.title('Average City MPG by Make after 2010')\nax.invert_yaxis()\nplt.tight_layout()\nplt.show()\n</pre> # plot brands with highest average city MPG after 2010 plt.figure(figsize=(5, 3)) cars_means = df_cars_filtered[df_cars_filtered['Year'] &gt;= 2010].groupby(\"Make\")[\"city mpg\"].mean() cars_means = cars_means.sort_values(ascending=False).head(10) ax = cars_means.plot(kind=\"barh\") plt.xlabel('Average City MPG') plt.title('Average City MPG by Make after 2010') ax.invert_yaxis() plt.tight_layout() plt.show() In\u00a0[24]: Copied! <pre>df_iris = pd.read_csv('../../Datasets/iris.csv')\ndf_iris.head()\n</pre> df_iris = pd.read_csv('../../Datasets/iris.csv') df_iris.head() Out[24]: Id SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm Species 0 1 5.1 3.5 1.4 0.2 Iris-setosa 1 2 4.9 3.0 1.4 0.2 Iris-setosa 2 3 4.7 3.2 1.3 0.2 Iris-setosa 3 4 4.6 3.1 1.5 0.2 Iris-setosa 4 5 5.0 3.6 1.4 0.2 Iris-setosa In\u00a0[25]: Copied! <pre>fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(5, 4))\n\n# Plot SepalLengthCm\naxes[0, 0].hist(df_iris['SepalLengthCm'], bins=20)\naxes[0, 0].set_xlabel('Sepal Length (cm)')\naxes[0, 0].set_ylabel('Frequency')\naxes[0, 0].set_title('Histogram of Sepal Length')\n\n# Plot SepalWidthCm\naxes[0, 1].hist(df_iris['SepalWidthCm'], bins=20)\naxes[0, 1].set_xlabel('Sepal Width (cm)')\naxes[0, 1].set_ylabel('Frequency')\naxes[0, 1].set_title('Histogram of Sepal Width')\n\n# Plot PetalLengthCm\naxes[1, 0].hist(df_iris['PetalLengthCm'], bins=20)\naxes[1, 0].set_xlabel('Petal Length (cm)')\naxes[1, 0].set_ylabel('Frequency')\naxes[1, 0].set_title('Histogram of Petal Length')\n\n# Plot PetalWidthCm\naxes[1, 1].hist(df_iris['PetalWidthCm'], bins=20)\naxes[1, 1].set_xlabel('Petal Width (cm)')\naxes[1, 1].set_ylabel('Frequency')\naxes[1, 1].set_title('Histogram of Petal Width')\n\nplt.tight_layout()\nplt.show()\n</pre> fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(5, 4))  # Plot SepalLengthCm axes[0, 0].hist(df_iris['SepalLengthCm'], bins=20) axes[0, 0].set_xlabel('Sepal Length (cm)') axes[0, 0].set_ylabel('Frequency') axes[0, 0].set_title('Histogram of Sepal Length')  # Plot SepalWidthCm axes[0, 1].hist(df_iris['SepalWidthCm'], bins=20) axes[0, 1].set_xlabel('Sepal Width (cm)') axes[0, 1].set_ylabel('Frequency') axes[0, 1].set_title('Histogram of Sepal Width')  # Plot PetalLengthCm axes[1, 0].hist(df_iris['PetalLengthCm'], bins=20) axes[1, 0].set_xlabel('Petal Length (cm)') axes[1, 0].set_ylabel('Frequency') axes[1, 0].set_title('Histogram of Petal Length')  # Plot PetalWidthCm axes[1, 1].hist(df_iris['PetalWidthCm'], bins=20) axes[1, 1].set_xlabel('Petal Width (cm)') axes[1, 1].set_ylabel('Frequency') axes[1, 1].set_title('Histogram of Petal Width')  plt.tight_layout() plt.show() In\u00a0[26]: Copied! <pre>fig = plt.figure(figsize=(5, 3))\nfor species, group in df_iris.groupby('Species'):\n    plt.hist(group['PetalLengthCm'], bins=10, alpha=0.5, label=species)\nplt.xlabel('Petal Length (cm)')\nplt.ylabel('Frequency')\nplt.title('Histogram of Petal Length')\nplt.legend()\nplt.tight_layout()\nplt.show()\n</pre> fig = plt.figure(figsize=(5, 3)) for species, group in df_iris.groupby('Species'):     plt.hist(group['PetalLengthCm'], bins=10, alpha=0.5, label=species) plt.xlabel('Petal Length (cm)') plt.ylabel('Frequency') plt.title('Histogram of Petal Length') plt.legend() plt.tight_layout() plt.show() In\u00a0[27]: Copied! <pre>dtypes = {\n    \"Tweet Id\" : \"str\",\n    \"ConversationId\": \"str\",\n}\ndf_ukraine = pd.read_csv('../../Datasets/Ukraine_tweets.csv',dtype=dtypes)\ndf_ukraine.head()\n</pre> dtypes = {     \"Tweet Id\" : \"str\",     \"ConversationId\": \"str\", } df_ukraine = pd.read_csv('../../Datasets/Ukraine_tweets.csv',dtype=dtypes) df_ukraine.head() <pre>/var/folders/jh/xkyk5yn976z_y46xvbg2kjjm0000gn/T/ipykernel_7232/1895728512.py:5: DtypeWarning: Columns (0,2,3,4,5,6,7,11,13,14,15,16,17,18) have mixed types. Specify dtype option on import or set low_memory=False.\n  df_ukraine = pd.read_csv('../../Datasets/Ukraine_tweets.csv',dtype=dtypes)\n</pre> Out[27]: Datetime Tweet Id Text Username Permalink User Outlinks CountLinks ReplyCount RetweetCount LikeCount QuoteCount ConversationId Language Source Media QuotedTweet MentionedUsers hashtag hastag_counts 0 2022-02-24 03:12:47+00:00 1496684505247141897 \ud83c\uddfa\ud83c\udde6 Massive explosions rocks #Kharkiv. #Russia ... IdeologyWars https://twitter.com/IdeologyWars/status/149668... https://twitter.com/IdeologyWars NaN NaN 2.0 17.0 47.0 1.0 1496493071495987201 en &lt;a href=\"https://mobile.twitter.com\" rel=\"nofo... [Photo(previewUrl='https://pbs.twimg.com/media... NaN NaN ['#Kharkiv.', '#Russia', '#Ukraine', '#Ukraine... 9.0 1 2022-02-24 03:18:54+00:00 1496686044275695616 \ud83c\uddfa\ud83c\udde6 Kharkiv is officially being struck by major... IdeologyWars https://twitter.com/IdeologyWars/status/149668... https://twitter.com/IdeologyWars NaN NaN 2.0 104.0 188.0 8.0 1496493071495987201 en &lt;a href=\"https://mobile.twitter.com\" rel=\"nofo... [Video(thumbnailUrl='https://pbs.twimg.com/ext... NaN NaN ['#Russia', '#Ukraine', '#UkraineWar', '#Russi... 8.0 2 2022-02-24 03:22:42+00:00 1496687000375726080 \ud83c\uddfa\ud83c\udde6 More angles on that strike in Kharkiv. #Rus... IdeologyWars https://twitter.com/IdeologyWars/status/149668... https://twitter.com/IdeologyWars NaN NaN 1.0 41.0 75.0 5.0 1496493071495987201 en &lt;a href=\"https://mobile.twitter.com\" rel=\"nofo... [Video(thumbnailUrl='https://pbs.twimg.com/ext... NaN NaN ['#Russia', '#Ukraine', '#UkraineWar', '#Russi... 8.0 3 2022-02-24 03:25:36+00:00 1496687731434565636 \ud83c\uddfa\ud83c\udde6 BM-21 Grad strikes opening on #Mariupol cit... IdeologyWars https://twitter.com/IdeologyWars/status/149668... https://twitter.com/IdeologyWars NaN NaN 21.0 407.0 1099.0 149.0 1496493071495987201 en &lt;a href=\"https://mobile.twitter.com\" rel=\"nofo... [Video(thumbnailUrl='https://pbs.twimg.com/ext... NaN NaN ['#Mariupol', '#Russia', '#Ukraine', '#Ukraine... 9.0 4 2022-02-24 03:27:28+00:00 1496688201242759177 \ud83c\uddfa\ud83c\udde6 Damage caused by strike in Kharkiv... #Russ... IdeologyWars https://twitter.com/IdeologyWars/status/149668... https://twitter.com/IdeologyWars NaN NaN 7.0 152.0 207.0 22.0 1496493071495987201 en &lt;a href=\"https://mobile.twitter.com\" rel=\"nofo... [Video(thumbnailUrl='https://pbs.twimg.com/ext... NaN NaN ['#Russia', '#Ukraine', '#UkraineWar', '#Russi... 8.0 <p>Do we have any missing values in the dataset?</p> In\u00a0[28]: Copied! <pre># Summary with missing values\ndf_ukraine.describe()\n</pre> # Summary with missing values df_ukraine.describe() Out[28]: ReplyCount RetweetCount LikeCount hastag_counts count 44070.000000 44070.000000 4.407000e+04 44066.000000 mean 0.887474 2.671681 1.368781e+14 4.039827 std 10.443866 37.890258 1.436683e+16 2.685003 min 0.000000 0.000000 0.000000e+00 0.000000 25% 0.000000 0.000000 0.000000e+00 2.000000 50% 0.000000 0.000000 1.000000e+00 3.000000 75% 0.000000 1.000000 2.000000e+00 5.000000 max 945.000000 5436.000000 1.508172e+18 29.000000 <p>Let's visualize the distribution of the number of times a tweet was retweeted.</p> In\u00a0[29]: Copied! <pre># distribution of number of retweets\nplt.figure(figsize=(5, 3))\nretweetCount = df_ukraine['RetweetCount'].dropna() # Remove missing values\nplt.hist(retweetCount, density=True)\nplt.xlabel('Retweet Count')\nplt.ylabel('Density')\nplt.title('Histogram of Retweet Count')\nplt.tight_layout()\nplt.show()\n\n# increase number of bins\n</pre> # distribution of number of retweets plt.figure(figsize=(5, 3)) retweetCount = df_ukraine['RetweetCount'].dropna() # Remove missing values plt.hist(retweetCount, density=True) plt.xlabel('Retweet Count') plt.ylabel('Density') plt.title('Histogram of Retweet Count') plt.tight_layout() plt.show()  # increase number of bins <p>Oh no, we can't see much. Let's try to use a log-log scale to visualize the distribution of retweet counts.</p> In\u00a0[30]: Copied! <pre># distribution of number of retweets\nplt.figure(figsize=(5, 3))\nretweetCount = df_ukraine['RetweetCount'].dropna() # Remove missing values\nbins = np.logspace(0,np.log10(retweetCount.max()), 21)\nplt.hist(retweetCount, density=True,bins=bins)\nplt.xscale(\"log\")\nplt.yscale(\"log\")\nplt.xlabel('Retweet Count')\nplt.ylabel('Density')\nplt.title('Histogram of Retweet Count')\nplt.tight_layout()\nplt.show()\n</pre> # distribution of number of retweets plt.figure(figsize=(5, 3)) retweetCount = df_ukraine['RetweetCount'].dropna() # Remove missing values bins = np.logspace(0,np.log10(retweetCount.max()), 21) plt.hist(retweetCount, density=True,bins=bins) plt.xscale(\"log\") plt.yscale(\"log\") plt.xlabel('Retweet Count') plt.ylabel('Density') plt.title('Histogram of Retweet Count') plt.tight_layout() plt.show() <p>We can also use a circles instead of bars. This way, we can see the distribution more clearly.</p> In\u00a0[31]: Copied! <pre># distribution of number of retweets\nplt.figure(figsize=(5, 3))\nhist, bins = np.histogram(retweetCount, bins=np.logspace(1, np.log10(retweetCount.max()), 21),density=True)\nplt.plot(bins[:-1], hist,\"o \",ms=4)\nplt.xscale(\"log\")\nplt.yscale(\"log\")\nplt.xlabel('Retweet Count')\nplt.ylabel('Density')\nplt.title('Histogram of Retweet Count')\nplt.tight_layout()\nplt.show()\n</pre> # distribution of number of retweets plt.figure(figsize=(5, 3)) hist, bins = np.histogram(retweetCount, bins=np.logspace(1, np.log10(retweetCount.max()), 21),density=True) plt.plot(bins[:-1], hist,\"o \",ms=4) plt.xscale(\"log\") plt.yscale(\"log\") plt.xlabel('Retweet Count') plt.ylabel('Density') plt.title('Histogram of Retweet Count') plt.tight_layout() plt.show()  In\u00a0[32]: Copied! <pre>df_anscombe = pd.read_csv('../../Datasets/Anscombe_quartet_data.csv')\ndf_anscombe.head()\n</pre> df_anscombe = pd.read_csv('../../Datasets/Anscombe_quartet_data.csv') df_anscombe.head() Out[32]: x123 y1 y2 y3 x4 y4 0 10.0 8.04 9.14 7.46 8.0 6.58 1 8.0 6.95 8.14 6.77 8.0 5.76 2 13.0 7.58 8.74 12.74 8.0 7.71 3 9.0 8.81 8.77 7.11 8.0 8.84 4 11.0 8.33 9.26 7.81 8.0 8.47 In\u00a0[33]: Copied! <pre>df_anscombe.mean()\n</pre> df_anscombe.mean() Out[33]: <pre>x123    9.000000\ny1      7.500909\ny2      7.500909\ny3      7.500000\nx4      9.000000\ny4      7.500909\ndtype: float64</pre> In\u00a0[34]: Copied! <pre>df_anscombe.std()\n</pre> df_anscombe.std() Out[34]: <pre>x123    3.316625\ny1      2.031568\ny2      2.031657\ny3      2.030424\nx4      3.316625\ny4      2.030579\ndtype: float64</pre> <p>Let's calculate the correlation between the variables.</p> In\u00a0[35]: Copied! <pre>correlation_x123_y1 = df_anscombe['x123'].corr(df_anscombe['y1'])\ncorrelation_x123_y2 = df_anscombe['x123'].corr(df_anscombe['y2'])\ncorrelation_x123_y3 = df_anscombe['x123'].corr(df_anscombe['y3'])\ncorrelation_x4_y4 = df_anscombe['x4'].corr(df_anscombe['y4'])\n\nprint('Correlation between x1 and y1: ', correlation_x123_y1)\nprint('Correlation between x2 and y2: ', correlation_x123_y2)\nprint('Correlation between x3 and y3: ', correlation_x123_y3)\nprint('Correlation between x4 and y4: ', correlation_x4_y4)\n</pre> correlation_x123_y1 = df_anscombe['x123'].corr(df_anscombe['y1']) correlation_x123_y2 = df_anscombe['x123'].corr(df_anscombe['y2']) correlation_x123_y3 = df_anscombe['x123'].corr(df_anscombe['y3']) correlation_x4_y4 = df_anscombe['x4'].corr(df_anscombe['y4'])  print('Correlation between x1 and y1: ', correlation_x123_y1) print('Correlation between x2 and y2: ', correlation_x123_y2) print('Correlation between x3 and y3: ', correlation_x123_y3) print('Correlation between x4 and y4: ', correlation_x4_y4)  <pre>Correlation between x1 and y1:  0.81642051634484\nCorrelation between x2 and y2:  0.8162365060002428\nCorrelation between x3 and y3:  0.8162867394895984\nCorrelation between x4 and y4:  0.8165214368885028\n</pre> <p>Now, it is time to visualize the data.</p> In\u00a0[36]: Copied! <pre>fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(6, 4))\n\n# Panel 1\naxes[0, 0].plot(df_anscombe['x123'], df_anscombe['y1'], 'o')\naxes[0, 0].set_xlabel('x1')\naxes[0, 0].set_ylabel('y1')\naxes[0, 0].set_title('Panel 1')\n\n# Panel 2\naxes[0, 1].plot(df_anscombe['x123'], df_anscombe['y2'], 'o')\naxes[0, 1].set_xlabel('x2')\naxes[0, 1].set_ylabel('y2')\naxes[0, 1].set_title('Panel 2')\n\n# Panel 3\naxes[1, 0].plot(df_anscombe['x123'], df_anscombe['y3'], 'o')\naxes[1, 0].set_xlabel('x3')\naxes[1, 0].set_ylabel('y3')\naxes[1, 0].set_title('Panel 3')\n\n# Panel 4\naxes[1, 1].plot(df_anscombe['x4'], df_anscombe['y4'], 'o')\naxes[1, 1].set_xlabel('x4')\naxes[1, 1].set_ylabel('y4')\naxes[1, 1].set_title('Panel 4')\n\nplt.tight_layout()\nplt.show()\n</pre> fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(6, 4))  # Panel 1 axes[0, 0].plot(df_anscombe['x123'], df_anscombe['y1'], 'o') axes[0, 0].set_xlabel('x1') axes[0, 0].set_ylabel('y1') axes[0, 0].set_title('Panel 1')  # Panel 2 axes[0, 1].plot(df_anscombe['x123'], df_anscombe['y2'], 'o') axes[0, 1].set_xlabel('x2') axes[0, 1].set_ylabel('y2') axes[0, 1].set_title('Panel 2')  # Panel 3 axes[1, 0].plot(df_anscombe['x123'], df_anscombe['y3'], 'o') axes[1, 0].set_xlabel('x3') axes[1, 0].set_ylabel('y3') axes[1, 0].set_title('Panel 3')  # Panel 4 axes[1, 1].plot(df_anscombe['x4'], df_anscombe['y4'], 'o') axes[1, 1].set_xlabel('x4') axes[1, 1].set_ylabel('y4') axes[1, 1].set_title('Panel 4')  plt.tight_layout() plt.show()  <p>See, the datasets are very similar in terms of mean, variance, correlation, and regression line, but they look very different when plotted.</p> In\u00a0[37]: Copied! <pre># for the iris dataset\nsns.set_theme(font_scale=0.75)  # Set the font scale to reduce the font size\nsns.pairplot(df_iris, hue='Species', height=1.5, markers='.', plot_kws={'s': 30}) \nplt.show()\n</pre> # for the iris dataset sns.set_theme(font_scale=0.75)  # Set the font scale to reduce the font size sns.pairplot(df_iris, hue='Species', height=1.5, markers='.', plot_kws={'s': 30})  plt.show() In\u00a0[38]: Copied! <pre># For the cars dataset\ndf_cars_filtered_numeric = df_cars_filtered.select_dtypes(include=np.number)\nsns.set(font_scale=0.75)  # Set the font scale to reduce the font size\nsns.pairplot(df_cars_filtered_numeric, height=1, markers='.', plot_kws={'s': 10})\nplt.title(\"Scatterplot Matrix of Filtered Cars Dataset\")\nplt.show()\n\n# More confusing, right? But we can see patterns already\n</pre> # For the cars dataset df_cars_filtered_numeric = df_cars_filtered.select_dtypes(include=np.number) sns.set(font_scale=0.75)  # Set the font scale to reduce the font size sns.pairplot(df_cars_filtered_numeric, height=1, markers='.', plot_kws={'s': 10}) plt.title(\"Scatterplot Matrix of Filtered Cars Dataset\") plt.show()  # More confusing, right? But we can see patterns already"},{"location":"m05-ml_eda/exploratory_data_analysis/#module-05-exploratory-data-analysis","title":"Module 05 - Exploratory Data Analysis\u00b6","text":"<p>In this module, we look into Exploratory Data Analysis (EDA), a critical first step in any data science project. We will set up our environment by importing essential libraries, loading datasets, and performing preliminary analysis to understand the data\u2019s structure, distribution, and potential issues. This process lays the groundwork for any further modeling or hypothesis testing.</p>"},{"location":"m05-ml_eda/exploratory_data_analysis/#set-up-the-environment","title":"Set up the environment\u00b6","text":"<p>In the following steps, we ensure that our workspace is equipped with the necessary tools for data manipulation, visualization, and statistical analysis.</p>"},{"location":"m05-ml_eda/exploratory_data_analysis/#loading-the-car-features-dataset","title":"Loading the Car Features Dataset\u00b6","text":"<p>In this step, we load a dataset that contains various features of cars. This dataset includes details such as the car's make, model, year, fuel type, and performance metrics like city and highway MPG. Understanding the structure of this dataset will help us in performing meaningful exploratory analysis later on.</p>"},{"location":"m05-ml_eda/exploratory_data_analysis/#data-familiarization","title":"Data Familiarization\u00b6","text":"<p>In this section, we perform an initial exploration of the car dataset.</p> <p>Our goals are to:</p> <ul> <li>Identify data types: Understand what kind of data each column holds (e.g., numerical, categorical).</li> <li>Detect missing values: Find any absent data points that might need handling.</li> <li>Count entries: Check the total number of rows to gauge the dataset's size.</li> <li>Examine ranges: Look at the minimum and maximum values for numerical columns.</li> <li>Sample entries: Display a few rows to visually inspect the data.</li> <li>Review unique values: For categorical variables, list unique values to understand the diversity in the data.</li> </ul> <p>These steps are crucial to ensure data quality before moving forward with deeper analyses.</p>"},{"location":"m05-ml_eda/exploratory_data_analysis/#descriptive-statistics","title":"Descriptive Statistics\u00b6","text":"<p>Here, we compute key statistical metrics that summarize the main features of the dataset:</p> <ul> <li>Mean: The average value, which gives an idea of the central tendency.</li> <li>Standard Deviation: Indicates how much variation exists around the mean.</li> <li>Median: The middle value when the data is sorted, less affected by outliers.</li> <li>Quantiles (e.g., quartiles): Divide the data into intervals, offering insight into the distribution spread.</li> </ul> <p>These statistics provide an overall summary that can be used to compare and contrast different subsets of the data.</p>"},{"location":"m05-ml_eda/exploratory_data_analysis/#statistical-summary-of-the-data","title":"Statistical Summary of the Data\u00b6","text":"<p>In this step, we generate a comprehensive summary table of the dataset that includes the count, mean, standard deviation, and various percentiles for each numeric column. This summary gives a quick, overall snapshot of the distribution and central tendencies within the data.</p>"},{"location":"m05-ml_eda/exploratory_data_analysis/#distributions","title":"Distributions\u00b6","text":"<p>We now turn our attention to the distribution of key variables by visualizing them with histograms. Histograms allow us to see the frequency of data points across different ranges, highlighting the shape of the distribution (e.g., skewness, modality) and potential anomalies or outliers. In the following example, we visualize the distribution of 'city mpg'.</p>"},{"location":"m05-ml_eda/exploratory_data_analysis/#investigating-outliers-in-the-cars-dataset","title":"Investigating Outliers in the Cars Dataset\u00b6","text":"<p>Now let's return to our cars dataset to identify potential outliers. Outliers can significantly influence statistical analyses. Here, we first visualize the distribution of the 'city mpg' variable, then apply a z-score normalization to highlight data points that lie far from the mean. This helps us in detecting and potentially handling extreme values in our dataset.</p>"},{"location":"m05-ml_eda/exploratory_data_analysis/#correlation-analysis-multivariate-exploration","title":"Correlation Analysis (Multivariate Exploration)\u00b6","text":"<p>In this section, we examine the relationships between numeric variables within the filtered cars dataset. By calculating correlation coefficients, we can quantify how strongly variables are related to one another.</p> <p>We will:</p> <ul> <li>Compute the correlation between specific variables (e.g., city mpg vs. highway MPG).</li> <li>Generate a correlation matrix for all numeric variables.</li> <li>Visualize the correlation matrix using heatmaps for both Pearson and Spearman methods.</li> </ul>"},{"location":"m05-ml_eda/exploratory_data_analysis/#visualizing-some-trends-in-the-car-dataset","title":"Visualizing some trends in the car dataset\u00b6","text":"<p>In this section, we will explore some trends in the car dataset by visualizing the relationship between different variables.</p>"},{"location":"m05-ml_eda/exploratory_data_analysis/#exploring-a-different-dataset-the-iris-dataset","title":"Exploring a Different Dataset: The Iris Dataset\u00b6","text":"<p>Let's try a different dataset: the Iris dataset, which is widely used in machine learning and statistics. This dataset contains measurements of different Iris flower species, including:</p> <ul> <li>Sepal Length and Width</li> <li>Petal Length and Width</li> </ul> <p>By visualizing the histograms of these measurements, we can observe differences in distribution among the species and gain insights into the inherent patterns of this classic dataset.</p> <p>Let's try a different dataset (Iris dataset). This one contains measurements of different species of Iris flowers.</p>"},{"location":"m05-ml_eda/exploratory_data_analysis/#analyzing-petal-length-by-species","title":"Analyzing Petal Length by Species\u00b6","text":"<p>Here, we take a closer look at the 'PetalLengthCm' feature in the Iris dataset by plotting its distribution for each species. This comparison helps reveal how petal length varies across different types of Iris flowers and can be an indicator for species classification or further statistical analysis.</p>"},{"location":"m05-ml_eda/exploratory_data_analysis/#analyzing-social-media-data-tweets-on-the-ukraine-russia-conflict","title":"Analyzing Social Media Data: Tweets on the Ukraine-Russia Conflict\u00b6","text":"<p>Now let's see some social media data. Tweets about the Ukraine-Russia conflict.</p> <p>We load the dataset, define data types for specific columns, and display a sample of the data. The analysis will include:</p> <ul> <li>Checking for missing values in the dataset.</li> <li>Visualizing the distribution of the 'RetweetCount' field.</li> </ul> <p>This helps in understanding the dynamics and reach of tweets regarding this ongoing conflict.</p>"},{"location":"m05-ml_eda/exploratory_data_analysis/#investigating-the-anscombe-quartet-data","title":"Investigating the Anscombe Quartet data\u00b6","text":"<p>The Anscombe's quartet is a set of four datasets that have nearly identical simple descriptive statistics, yet appear very different when graphed. This example illustrates the importance of visualizing data before drawing conclusions.</p> <p>Let's start loading it...</p>"},{"location":"m05-ml_eda/exploratory_data_analysis/#scatterplot-matrix","title":"Scatterplot Matrix\u00b6","text":"<p>A scatterplot matrix is a grid of scatterplots that allows us to visualize the pairwise relationships between multiple variables in a dataset. It is a powerful tool for identifying correlations, trends, and potential outliers across different dimensions of the data. In this example, we will create a scatterplot matrix for the cars dataset.</p> <p>Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics. Seaborn works well with pandas data structures and integrates closely with matplotlib. With seaborn it is easy to create scatterplot matrices.</p> <p>For more examples check out the seaborn documentation: https://seaborn.pydata.org/examples/scatterplot_matrix.html</p>"},{"location":"m06-regression/assignment_m06_regression/","title":"Homework 6 - Regression","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport sqlite3\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score\n</pre> import pandas as pd import numpy as np import sqlite3 from sklearn.model_selection import train_test_split, KFold from sklearn.linear_model import LinearRegression, Ridge, Lasso from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.metrics import r2_score <p>We're going to use the soccer data to run regressions. In the cell below, connect to the database.</p> In\u00a0[\u00a0]: Copied! <pre># Input Code Here\ndataset_path = \"../../Datasets/fifa_soccer_dataset.sqlite\" # Fix your path accordingly\n\nconn = sqlite3.connect(dataset_path)\n</pre> # Input Code Here dataset_path = \"../../Datasets/fifa_soccer_dataset.sqlite\" # Fix your path accordingly  conn = sqlite3.connect(dataset_path)  <p>To get started, let's write a query to grab all of the entries from the <code>Player_Attributes</code> table, and print the first 5 rows below.</p> In\u00a0[\u00a0]: Copied! <pre>player_attr_df = pd.read_sql(\"Your Query Here\", _) # Input Code Here\n\n# Display the first 5 rows of the table\n</pre> player_attr_df = pd.read_sql(\"Your Query Here\", _) # Input Code Here  # Display the first 5 rows of the table <p>We are going to play with two fields today, the <code>gk_handling</code> field as the dependent feature and the <code>gk_reflexes</code> field as the independent feature. Let's drop some missing values from these two columns as well. They represent the goalkeeping handling and reflexes of a player respectively.</p> In\u00a0[\u00a0]: Copied! <pre># Input Code Here - Drop null values from the mentioned columns\n</pre> # Input Code Here - Drop null values from the mentioned columns <p>Let's store those columns in their own variables for easy reading.</p> In\u00a0[\u00a0]: Copied! <pre>x = player_attr_df[['Column Here']].values # Input Code Here\ny = player_attr_df[['Column Here']].values # Input Code Here\n</pre> x = player_attr_df[['Column Here']].values # Input Code Here y = player_attr_df[['Column Here']].values # Input Code Here <p>To preform and evaluate our linear regression, we need to split our data into test and training batches. We can do this by using the <code>train_test_split()</code> function. In the cell below, use this function and pass it <code>x</code> and <code>y</code> as the data for it to split. The final parameter <code>test_size</code> indicates how big the test batch should be, in this case 30% of the initial dataset inputted.</p> In\u00a0[\u00a0]: Copied! <pre>X_train, X_test, Y_train, Y_test = _(_, _, test_size = 0.3) #Input Code here\n</pre> X_train, X_test, Y_train, Y_test = _(_, _, test_size = 0.3) #Input Code here <p>We can now preform the fitting. Let's call the <code>fit()</code> function on our <code>lm</code> variable, passing the <code>X_train</code> and <code>Y_train</code> data as parameters.</p> In\u00a0[\u00a0]: Copied! <pre>lm = LinearRegression()\nlm._(_, _) # Input Code Here\n</pre> lm = LinearRegression() lm._(_, _) # Input Code Here <p>Great! Now we can use the predict funtion to see how the model preforms against our test data set. Call the <code>predict()</code> function on <code>lm</code> and pass <code>X_test</code> as our input parameter. We'll then see the r2 score to see how correlated these values are.</p> In\u00a0[\u00a0]: Copied! <pre>Y_predicted = lm._(_) # Input Code Here\nrsquared = r2_score(_, _)\nprint(\"R2 Score: \" + str(rsquared))\n</pre> Y_predicted = lm._(_) # Input Code Here rsquared = r2_score(_, _) print(\"R2 Score: \" + str(rsquared)) <p>These values are pretty correlated! We can also use the <code>StandarScalar()</code> to transform our values before fitting our model. In the cell below, call the <code>StandardScalar()</code> function and pass <code>x</code> and <code>y</code> to the <code>fit_transform()</code> functions.</p> In\u00a0[\u00a0]: Copied! <pre>sc = _() # Input Code Here\n\nx_scaled = sc.fit_transform(_) # Input Code Here\ny_scaled = sc.fit_transform(_) # Input Code Here\n</pre> sc = _() # Input Code Here  x_scaled = sc.fit_transform(_) # Input Code Here y_scaled = sc.fit_transform(_) # Input Code Here <p>Now we can run the model again as we did before. We'll need to split the training and test batches again, then run a new <code>fit()</code>. Once fitted, we can again use <code>predict()</code> and run a r2 score again.</p> In\u00a0[\u00a0]: Copied! <pre># Input Code Here to split the scaled dataset\n</pre> # Input Code Here to split the scaled dataset In\u00a0[\u00a0]: Copied! <pre># Input Code Here to create and fit the model\n</pre> # Input Code Here to create and fit the model In\u00a0[\u00a0]: Copied! <pre>Y_predicted = lm._(X_test) # Input Code Here\n# Input Code Here - Grab the R2 Score like we did above\nprint(\"R2 Score: \" + str(rsquared))\n</pre> Y_predicted = lm._(X_test) # Input Code Here # Input Code Here - Grab the R2 Score like we did above print(\"R2 Score: \" + str(rsquared)) <p>Implementing various models - LinearRegression(), Ridge(), Lasso() along with K-Fold CrossValidation with 5 splits. Use the unscaled data for this step.</p> In\u00a0[\u00a0]: Copied! <pre># Apply Linear regression, ridge regularization, lasso regularization with cross validation\n\n# Define models\nmodel_lr = _\nmodel_ridge = _\nmodel_lasso = _\n\n# Cross validation\nkf = KFold(n_splits = _)\nlist_r2_score = [] # to keep the r2 score\n# Split the train set:\nfor train_index, test_index in kf.split(_):\n    X_train, X_test = _[train_index], _[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    k_fold_r2 = []\n    for model in [_, _, _]:\n        model._(X_train, y_train)\n        pred = model.predict(_)\n        k_fold_r2.append(r2_score(_, _))\n    list_r2_score.append(k_fold_r2)\n    \n# Show the result - Add Mean and Standard Deviation of the R2-scores\nlist_r2_score.append(list(np.mean(list_r2_score, axis = _)))\nlist_r2_score.append(list(np.std(list_r2_score[:-1], axis = _)))\nresult_r2 = pd.DataFrame(list_r2_score)\nresult_r2.columns = ['Linear Regression', 'Ridge', 'Lasso']\nresult_r2.index = ['k1', 'k2', 'k3', 'k4', 'k5', 'average', 'std']\n\nprint('The result of r2 scores for k=5 cross validation')\ndisplay(result_r2)\n</pre> # Apply Linear regression, ridge regularization, lasso regularization with cross validation  # Define models model_lr = _ model_ridge = _ model_lasso = _  # Cross validation kf = KFold(n_splits = _) list_r2_score = [] # to keep the r2 score # Split the train set: for train_index, test_index in kf.split(_):     X_train, X_test = _[train_index], _[test_index]     y_train, y_test = y[train_index], y[test_index]     k_fold_r2 = []     for model in [_, _, _]:         model._(X_train, y_train)         pred = model.predict(_)         k_fold_r2.append(r2_score(_, _))     list_r2_score.append(k_fold_r2)      # Show the result - Add Mean and Standard Deviation of the R2-scores list_r2_score.append(list(np.mean(list_r2_score, axis = _))) list_r2_score.append(list(np.std(list_r2_score[:-1], axis = _))) result_r2 = pd.DataFrame(list_r2_score) result_r2.columns = ['Linear Regression', 'Ridge', 'Lasso'] result_r2.index = ['k1', 'k2', 'k3', 'k4', 'k5', 'average', 'std']  print('The result of r2 scores for k=5 cross validation') display(result_r2) <p>And that's basic linear regression with python. Please turn in this notebook completed with your outputs displayed in html or pdf formats.</p>"},{"location":"m06-regression/assignment_m06_regression/#homework-6-regression","title":"Homework 6 - Regression\u00b6","text":"<p>In this guide, we will be exploring using regression as an intro to artificial intelligence. For this week's assignment, we will be exploring linear regression. We'll be using the data from our soccer database from assignment 4.</p>"},{"location":"m06-regression/assignment_m06_regression/#instructions","title":"Instructions\u00b6","text":"<ol> <li>Follow the instructions on how to setup your Python and Jupyter (or VSCode) environment and cloning or downloading our repository. Instructions can be found in the class notes.</li> <li>Import soccer database using pandas.</li> <li>Load the values from the attributes <code>gk_reflexes</code> and <code>gk_handling</code> from table <code>Player_Attributes</code>.</li> <li>Use <code>gk_reflexes</code> (as x) and <code>gk_handling</code> (as y) as your data.</li> <li>Drop the missing values from these two columns.</li> <li>Scale the dataset using a standard scaler.</li> <li>Split the data into training and testing in a 0.3 ratio (70% training, 30% testing).</li> <li>Apply Linear Regression, Cross-Validation (with 5 splits), Ridge Regularization, and Lasso Regularizations and print the co-relation result of each technique using <code>r2_score</code>. All of the functions for this last step are located in sklearn.</li> <li>Answer the questions in the notebook through code.</li> <li>Run the notebook and make sure everything works.</li> <li>Export the notebook as HTML or PDF.</li> <li>Submit the notebook through Canvas.</li> </ol> <p>Remember to fill the missing pieces of code in the provided notebook.</p>"},{"location":"m06-regression/assignment_m06_regression/#dataset-overview","title":"Dataset Overview\u00b6","text":"<p>The dataset covers information about soccer players in sqlite format. This file is located in the <code>Datasets</code> directory of this repository. The file is called <code>fifa_soccer_dataset.sqlite.gz</code>. This is the same file from the previous homework (assignment 4).</p> <p>If you haven't decompressed the file, you may need to follow the instructions below to decompress it.</p> <p>IMPORTANT The database is compressed and needs to be decompressed before use. You can do this by running the following command in your terminal on Linux or MacOS:</p> <pre>gunzip Datasets/fifa_soccer_dataset.sqlite.gz\n</pre> <p>If you are using Windows, you can use the following command in your powershell:</p> <pre>$sourceFile = \"$PWD\\Datasets\\fifa_soccer_dataset.sqlite.gz\"\n$destinationFile = \"$PWD\\Datasets\\fifa_soccer_dataset.sqlite\"\n\n$inputStream = [System.IO.File]::OpenRead($sourceFile)\n$outputStream = [System.IO.File]::Create($destinationFile)\n$gzipStream = New-Object System.IO.Compression.GzipStream($inputStream, [System.IO.Compression.CompressionMode]::Decompress)\n$gzipStream.CopyTo($outputStream)\n\n$gzipStream.Close()\n$outputStream.Close()\n$inputStream.Close()\n</pre> <p>Alternatively, you can extract the file using the GUI of your operating system.</p>"},{"location":"m06-regression/assignment_m06_regression/#submission-guidelines","title":"Submission Guidelines\u00b6","text":"<ul> <li>Submit your completed notebook as a HTML export, or a PDF file.</li> </ul> <p>To export to HTML, if you are on Jupyter, select <code>File</code> &gt; <code>Export Notebook As</code> &gt; <code>HTML</code>.</p> <p>If you are on VSCode, you can use the <code>Jupyter: Export to HTML</code> command.</p> <ul> <li>Open the command palette (Ctrl+Shift+P or Cmd+Shift+P on Mac).<ul> <li>Search for <code>Jupyter: Export to HTML</code>.</li> <li>Save the HTML file to your computer and submit it via Canvas.</li> </ul> </li> </ul> <p>To begin, we'll need quite a few imports.</p>"},{"location":"m07-classification_clustering/assignment_m07_classification/","title":"Homework 7 - Classification","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport sqlite3\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n</pre> import pandas as pd import sqlite3 from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score from sklearn.svm import SVC from sklearn.neighbors import KNeighborsClassifier <p>To start this assignment, we first need to connect to the sqlite database, do so below.</p> In\u00a0[\u00a0]: Copied! <pre># Input Code Here\ndataset_path = \"../../Datasets/fifa_soccer_dataset.sqlite\" # Fix your path accordingly\n\n# Your Code Here\n</pre> # Input Code Here dataset_path = \"../../Datasets/fifa_soccer_dataset.sqlite\" # Fix your path accordingly  # Your Code Here <p>Now connected, let's grab required attributes for the scenario from the <code>Player_Attributes</code>(Using gk_reflexes and gk_kicking) table.</p> In\u00a0[\u00a0]: Copied! <pre>player_attr_df = # Your Code Here\nplayer_attr_df.head()\n</pre> player_attr_df = # Your Code Here player_attr_df.head() <p>Droping the rows with are having missing values</p> In\u00a0[\u00a0]: Copied! <pre> # Your Code Here\n</pre>  # Your Code Here <p>For this classifying, we'll be using the <code> gk_reflexes</code> and <code>gk_kicking</code>. Pull these values into <code>x</code> and <code>y</code>.</p> In\u00a0[\u00a0]: Copied! <pre>x = player_attr_df[['']].values # Your Code Here\ny = player_attr_df[['']].values # Your Code Here\n</pre> x = player_attr_df[['']].values # Your Code Here y = player_attr_df[['']].values # Your Code Here <p>the target variable should be reduced to just 5 classes.</p> In\u00a0[\u00a0]: Copied! <pre>____ =  pd.cut(y, bins=5, labels=['Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5'])\n</pre> ____ =  pd.cut(y, bins=5, labels=['Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5']) <p>Let's split the data set into test and training sets using the <code>train_test_split()</code> function. We'll want to transform our <code>x</code> variable, which can be done by calling the <code>transform()</code> function.</p> In\u00a0[\u00a0]: Copied! <pre>X_train, X_test, y_train, y_test=  # Your Code Here\nsc= StandardScaler()\nsc.fit(X_train)\nX_train_std= sc._(X_train) # Your Code Here\nX_test_std= sc._(X_test) # Your Code Here\n</pre> X_train, X_test, y_train, y_test=  # Your Code Here sc= StandardScaler() sc.fit(X_train) X_train_std= sc._(X_train) # Your Code Here X_test_std= sc._(X_test) # Your Code Here <p>To preform a logistic regression, we'll use the <code>LogisticRegression()</code> function. This may take a couple moments to run.</p> In\u00a0[\u00a0]: Copied! <pre>lr= _(C=1000.0, random_state=0,max_iter=1000) #Your Code Here\nlr.fit(X_train_std, y_train.ravel())\ny_pred= lr.predict(X_test_std)\n\nprint(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n</pre> lr= _(C=1000.0, random_state=0,max_iter=1000) #Your Code Here lr.fit(X_train_std, y_train.ravel()) y_pred= lr.predict(X_test_std)  print(f'Accuracy: {accuracy_score(y_test, y_pred)}') <p>Great! Let's try applying SVM instead. Try using <code>SVC()</code> below, then use the same prediction and output methods as the above cell.</p> In\u00a0[\u00a0]: Copied! <pre>svm= _(kernel='linear', C=1.0, random_state=0, cache_size=7000) # Your Code Here\nsvm.fit(X_train_std, y_train.ravel())\ny_pred = # Your Prediction Code Here\n\nprint(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n</pre> svm= _(kernel='linear', C=1.0, random_state=0, cache_size=7000) # Your Code Here svm.fit(X_train_std, y_train.ravel()) y_pred = # Your Prediction Code Here  print(f'Accuracy: {accuracy_score(y_test, y_pred)}') <p>Let's try using a KNeightbors Classifier. We can call the <code>KNeighborsClassifier()</code> function, and supply 2 parameters: <code>n_neighbors=5</code> and <code>matric='euclidean</code>. Once you run this method, display the accuracy of your model as you did in the above cells.</p> In\u00a0[\u00a0]: Copied! <pre>knn= # Your Code here\n\n#Your Code to fit the model here\n\ny_pred= # Your Prediction Code Here\n\n# Your Accuracy Output Code Here\n</pre> knn= # Your Code here  #Your Code to fit the model here  y_pred= # Your Prediction Code Here  # Your Accuracy Output Code Here <p>let's repeat the above steps agian with gk_kicking and gk_handling.</p> In\u00a0[\u00a0]: Copied! <pre> # Your Code Here\n</pre>  # Your Code Here <p>Lastly, in the cell below, answer the question: Since this assignment (Classification) and the previous assignment (Regression) are with the same data, can you compare and conclude which technique is yielding best results?</p> In\u00a0[\u00a0]: Copied! <pre>#Your Answer Here\n</pre> #Your Answer Here"},{"location":"m07-classification_clustering/assignment_m07_classification/#homework-7-classification","title":"Homework 7 - Classification\u00b6","text":"<p>In this assignment, we will be applying some basic classification methods to the soccer database (found on canvas). We will first need to import all the libraries required for this guide.</p>"},{"location":"m07-classification_clustering/assignment_m07_classification/#instructions","title":"Instructions\u00b6","text":"<p>In this assignment, you will be performing the specified classification methods in Python.</p>"},{"location":"m07-classification_clustering/assignment_m07_classification/#step-1-load-data","title":"Step 1: Load Data\u00b6","text":"<ul> <li>Load the following attributes from <code>Player_Attributes</code>:<ul> <li><code>gk_reflexes</code></li> <li><code>gk_kicking</code></li> <li><code>gk_handling</code></li> </ul> </li> </ul> <p>These values will be used for classification.</p>"},{"location":"m07-classification_clustering/assignment_m07_classification/#step-2-classification-part-1","title":"Step 2: Classification (Part 1)\u00b6","text":"<ul> <li>Use <code>gk_reflexes</code> and <code>gk_kicking</code>.</li> <li>Choose one of the attributes as the target attribute.</li> <li>Generate five classes in the target property by reducing the range of values in the target data.</li> <li>Split the data into training and testing sets.</li> <li>Apply the following methods and print the resulting <code>accuracy_score</code> from <code>sklearn.metrics</code>:<ul> <li>Logistic Regression</li> <li>Support Vector Machine (SVM)</li> <li>Decision Tree</li> <li>K-Nearest Neighbors (KNN)</li> </ul> </li> </ul>"},{"location":"m07-classification_clustering/assignment_m07_classification/#step-3-classification-part-2","title":"Step 3: Classification (Part 2)\u00b6","text":"<ul> <li><p>Repeat Step 2, this time using:</p> <ul> <li><code>gk_kicking</code> and <code>gk_handling</code></li> </ul> </li> <li><p>Again, print the corresponding <code>accuracy_score</code> for each classification method.</p> </li> </ul>"},{"location":"m07-classification_clustering/assignment_m07_classification/#step-4-analysis-comment-in-python-file","title":"Step 4: Analysis (Comment in Python file)\u00b6","text":"<p>Answer the following question as a comment in your Python file:</p> <p>Since this assignment (Classification) and the previous assignment (Regression) are with the same data, can you compare and conclude which technique is yielding best results?</p>"},{"location":"m07-classification_clustering/assignment_m07_classification/#dataset-overview","title":"Dataset Overview\u00b6","text":"<p>The dataset covers information about soccer players in sqlite format. This file is located in the <code>Datasets</code> directory of this repository. The file is called <code>fifa_soccer_dataset.sqlite.gz</code>. This is the same file from the previous homework (assignment 4).</p> <p>If you haven't decompressed the file, you may need to follow the instructions below to decompress it.</p> <p>IMPORTANT The database is compressed and needs to be decompressed before use. You can do this by running the following command in your terminal on Linux or MacOS:</p> <pre>gunzip Datasets/fifa_soccer_dataset.sqlite.gz\n</pre> <p>If you are using Windows, you can use the following command in your powershell:</p> <pre>$sourceFile = \"$PWD\\Datasets\\fifa_soccer_dataset.sqlite.gz\"\n$destinationFile = \"$PWD\\Datasets\\fifa_soccer_dataset.sqlite\"\n\n$inputStream = [System.IO.File]::OpenRead($sourceFile)\n$outputStream = [System.IO.File]::Create($destinationFile)\n$gzipStream = New-Object System.IO.Compression.GzipStream($inputStream, [System.IO.Compression.CompressionMode]::Decompress)\n$gzipStream.CopyTo($outputStream)\n\n$gzipStream.Close()\n$outputStream.Close()\n$inputStream.Close()\n</pre> <p>Alternatively, you can extract the file using the GUI of your operating system.</p>"},{"location":"m07-classification_clustering/assignment_m07_classification/#submission-guidelines","title":"Submission Guidelines\u00b6","text":"<ul> <li>Submit your completed notebook as a HTML export, or a PDF file.</li> </ul> <p>To export to HTML, if you are on Jupyter, select <code>File</code> &gt; <code>Export Notebook As</code> &gt; <code>HTML</code>.</p> <p>If you are on VSCode, you can use the <code>Jupyter: Export to HTML</code> command.</p> <ul> <li>Open the command palette (Ctrl+Shift+P or Cmd+Shift+P on Mac).<ul> <li>Search for <code>Jupyter: Export to HTML</code>.</li> <li>Save the HTML file to your computer and submit it via Canvas.</li> </ul> </li> </ul>"},{"location":"m08-model_evaluation/assignment_m08_clustering/","title":"Homework 8 - Clustering","text":"In\u00a0[1]: Copied! <pre>import sqlite3\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n</pre> import sqlite3 import pandas as pd from sklearn.cluster import KMeans import matplotlib.pyplot as plt In\u00a0[33]: Copied! <pre>dataset_path = \"../../Datasets/fifa_soccer_dataset.sqlite\" # Fix your path accordingly\n\n# Your Code Here\n</pre> dataset_path = \"../../Datasets/fifa_soccer_dataset.sqlite\" # Fix your path accordingly  # Your Code Here <p>First, let's connect to the database as we've done in previous guides below.</p> In\u00a0[76]: Copied! <pre>conn = sqlite3.connect(dataset_path)\n</pre> conn = sqlite3.connect(dataset_path) <p>From the database, grab <code>gk_reflexes</code> and <code>gk_kicking</code> from the <code>Player_Attributes</code> table.</p> In\u00a0[\u00a0]: Copied! <pre># get only `gk_reflexes` and `gk_kicking\nplayer_attr_df = ...\n</pre> # get only `gk_reflexes` and `gk_kicking player_attr_df = ... <p>Now, let's drop the missing values.</p> In\u00a0[93]: Copied! <pre># drop the null values\nplayer_attr_df = player_attr_df.dropna()\nplayer_attr_df.head()\n</pre> # drop the null values player_attr_df = player_attr_df.dropna() player_attr_df.head()  Out[93]: gk_reflexes gk_kicking 0 8.0 10.0 1 8.0 10.0 2 8.0 10.0 3 7.0 9.0 4 7.0 9.0 <p>Let's run the <code>KMeans()</code> function, using <code>n_clusers = 5</code> as our starting cluster count below.</p> In\u00a0[94]: Copied! <pre>km = KMeans(n_clusters = 5, init = 'random', n_init = 10, max_iter= 300, tol=1e-04, random_state=1)\nykm = km.fit_predict(player_attr_df)\n</pre> km = KMeans(n_clusters = 5, init = 'random', n_init = 10, max_iter= 300, tol=1e-04, random_state=1) ykm = km.fit_predict(player_attr_df) <p>Plot the results using the <code>scatter()</code> function, passing <code>player_attr_df.gk_reflexes</code> and <code>player_attr_df.gk_kicking</code> as the first two arguments.</p> In\u00a0[\u00a0]: Copied! <pre>plt.scatter(_, _, c=ykm, marker='o', s = 50, cmap='tab10')\ncenters = km.cluster_centers_\nplt.scatter(centers[:,0], centers[:,1], c ='black', s = 200)\n\nplt.grid()\nplt.tight_layout()\nplt.show()\n</pre> plt.scatter(_, _, c=ykm, marker='o', s = 50, cmap='tab10') centers = km.cluster_centers_ plt.scatter(centers[:,0], centers[:,1], c ='black', s = 200)  plt.grid() plt.tight_layout() plt.show() <p>Above we can see the 5 centroids from the clustering. It looks like we can probably choose a better cluster number. Let's run the elbow method below to find a better cluster count. Once done, plot the distance in the <code>plt.plot()</code> so we can see which value to use.</p> In\u00a0[\u00a0]: Copied! <pre>dist= []\nfor i in range(1, 11):\n    km = KMeans(n_clusters = i, init = 'k-means++', n_init = 10, max_iter= 300, random_state=0)\n    km.fit(player_attr_df)\n    dist.append(km.inertia_)\n\n# Pass dist where the parameter underscore is below\nplt._(range(1,11), _, marker = 'o')\nplt.tight_layout()\nplt.show()\n</pre> dist= [] for i in range(1, 11):     km = KMeans(n_clusters = i, init = 'k-means++', n_init = 10, max_iter= 300, random_state=0)     km.fit(player_attr_df)     dist.append(km.inertia_)  # Pass dist where the parameter underscore is below plt._(range(1,11), _, marker = 'o') plt.tight_layout() plt.show() <p>Looks like we should use 2! In the cell below, run <code>KMeans()</code> again, same as previously in this guide, and use 2 clusters.</p> In\u00a0[\u00a0]: Copied! <pre>km = # Run KMeans again here\nykm = # Fit Predict Here\n</pre> km = # Run KMeans again here ykm = # Fit Predict Here <p>Lastly, plot your results.</p> In\u00a0[1]: Copied! <pre># Plot as we did before\n</pre> # Plot as we did before"},{"location":"m08-model_evaluation/assignment_m08_clustering/#homework-8-clustering","title":"Homework 8 - Clustering\u00b6","text":"<p>In this assignment, you will be preforming the specified clustering methods in python</p>"},{"location":"m08-model_evaluation/assignment_m08_clustering/#step-1-load-data","title":"Step 1: Load Data\u00b6","text":"<ul> <li>Load the following attributes from <code>Player_Attributes</code>:<ul> <li><code>gk_reflexes</code></li> <li><code>gk_kicking</code></li> </ul> </li> </ul> <p>These values will be used for clustering.</p>"},{"location":"m08-model_evaluation/assignment_m08_clustering/#step-2-clustering","title":"Step 2: Clustering\u00b6","text":"<ul> <li>Apply Kmeans  (5 clusters), plot the results</li> <li>Use the Elbow technique, and report the optimal K value</li> <li>Run Kmeans again, with the k value from the previous step. Plot your results.</li> </ul>"},{"location":"m08-model_evaluation/assignment_m08_clustering/#dataset-overview","title":"Dataset Overview\u00b6","text":"<p>The dataset covers information about soccer players in sqlite format. This file is located in the <code>Datasets</code> directory of this repository. The file is called <code>fifa_soccer_dataset.sqlite.gz</code>. This is the same file from the previous homework (assignment 4).</p> <p>If you haven't decompressed the file, you may need to follow the instructions below to decompress it.</p> <p>IMPORTANT The database is compressed and needs to be decompressed before use. You can do this by running the following command in your terminal on Linux or MacOS:</p> <pre>gunzip Datasets/fifa_soccer_dataset.sqlite.gz\n</pre> <p>If you are using Windows, you can use the following command in your powershell:</p> <pre>$sourceFile = \"$PWD\\Datasets\\fifa_soccer_dataset.sqlite.gz\"\n$destinationFile = \"$PWD\\Datasets\\fifa_soccer_dataset.sqlite\"\n\n$inputStream = [System.IO.File]::OpenRead($sourceFile)\n$outputStream = [System.IO.File]::Create($destinationFile)\n$gzipStream = New-Object System.IO.Compression.GzipStream($inputStream, [System.IO.Compression.CompressionMode]::Decompress)\n$gzipStream.CopyTo($outputStream)\n\n$gzipStream.Close()\n$outputStream.Close()\n$inputStream.Close()\n</pre> <p>Alternatively, you can extract the file using the GUI of your operating system.</p>"},{"location":"m08-model_evaluation/assignment_m08_clustering/#submission-guidelines","title":"Submission Guidelines\u00b6","text":"<ul> <li>Submit your completed notebook as a HTML export, or a PDF file.</li> </ul> <p>To export to HTML, if you are on Jupyter, select <code>File</code> &gt; <code>Export Notebook As</code> &gt; <code>HTML</code>.</p> <p>If you are on VSCode, you can use the <code>Jupyter: Export to HTML</code> command.</p> <ul> <li>Open the command palette (Ctrl+Shift+P or Cmd+Shift+P on Mac).<ul> <li>Search for <code>Jupyter: Export to HTML</code>.</li> <li>Save the HTML file to your computer and submit it via Canvas.</li> </ul> </li> </ul>"},{"location":"m10-text/assignment_m10_sentiment_analysis/","title":"Homework 10 - Sentiment Analysis with TF-IDF Vectorization","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport string\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nimport nltk\nimport re\n%matplotlib inline\n</pre> import pandas as pd import numpy as np import string import matplotlib.pyplot as plt  from sklearn.feature_extraction.text import TfidfVectorizer from nltk.corpus import stopwords import nltk import re %matplotlib inline <p>The datasets have two columns: <code>sentence</code> and <code>score</code>. The <code>sentence</code> column contains the text of the sentence, and the <code>score</code> column contains the sentiment label (1 for positive, 0 for negative).</p> In\u00a0[4]: Copied! <pre>df = pd.read_csv(\"../../Datasets/yelp_labelled.tsv\", sep=\"\\t\")\ndf.head()\n</pre> df = pd.read_csv(\"../../Datasets/yelp_labelled.tsv\", sep=\"\\t\") df.head() Out[4]: sentence score 0 Wow... Loved this place. 1 1 Crust is not good. 0 2 Not tasty and the texture was just nasty. 0 3 Stopped by during the late May bank holiday of... 1 4 The selection on the menu was great and so wer... 1 <p>Great! Now we need to clean up the dataframe by removing non words like stop-test, and punctuation. Fill in the code for the <code>remove_punctuation()</code> and <code>remove_stopwords()</code> functions as described in lecture.</p> <p>Note: In addition to the 'remove_punctuation' and 'remove_stopwords', you can also try to check for lower case and upper case and convert to lower case accordingly. You can also tokenize the text, stemming the tokens and then join the stemmed tokens back into a string.</p> In\u00a0[\u00a0]: Copied! <pre>nltk.download('stopwords')\nstop = stopwords.words('english')\n\ndef remove_punctuation(text):\n    # Your Code Here\n\ndef remove_stopwords(text):\n    # Your Code Here\n\ndf['sentence'] = df['sentence'].apply(remove_punctuation).apply(remove_stopwords)\ndf.head()\n</pre> nltk.download('stopwords') stop = stopwords.words('english')  def remove_punctuation(text):     # Your Code Here  def remove_stopwords(text):     # Your Code Here  df['sentence'] = df['sentence'].apply(remove_punctuation).apply(remove_stopwords) df.head() <p>Split the cleaned dataset using train test split</p> In\u00a0[\u00a0]: Copied! <pre># Your code here\n</pre> # Your code here <p>Define a pipeline with the asked models(tfifd and Logistic Regression) in our case</p> <p>Next we can call the <code>TfidfVectorizer()</code> function, passing it 'english' as a parameter.</p> In\u00a0[\u00a0]: Copied! <pre>pipe = Pipeline((), () ) # Your code here\n</pre> pipe = Pipeline((), () ) # Your code here <p>Next, you can define a parameter grid for finding the best hyperparameters, then use GridsearchCV and pass the pipeline to find the best parameters and then fit the model using the best hyperparameters</p> In\u00a0[\u00a0]: Copied! <pre>param_grid = { } # Your code here\n</pre> param_grid = { } # Your code here In\u00a0[\u00a0]: Copied! <pre># Your code for GridsearchCV\n</pre> # Your code for GridsearchCV In\u00a0[\u00a0]: Copied! <pre># Your code to fit the model\n</pre> # Your code to fit the model <p>Write a code to print the best_params_, best_score_, score.</p> In\u00a0[1]: Copied! <pre># Your code here\n</pre> # Your code here"},{"location":"m10-text/assignment_m10_sentiment_analysis/#homework-10-sentiment-analysis-with-tf-idf-vectorization","title":"Homework 10 - Sentiment Analysis with TF-IDF Vectorization\u00b6","text":"<p>In this assignment, we will apply NLP concepts from lecture and use TF-IDF Vectorization. We will need to use the sentiment dataset linked to from the canvas assignment page. Make sure to have this downloaded for using this guide. As always, we'll first need a few libraries for this assignment.</p> <p>Complete the missing parts in this guide.</p>"},{"location":"m10-text/assignment_m10_sentiment_analysis/#step-1-load-data","title":"Step 1: Load Data\u00b6","text":"<p>You can load the data from the provided TSV file using <code>pandas</code>.</p>"},{"location":"m10-text/assignment_m10_sentiment_analysis/#step-2-preprocess","title":"Step 2: Preprocess\u00b6","text":"<ul> <li>Clean the data by removing stop-words, punctuations, emoticons etc..</li> </ul>"},{"location":"m10-text/assignment_m10_sentiment_analysis/#step-3-train-and-test-a-model-to-predict-the-sentiment-of-each-sentence","title":"Step 3: Train and test a model to predict the sentiment of each sentence\u00b6","text":"<ul> <li>Train and test the model using TfidfVectorizer, Pipeline, Logistic regression with this data.</li> <li>Print the best_params_, best_score_, score.</li> </ul>"},{"location":"m10-text/assignment_m10_sentiment_analysis/#step-4-repeat-for-all-the-datasets","title":"Step 4: Repeat for all the datasets\u00b6","text":"<ul> <li>'amazon_cells_labelled.tsv'</li> <li>'yelp_labelled.tsv'</li> <li>'imdb_labelled.tsv'</li> </ul>"},{"location":"m10-text/assignment_m10_sentiment_analysis/#dataset-overview","title":"Dataset Overview\u00b6","text":"<p>The dataset obtained originally from https://archive.ics.uci.edu/dataset/331/sentiment+labelled+sentences contains sentences labeled with sentiment. Each sentence is associated with a sentiment label (positive or negative). The dataset is split into three parts, each containing sentences from different sources: Amazon, Yelp, and IMDb. Score is either 1 (for positive) or 0 (for negative)</p>"},{"location":"m10-text/assignment_m10_sentiment_analysis/#submission-guidelines","title":"Submission Guidelines\u00b6","text":"<ul> <li>Submit your completed notebook as a HTML export, or a PDF file.</li> </ul> <p>To export to HTML, if you are on Jupyter, select <code>File</code> &gt; <code>Export Notebook As</code> &gt; <code>HTML</code>.</p> <p>If you are on VSCode, you can use the <code>Jupyter: Export to HTML</code> command.</p> <ul> <li>Open the command palette (Ctrl+Shift+P or Cmd+Shift+P on Mac).<ul> <li>Search for <code>Jupyter: Export to HTML</code>.</li> <li>Save the HTML file to your computer and submit it via Canvas.</li> </ul> </li> </ul>"},{"location":"m10-text/assignment_m10_sentiment_analysis/#now-repeat-the-above-steps-for-the-remaining-datasets","title":"Now repeat the above steps for the remaining datasets\u00b6","text":""},{"location":"m11-artificial_intelligence/assignment_m11_artificial_intelligence/","title":"Homework 11 - LDA and ChatBot","text":"In\u00a0[\u00a0]: Copied! <pre>from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nimport string\nimport random\nimport nltk\nfrom nltk.corpus import stopwords\nimport re\nimport pandas as pd\nimport numpy as np\n</pre> from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer from sklearn.decomposition import LatentDirichletAllocation import string import random import nltk from nltk.corpus import stopwords import re import pandas as pd import numpy as np <p>Like last week, we need to load the data from the TSV file. This time we will only use the amazon reviews dataset.</p> In\u00a0[\u00a0]: Copied! <pre>df = pd.read_csv(\"../../Datasets/amazon_cells_labelled.tsv\", sep=\"\\t\") # adjust the path as needed\ndf.head()\n</pre> df = pd.read_csv(\"../../Datasets/amazon_cells_labelled.tsv\", sep=\"\\t\") # adjust the path as needed df.head() <p>With our dataframe made, we now need to clean it before analyzing. Apply the <code>remove_punctuation()</code> and <code>remove_stopwords()</code> functions on our dataset to clean it. Save the cleaned data to a new column named <code>cleaned_sentence</code>.</p> In\u00a0[\u00a0]: Copied! <pre>#Uncomment the line below if you need to download the stopwords\n#nltk.download('stopwords')\nstop = stopwords.words('english')\n\ndef remove_punctuation(text):\n    translator = str.maketrans('','', string.punctuation)\n    return text.translate(translator)\n\ndef remove_stopwords(text):\n    text = [word.lower() for word in text.split() if word.lower() not in stop]\n    return \" \".join(text)\n\ndf[_] = df['sentence'].apply(remove_punctuation).apply(remove_stopwords)\ndf.head()\n</pre> #Uncomment the line below if you need to download the stopwords #nltk.download('stopwords') stop = stopwords.words('english')  def remove_punctuation(text):     translator = str.maketrans('','', string.punctuation)     return text.translate(translator)  def remove_stopwords(text):     text = [word.lower() for word in text.split() if word.lower() not in stop]     return \" \".join(text)  df[_] = df['sentence'].apply(remove_punctuation).apply(remove_stopwords) df.head() <p>We need to adjust our data slightly before using LDA. In the cell below, use the <code>CountVectorizer()</code> function. Then, use <code>fit_transform()</code> with <code>df['cleaned_sentence']</code> as a parameter</p> In\u00a0[\u00a0]: Copied! <pre>vect = _(max_features = 5000, max_df=.15) # Your Code Here\nX = vect._(df[_]) # Your Code Here\n</pre> vect = _(max_features = 5000, max_df=.15) # Your Code Here X = vect._(df[_]) # Your Code Here <p>Using the <code>LatenDirichletAllocation()</code> function below, we want to pass it 10 components. You can adjust the max iterations for your local setup, or leave it as 25 if unsure.</p> In\u00a0[\u00a0]: Copied! <pre>lda = _(n_components=10, learning_method=\"batch\", max_iter=25, random_state=0) # Your Code Here\ndocument_topics = lda._(X)\n</pre>  lda = _(n_components=10, learning_method=\"batch\", max_iter=25, random_state=0) # Your Code Here document_topics = lda._(X) <p>And finally' let's see the results! Call the <code>print_topics()</code> function below, passing in <code>feature_names</code> and <code>sorting</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\nfeature_names = np.array(vect.get_feature_names_out())\n\n#slide 27\ndef print_topics(topics, feature_names, sorting, topics_per_chunk, n_words):\n    for i in range(0, len(topics), topics_per_chunk):\n        these_topics = topics[i: i + topics_per_chunk]\n        len_this_chunk = len(topics)\n        \n        print(*these_topics)\n        print(\"----------------------\")\n\n\n        for i in range(n_words):\n            try:\n                print(*feature_names[sorting[these_topics, i]])\n            except Exception as e:\n                print(e)\n                pass\n            \n            \n_(topics=range(10), feature_names=feature_names, sorting=sorting, topics_per_chunk=5, n_words=10) # Your Code Here\n</pre> sorting = np.argsort(lda.components_, axis=1)[:, ::-1] feature_names = np.array(vect.get_feature_names_out())  #slide 27 def print_topics(topics, feature_names, sorting, topics_per_chunk, n_words):     for i in range(0, len(topics), topics_per_chunk):         these_topics = topics[i: i + topics_per_chunk]         len_this_chunk = len(topics)                  print(*these_topics)         print(\"----------------------\")           for i in range(n_words):             try:                 print(*feature_names[sorting[these_topics, i]])             except Exception as e:                 print(e)                 pass                           _(topics=range(10), feature_names=feature_names, sorting=sorting, topics_per_chunk=5, n_words=10) # Your Code Here    <p>Let's build a simple chatbot using rules and sentence similarity. In this particular case we will use the TFIDF vectorizer to convert our sentences into vectors. Note that modern chatbots now use deep learning models, but this is a good exercise to understand the basics of how chatbots can work.</p> <p>We will perform a little bit more preprocessing this time. In addition to removal of punctuation and stopwords, we will also lemmatize the words in our dataset. Lemmatization is the process of reducing a word to its base or root form. For example, \"running\" becomes \"run\". This helps in reducing the dimensionality of our dataset and improves the performance of our model.</p> In\u00a0[\u00a0]: Copied! <pre>import random\nimport string\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import sent_tokenize, word_tokenize\nsent_tokens = df[_].str.lower().tolist()\n</pre> import random import string from nltk.stem import WordNetLemmatizer from nltk import sent_tokenize, word_tokenize sent_tokens = df[_].str.lower().tolist()  <p>Let's define our lemmatization function first. You need to use the <code>WordNetLemmatizer</code> from the <code>nltk</code> library. Make sure to download the WordNet data if you haven't already.</p> In\u00a0[\u00a0]: Copied! <pre>nltk.download('punkt')    # sentence/token splitter\nnltk.download('wordnet')  # for lemmatization\n\nlemmer = WordNetLemmatizer()\n\n# remove punctuation, tokenize, and lemmatize in one call\nremove_punct = dict((ord(p), None) for p in string.punctuation)\ndef LemTokens(tokens):\n    return [lemmer.lemmatize(t) for t in tokens]\n\ndef LemNormalize(text): # Normalize text by removing punctuation, tokenizing, and lemmatizing\n    return LemTokens(word_tokenize(text.lower().translate(remove_punct)))\n</pre> nltk.download('punkt')    # sentence/token splitter nltk.download('wordnet')  # for lemmatization  lemmer = WordNetLemmatizer()  # remove punctuation, tokenize, and lemmatize in one call remove_punct = dict((ord(p), None) for p in string.punctuation) def LemTokens(tokens):     return [lemmer.lemmatize(t) for t in tokens]  def LemNormalize(text): # Normalize text by removing punctuation, tokenizing, and lemmatizing     return LemTokens(word_tokenize(text.lower().translate(remove_punct)))  <p>Let's define some greeting inputs and responses. These will be used to match user inputs with predefined responses. Add your own greetings and responses to the lists below.</p> In\u00a0[\u00a0]: Copied! <pre>GREETING_INPUTS  = {\"hello\",\"hi\", ...} # Add more greetings as needed\nGREETING_RESPONSES = [\"hi\",\"hey\", \"sup\", ...]\n# Add more greeting responses as needed\n\ndef greeting(sentence):\n    for word in sentence.split():\n        if word in GREETING_INPUTS:\n            return random.choice(GREETING_RESPONSES)\n    return None\n</pre> GREETING_INPUTS  = {\"hello\",\"hi\", ...} # Add more greetings as needed GREETING_RESPONSES = [\"hi\",\"hey\", \"sup\", ...] # Add more greeting responses as needed  def greeting(sentence):     for word in sentence.split():         if word in GREETING_INPUTS:             return random.choice(GREETING_RESPONSES)     return None  <p>We need now to define a response function that will take user input and return a response based on  the most similar entries in our dataset.</p> In\u00a0[\u00a0]: Copied! <pre>from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise    import cosine_similarity\n\ndef response(user_response):\n    robo_response = \"\"\n    # temporarily add user query so TF-IDF matrix includes it\n    sent_tokens.append(user_response)\n    tfidf     = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english').fit_transform(sent_tokens)\n    vals      = cosine_similarity(tfidf[-1], tfidf)\n    idx       = vals.argsort()[0][-2]    # second-highest similarity\n    flat      = vals.flatten()\n    flat.sort()\n    sim_score = flat[-2]\n    sent_tokens.pop()                    # remove user query\n\n    if sim_score == 0: # if no similar sentences found\n        robo_response = \"I\u2019m sorry, I don\u2019t understand.\"\n    else:\n        robo_response = sent_tokens[idx]\n    return robo_response\n</pre> from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.metrics.pairwise    import cosine_similarity  def response(user_response):     robo_response = \"\"     # temporarily add user query so TF-IDF matrix includes it     sent_tokens.append(user_response)     tfidf     = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english').fit_transform(sent_tokens)     vals      = cosine_similarity(tfidf[-1], tfidf)     idx       = vals.argsort()[0][-2]    # second-highest similarity     flat      = vals.flatten()     flat.sort()     sim_score = flat[-2]     sent_tokens.pop()                    # remove user query      if sim_score == 0: # if no similar sentences found         robo_response = \"I\u2019m sorry, I don\u2019t understand.\"     else:         robo_response = sent_tokens[idx]     return robo_response <p>Now Let's create the interface.</p> In\u00a0[\u00a0]: Copied! <pre>def chat():\n    # Create a s\n    print(\"ROBO: My name is Robo. Ask me anything about product reviews. Type 'bye' to exit.\")\n    while True:\n        user_input = input(\"YOU: \").lower().strip()\n        if user_input == 'bye': # exit condition. Important!\n            print(\"ROBO: Goodbye! Take care.\")\n            break\n        if user_input in ('thanks','thank you'):\n            print(\"ROBO: You\u2019re welcome!\")\n            break\n        # greeting?\n        greet = greeting(user_input)\n        if greet:\n            print(f\"ROBO: {_}\") # YOUR Code Here\n        else:\n            print(f\"ROBO: {response(_)}\") # YOUR Code Here\n</pre> def chat():     # Create a s     print(\"ROBO: My name is Robo. Ask me anything about product reviews. Type 'bye' to exit.\")     while True:         user_input = input(\"YOU: \").lower().strip()         if user_input == 'bye': # exit condition. Important!             print(\"ROBO: Goodbye! Take care.\")             break         if user_input in ('thanks','thank you'):             print(\"ROBO: You\u2019re welcome!\")             break         # greeting?         greet = greeting(user_input)         if greet:             print(f\"ROBO: {_}\") # YOUR Code Here         else:             print(f\"ROBO: {response(_)}\") # YOUR Code Here <p>Now test it. See if you can find a few sentences that match the reviews in the dataset.</p> In\u00a0[\u00a0]: Copied! <pre>chat()\n</pre> chat()"},{"location":"m11-artificial_intelligence/assignment_m11_artificial_intelligence/#homework-11-lda-and-chatbot","title":"Homework 11 - LDA and ChatBot\u00b6","text":"<p>In this assignment, we will be applying LDA and building a simple chatbot using the provided datasets.</p> <p>Complete the missing parts in this guide.</p>"},{"location":"m11-artificial_intelligence/assignment_m11_artificial_intelligence/#step-1-load-data","title":"Step 1: Load Data\u00b6","text":"<p>You can load the data from the provided TSV file using <code>pandas</code>.</p>"},{"location":"m11-artificial_intelligence/assignment_m11_artificial_intelligence/#step-2-preprocess","title":"Step 2: Preprocess\u00b6","text":"<ul> <li>Clean the data by removing stop-words, punctuations, emoticons etc..</li> </ul>"},{"location":"m11-artificial_intelligence/assignment_m11_artificial_intelligence/#step-3-apply-lda","title":"Step 3: Apply LDA\u00b6","text":"<ul> <li>Find the topics in the dataset using LDA (Latent Dirichlet Allocation).</li> <li>Describe the topics found in the dataset.</li> </ul>"},{"location":"m11-artificial_intelligence/assignment_m11_artificial_intelligence/#step-4-create-a-simple-chatbot","title":"Step 4: Create a simple ChatBot\u00b6","text":"<ul> <li>Use nltk to create a simple chatbot that can respond to user queries based on similarity of sentences in the dataset and the user input.</li> </ul>"},{"location":"m11-artificial_intelligence/assignment_m11_artificial_intelligence/#dataset-overview","title":"Dataset Overview\u00b6","text":"<p>The dataset obtained originally from https://archive.ics.uci.edu/dataset/331/sentiment+labelled+sentences contains sentences labeled with sentiment. Each sentence is associated with a sentiment label (positive or negative). The dataset is split into three parts, each containing sentences from different sources: Amazon, Yelp, and IMDb. Score is either 1 (for positive) or 0 (for negative)</p>"},{"location":"m11-artificial_intelligence/assignment_m11_artificial_intelligence/#submission-guidelines","title":"Submission Guidelines\u00b6","text":"<ul> <li>Submit your completed notebook as a HTML export, or a PDF file.</li> </ul> <p>To export to HTML, if you are on Jupyter, select <code>File</code> &gt; <code>Export Notebook As</code> &gt; <code>HTML</code>.</p> <p>If you are on VSCode, you can use the <code>Jupyter: Export to HTML</code> command.</p> <ul> <li>Open the command palette (Ctrl+Shift+P or Cmd+Shift+P on Mac).<ul> <li>Search for <code>Jupyter: Export to HTML</code>.</li> <li>Save the HTML file to your computer and submit it via Canvas.</li> </ul> </li> </ul>"},{"location":"m11-artificial_intelligence/assignment_m11_artificial_intelligence/#chatbot","title":"ChatBot\u00b6","text":""}]}